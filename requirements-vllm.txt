# vLLM Engine Dependencies
# 适用于: cuda-vllm, cuda-vllm-async

-r requirements-base.txt

# Core
numpy==1.26.4

# PyTorch (vLLM 环境中使用的 CUDA/torch 组合)
--extra-index-url https://download.pytorch.org/whl/cu124
torch==2.9.0
torchvision==0.24.0
torchaudio==2.9.0

# vLLM & Inference
vllm==0.12.0

# gRPC & Communication
grpcio==1.76.0
protobuf==4.25.3

# Performance / runtime
triton==3.5.0
accelerate==1.12.0
bitsandbytes==0.48.2

# Worker Infra
fastapi==0.128.0
pyyaml==6.0.3
