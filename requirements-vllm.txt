# vLLM Engine Dependencies
# 适用于: cuda-vllm, cuda-vllm-async

-r requirements-base.txt

# PyTorch (vLLM 需要 torch>=2.4.0, CUDA 12.1+)
--extra-index-url https://download.pytorch.org/whl/cu121
torch==2.9.0
torchvision==0.24.0
torchaudio==2.9.0

# vLLM
vllm==0.12.0

# Optional: Flash Attention (Linux only)
# flash-attn>=2.5.0; platform_system == "Linux"

# Performance
triton==3.5.0
