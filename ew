#!/usr/bin/env bash
#curl -X POST http://localhost:8080/api/v1/inference/stream -H "Content-Type: application/json" -H "X-API-Key: 111" -d "{\"modelId\": \"default\", \"prompt\": \"ä½ å¥½\"}"
#export GATEWAY_SECURITY_API_KEY="111"
#export HF_ENDPOINT="https://hf-mirror.com"
# =============================================================================
# CY-LLM Engine - ç»Ÿä¸€éƒ¨ç½²å‘½ä»¤è¡Œå·¥å…·
# ç”¨æ³•: ./cy [command] [options] (å…¼å®¹: ./cy-llm, é—ç•™: ./ew)
# =============================================================================
set -e

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# é¡¹ç›®è·¯å¾„
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
BACKEND_DIR="$SCRIPT_DIR/CY_LLM_Backend"
GATEWAY_DIR="$BACKEND_DIR/gateway"
WORKER_DIR="$BACKEND_DIR/worker"
DEPLOY_DIR="$BACKEND_DIR/deploy"

# é»˜è®¤é…ç½®
DEFAULT_ENV_NAME="${CY_LLM_CONDA_ENV:-cy_llm_worker}"
DEFAULT_PYTHON_VERSION="${CY_LLM_PYTHON_VERSION:-3.10}"
DEFAULT_PORT="${CY_LLM_PORT:-8080}"
DEFAULT_WORKER_PORT="${CY_LLM_WORKER_PORT:-50051}"
DEFAULT_ENGINE="${CY_LLM_ENGINE:-cuda-vllm}"

# å¼•æ“åˆ°ä¾èµ–æ–‡ä»¶çš„æ˜ å°„
declare -A ENGINE_REQUIREMENTS=(
    ["cuda-vllm"]="requirements-vllm.txt"
    ["cuda-vllm-async"]="requirements-vllm.txt"
    ["cuda-trt"]="requirements-trt.txt"
    ["nvidia"]="requirements-nvidia.txt"
    ["cuda"]="requirements-nvidia.txt"
    ["ascend-vllm"]="requirements_ascend.txt"
    ["ascend-mindie"]="requirements_ascend.txt"
)

# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

info()    { echo -e "${BLUE}[INFO]${NC} $*"; }
success() { echo -e "${GREEN}[OK]${NC} $*"; }
warn()    { echo -e "${YELLOW}[WARN]${NC} $*"; }
error()   { echo -e "${RED}[ERROR]${NC} $*" >&2; }
step()    { echo -e "\n${CYAN}â”â”â” $* â”â”â”${NC}"; }

check_command() {
    command -v "$1" &>/dev/null
}

get_conda() {
    if check_command conda; then
        echo "conda"
    elif check_command mamba; then
        echo "mamba"
    elif [[ -f "$HOME/miniforge3/bin/conda" ]]; then
        echo "$HOME/miniforge3/bin/conda"
    elif [[ -f "$HOME/miniconda3/bin/conda" ]]; then
        echo "$HOME/miniconda3/bin/conda"
    else
        error "Conda/Mamba æœªæ‰¾åˆ°ï¼Œè¯·å…ˆå®‰è£… Miniforge/Miniconda"
        exit 1
    fi
}

get_python_path() {
    local env_name="${1:-$DEFAULT_ENV_NAME}"
    local conda_cmd=$(get_conda)
    $conda_cmd run -n "$env_name" which python 2>/dev/null
}

# =============================================================================
# å‘½ä»¤: setup - åˆå§‹åŒ–ç¯å¢ƒ
# =============================================================================
cmd_setup() {
    local env_name="$DEFAULT_ENV_NAME"
    local python_version="$DEFAULT_PYTHON_VERSION"
    local engine="$DEFAULT_ENGINE"
    local skip_gradle=false
    local skip_deps=false

    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)      env_name="$2"; shift 2 ;;
            --python)   python_version="$2"; shift 2 ;;
            --engine)   engine="$2"; shift 2 ;;
            --skip-gradle) skip_gradle=true; shift ;;
            --skip-deps)   skip_deps=true; shift ;;
            *) shift ;;
        esac
    done

    # éªŒè¯å¼•æ“ç±»å‹
    if [[ ! -v ENGINE_REQUIREMENTS["$engine"] ]]; then
        error "ä¸æ”¯æŒçš„å¼•æ“ç±»å‹: $engine"
        echo "æ”¯æŒçš„å¼•æ“: ${!ENGINE_REQUIREMENTS[@]}"
        exit 1
    fi

    local req_file="${ENGINE_REQUIREMENTS[$engine]}"

    step "ğŸš€ CY-LLM Engine ç¯å¢ƒåˆå§‹åŒ–"
    info "å¼•æ“: $engine"
    info "ç¯å¢ƒåç§°: $env_name"
    info "ä¾èµ–æ–‡ä»¶: $req_file"
    local conda_cmd=$(get_conda)
    info "ä½¿ç”¨åŒ…ç®¡ç†å™¨: $conda_cmd"

    # 1. åˆ›å»º/æ£€æŸ¥ Conda ç¯å¢ƒ
    step "1/3 Python ç¯å¢ƒ"
    if $conda_cmd env list | grep -q "^$env_name "; then
        success "ç¯å¢ƒ '$env_name' å·²å­˜åœ¨"
    else
        info "åˆ›å»ºç¯å¢ƒ '$env_name' (Python $python_version)..."
        $conda_cmd create -n "$env_name" python="$python_version" -y
        success "ç¯å¢ƒåˆ›å»ºå®Œæˆ"
    fi

    # 2. å®‰è£… Python ä¾èµ–
    if [[ "$skip_deps" != true ]]; then
        step "2/3 å®‰è£…ä¾èµ–: $req_file"
        if [[ -f "$SCRIPT_DIR/$req_file" ]]; then
            info "å®‰è£… Python ä¾èµ–..."
            $conda_cmd run -n "$env_name" pip install -r "$SCRIPT_DIR/$req_file" -q
            success "ä¾èµ–å®‰è£…å®Œæˆ"
        else
            warn "ä¾èµ–æ–‡ä»¶ä¸å­˜åœ¨: $req_file"
        fi

        # ä¸º TRT æ·»åŠ ç‰¹æ®Šæç¤º
        if [[ "$engine" == "cuda-trt" ]]; then
            echo ""
            echo "${YELLOW}âš ï¸  TensorRT-LLM éœ€è¦æ‰‹åŠ¨å®‰è£…${NC}"
            echo "  æ–¹æ³• 1: pip install tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com"
            echo "  æ–¹æ³• 2: ä½¿ç”¨ NGC å®¹å™¨ nvcr.io/nvidia/tensorrt-llm:latest"
            echo "  æ–‡æ¡£: https://github.com/NVIDIA/TensorRT-LLM"
            echo ""
        fi
    else
        info "è·³è¿‡ä¾èµ–å®‰è£…"
    fi
    success "ä¾èµ–å®‰è£…å®Œæˆ"
    
    # 3. æ„å»º Gateway (å¯é€‰)
    if [[ "$skip_gradle" == false ]]; then
        step "2/3 Gateway æ„å»º"
        if check_command java; then
            info "æ„å»º Gateway JAR..."
            (cd "$GATEWAY_DIR" && ./gradlew build -x test --quiet)
            success "Gateway æ„å»ºå®Œæˆ"
        else
            warn "Java æœªå®‰è£…ï¼Œè·³è¿‡ Gateway æ„å»º"
        fi
    else
        info "è·³è¿‡ Gateway æ„å»º"
    fi
    
    # 4. å®Œæˆ
    step "3/3 éªŒè¯"
    local python_path=$(get_python_path "$env_name")
    success "Python: $python_path"
    
    echo ""
    success "âœ¨ ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼"
    echo ""
    echo -e "ä¸‹ä¸€æ­¥: ${CYAN}./cy-llm start${NC} å¯åŠ¨æœåŠ¡ (å…¼å®¹ ./ew)"
    echo -e "        ${CYAN}./cy-llm worker${NC} ä»…å¯åŠ¨ Worker (å…¼å®¹ ./ew)"
}

# =============================================================================
# å‘½ä»¤: start - å¯åŠ¨å®Œæ•´æœåŠ¡ (Gateway + Worker)
# =============================================================================
cmd_start() {
    local env_name="$DEFAULT_ENV_NAME"
    local port="$DEFAULT_PORT"
    local worker_port="$DEFAULT_WORKER_PORT"
    local engine="$DEFAULT_ENGINE"
    local model=""
    local daemon=false
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)      env_name="$2"; shift 2 ;;
            --port)     port="$2"; shift 2 ;;
            --worker-port) worker_port="$2"; shift 2 ;;
            --engine)   engine="$2"; shift 2 ;;
            --model)    model="$2"; shift 2 ;;
            -d|--daemon) daemon=true; shift ;;
            *) shift ;;
        esac
    done

    step "ğŸŒŸ å¯åŠ¨ CY-LLM Engine æœåŠ¡"
    
    local python_path=$(get_python_path "$env_name")
    if [[ -z "$python_path" ]]; then
        error "ç¯å¢ƒ '$env_name' æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ: ./cy setup"
        exit 1
    fi
    
    local jar_path="$GATEWAY_DIR/build/libs/gateway-0.0.1-SNAPSHOT.jar"
    if [[ ! -f "$jar_path" ]]; then
        error "Gateway JAR æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ: ./cy setup"
        exit 1
    fi
    
    info "Engine: $engine"
    info "Gateway Port: $port"
    info "Worker Port: $worker_port"
    info "Python: $python_path"

    # ç»Ÿä¸€ç¯å¢ƒå˜é‡ï¼ˆä¼˜å…ˆ CY_LLM_*ï¼Œå…¼å®¹ EW_*ï¼‰
    export CY_LLM_ENGINE="$engine"
    [[ -n "$model" ]] && export CY_LLM_MODEL="$model"

    # Ensure VLLM GPU memory env vars are set for local starts (default to 0.8)
    export VLLM_GPU_MEM=${VLLM_GPU_MEM:-0.8}
    export VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-$VLLM_GPU_MEM}

    # CY_LLM model registry path
    export CY_LLM_MODEL_REGISTRY_PATH="${CY_LLM_MODEL_REGISTRY_PATH:-\"$SCRIPT_DIR/CY_LLM_Backend/deploy/models.json\"}"
    
    if [[ "$daemon" == true ]]; then
        info "åå°æ¨¡å¼å¯åŠ¨..."
        java -jar "$jar_path" \
            --server.port="$port" \
            --self-deploy.enabled=true \
            --self-deploy.python-executable="$python_path" \
            --self-deploy.worker-dir="$WORKER_DIR" \
            --self-deploy.worker-port="$worker_port" \
            > /tmp/cy-llm-gateway.log 2>&1 &
        echo $! > /tmp/cy-llm-gateway.pid
        success "Gateway PID: $(cat /tmp/cy-llm-gateway.pid)"
        success "æ—¥å¿—: /tmp/cy-llm-gateway.log"
    else
        java -jar "$jar_path" \
            --server.port="$port" \
            --self-deploy.enabled=true \
            --self-deploy.python-executable="$python_path" \
            --self-deploy.worker-dir="$WORKER_DIR" \
            --self-deploy.worker-port="$worker_port"
    fi
}

# =============================================================================
# å‘½ä»¤: worker - ä»…å¯åŠ¨ Worker
# =============================================================================
cmd_worker() {
    local env_name="$DEFAULT_ENV_NAME"
    local port="$DEFAULT_WORKER_PORT"
    local engine="$DEFAULT_ENGINE"
    local model=""
    local serve=true
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)      env_name="$2"; shift 2 ;;
            --port)     port="$2"; shift 2 ;;
            --engine)   engine="$2"; shift 2 ;;
            --model)    model="$2"; shift 2 ;;
            --no-serve) serve=false; shift ;;
            *) shift ;;
        esac
    done

    step "ğŸ”§ å¯åŠ¨ Worker"
    
    local conda_cmd=$(get_conda)
    
    export CY_LLM_ENGINE="$engine"
        export CY_LLM_ENGINE="$engine"
    [[ -n "$model" ]] && export CY_LLM_MODEL="$model"
        [[ -n "$model" ]] && export CY_LLM_MODEL="$model"
    
    info "Engine: $engine"
    info "Port: $port"
    
    local serve_flag=""
    [[ "$serve" == true ]] && serve_flag="--serve"
    
    $conda_cmd run -n "$env_name" python "$WORKER_DIR/main.py" \
        --port "$port" \
        ${model:+--model "$model"} \
        $serve_flag
}

# =============================================================================
# å‘½ä»¤: docker - Docker Compose éƒ¨ç½²
# =============================================================================
cmd_docker() {
    local action="${1:-up}"
    shift || true
    local engine="$DEFAULT_ENGINE"
    local scale=1
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --engine)   engine="$2"; shift 2 ;;
            --scale)    scale="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    step "ğŸ³ Docker éƒ¨ç½²"
    
    cd "$DEPLOY_DIR"
    
    export CY_LLM_ENGINE="$engine"
        export CY_LLM_ENGINE="$engine"
    
    case "$action" in
        up)
            info "å¯åŠ¨å®¹å™¨ (Engine: $engine, Scale: $scale)..."
            case "$engine" in
                ascend-*) docker compose --profile ascend up -d --scale worker-ascend="$scale" ;;
                *)        docker compose up -d --scale worker-nvidia="$scale" ;;
            esac
            success "å®¹å™¨å¯åŠ¨å®Œæˆ"
            docker compose ps
            ;;
        down)
            info "åœæ­¢å®¹å™¨..."
            docker compose --profile ascend down
            success "å®¹å™¨å·²åœæ­¢"
            ;;
        logs)
            docker compose logs -f
            ;;
        ps)
            docker compose ps
            ;;
        build)
            info "æ„å»ºé•œåƒ..."
            docker compose build
            success "é•œåƒæ„å»ºå®Œæˆ"
            ;;
        *)
            error "æœªçŸ¥æ“ä½œ: $action"
            echo "å¯ç”¨æ“ä½œ: up, down, logs, ps, build"
            exit 1
            ;;
    esac
}

# =============================================================================
# å‘½ä»¤: stop - åœæ­¢æœåŠ¡
# =============================================================================
cmd_stop() {
    step "ğŸ›‘ åœæ­¢æœåŠ¡"
    
    if [[ -f /tmp/cy-llm-gateway.pid ]]; then
        local pid=$(cat /tmp/cy-llm-gateway.pid)
        if kill -0 "$pid" 2>/dev/null; then
            kill "$pid"
            rm /tmp/cy-llm-gateway.pid
            success "Gateway (PID $pid) å·²åœæ­¢"
        fi
    fi
    
    # æŸ¥æ‰¾å¹¶åœæ­¢æ‰€æœ‰ç›¸å…³è¿›ç¨‹
    pkill -f "gateway-.*-SNAPSHOT.jar" 2>/dev/null && info "Gateway è¿›ç¨‹å·²åœæ­¢" || true
    pkill -f "worker/main.py" 2>/dev/null && info "Worker è¿›ç¨‹å·²åœæ­¢" || true
    
    success "æœåŠ¡å·²åœæ­¢"
}

# =============================================================================
# å‘½ä»¤: status - æŸ¥çœ‹çŠ¶æ€
# =============================================================================
cmd_status() {
    step "ğŸ“Š æœåŠ¡çŠ¶æ€"
    
    echo -e "\n${CYAN}è¿›ç¨‹çŠ¶æ€:${NC}"
    if pgrep -f "gateway-.*-SNAPSHOT.jar" >/dev/null; then
        success "Gateway: è¿è¡Œä¸­"
    else
        warn "Gateway: æœªè¿è¡Œ"
    fi
    
    if pgrep -f "worker/main.py" >/dev/null; then
        success "Worker: è¿è¡Œä¸­"
    else
        warn "Worker: æœªè¿è¡Œ"
    fi
    
    echo -e "\n${CYAN}ç«¯å£ç›‘å¬:${NC}"
    ss -tlnp 2>/dev/null | grep -E ":8080|:50051" || echo "  æ— ç›¸å…³ç«¯å£ç›‘å¬"
    
    echo -e "\n${CYAN}ç¯å¢ƒå˜é‡:${NC}"
    echo "  CY_LLM_ENGINE=${CY_LLM_ENGINE:-æœªè®¾ç½®}"
    echo "  CY_LLM_CONDA_ENV=${CY_LLM_CONDA_ENV:-$DEFAULT_ENV_NAME}"
}

cmd_diagnose() {
    local model_name="${CY_LLM_MODEL:-deepseek-v3}"

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --model)
                model_name="$2"
                shift 2
                ;;
            *)
                model_name="$1"
                shift
                ;;
        esac
    done

    step "ğŸ©º CY-LLM ç¯å¢ƒè¯Šæ–­: $model_name"

    export CY_LLM_MODEL="$model_name"
    export CY_LLM_MODEL_REGISTRY_PATH="$BACKEND_DIR/deploy/config.json"

    PYTHONPATH="$BACKEND_DIR" CY_LLM_MODEL="$model_name" \
        python - <<'PY'
import os
from worker.config.config_loader import load_worker_config
from worker.utils.diagnostic import (
    check_vram_for_model,
    format_vram_report,
    gather_gpu_summary,
    check_dependencies,
    format_dependency_summary,
)

config = load_worker_config(registry_path=os.environ.get("CY_LLM_MODEL_REGISTRY_PATH"))
model_id = os.environ["CY_LLM_MODEL"]
spec = config.model_registry.get(model_id)
if spec is None:
    raise SystemExit(f"æ¨¡å‹ {model_id} æœªåœ¨æ³¨å†Œè¡¨ä¸­å®šä¹‰")

gpu = gather_gpu_summary()
print("CY-LLM ç¯å¢ƒè¯Šæ–­æŠ¥å‘Š")
print("=====================")
print("ç¡¬ä»¶ä¿¡æ¯:")
if gpu.get("available"):
    print(f"  GPU: {gpu.get('name')} ({gpu.get('total')} æ€», {gpu.get('free')} å¯ç”¨)")
    print(f"  CUDA: {gpu.get('cuda')}")
else:
    print("  GPU: æœªæ£€æµ‹åˆ°å¯ç”¨ CUDA è®¾å¤‡")

report = check_vram_for_model(model_id, spec)
print("")
print(f"æ¨¡å‹æ£€æµ‹: {model_id}")
print(format_vram_report(report))

print("\nä¾èµ–æ£€æŸ¥:")
statuses = check_dependencies()
for line in format_dependency_summary(statuses):
    print(line)
PY
}

# =============================================================================
# å‘½ä»¤: test - è¿è¡Œæµ‹è¯•
# =============================================================================
cmd_test() {
    local env_name="$DEFAULT_ENV_NAME"
    local type="${1:-all}"
    
    step "ğŸ§ª è¿è¡Œæµ‹è¯•"
    
    local conda_cmd=$(get_conda)
    
    case "$type" in
        integration|int)
            info "è¿è¡Œé›†æˆæµ‹è¯•..."
            $conda_cmd run -n "$env_name" python "$BACKEND_DIR/tests/test_integration.py"
            ;;
        unit)
            info "è¿è¡Œå•å…ƒæµ‹è¯•..."
            (cd "$GATEWAY_DIR" && ./gradlew test)
            ;;
        all)
            cmd_test integration
            cmd_test unit
            ;;
        *)
            error "æœªçŸ¥æµ‹è¯•ç±»å‹: $type"
            echo "å¯ç”¨ç±»å‹: integration, unit, all"
            exit 1
            ;;
    esac
}

# =============================================================================
# å‘½ä»¤: models - æ¨¡å‹ç®¡ç†
# =============================================================================
cmd_models() {
    local env_name="$DEFAULT_ENV_NAME"
    local action="${1:-list}"
    
    local conda_cmd=$(get_conda)
    
    case "$action" in
        list|ls)
            step "ğŸ“¦ å¯ç”¨æ¨¡å‹"
            $conda_cmd run -n "$env_name" python "$WORKER_DIR/main.py" --list-models
            ;;
        *)
            error "æœªçŸ¥æ“ä½œ: $action"
            echo "å¯ç”¨æ“ä½œ: list"
            exit 1
            ;;
    esac
}

# =============================================================================
# å‘½ä»¤: doctor - ç¯å¢ƒè¯Šæ–­
# =============================================================================
cmd_doctor() {
    local env_name="${CY_LLM_CONDA_ENV:-${EW_CONDA_ENV:-$DEFAULT_ENV_NAME}}"

    step "ğŸ” ç¯å¢ƒè¯Šæ–­"

    # æ£€æŸ¥ Conda ç¯å¢ƒ
    echo -e "\n${CYAN}Conda ç¯å¢ƒ:${NC}"
    local conda_cmd=$(get_conda)
    if $conda_cmd env list | grep -q "^$env_name "; then
        echo "  âœ… ç¯å¢ƒå­˜åœ¨: $env_name"
    else
        echo "  âŒ ç¯å¢ƒä¸å­˜åœ¨: $env_name"
        echo "  è¿è¡Œ: ./cy-llm setup --engine <engine_type>"
        return 1
    fi

    # æ£€æŸ¥å…³é”®ä¾èµ–
    echo -e "\n${CYAN}ä¾èµ–æ£€æŸ¥:${NC}"
    $conda_cmd run -n "$env_name" python - <<'PY'
import sys
def check_import(module, name=None):
    try:
        mod = __import__(module)
        version = getattr(mod, "__version__", "æœªçŸ¥")
        print(f"  âœ… {name or module}: {version}")
        return True
    except ImportError:
        print(f"  âŒ {name or module}: æœªå®‰è£…")
        return False

check_import("torch", "PyTorch")
check_import("transformers")
check_import("vllm", "vLLM")
try:
    __import__("tensorrt_llm")
    print("  âœ… TensorRT-LLM: å·²å®‰è£…")
except ImportError:
    print("  âŒ TensorRT-LLM: æœªå®‰è£…")
PY

    # æ£€æŸ¥ GPU
    echo -e "\n${CYAN}GPU çŠ¶æ€:${NC}"
    if command -v nvidia-smi &> /dev/null; then
        nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader 2>/dev/null | \
            awk -F', ' '{printf "  GPU: %s (%s æ€», %s å¯ç”¨)\n", $1, $2, $3}' || \
            echo "  âš ï¸  æ— æ³•è·å– GPU ä¿¡æ¯"
    else
        echo "  âŒ nvidia-smi ä¸å¯ç”¨"
    fi

    echo ""
    success "è¯Šæ–­å®Œæˆ"
}

# =============================================================================
# å‘½ä»¤: convert-trt - æ¨¡å‹è½¬æ¢ä¸º TensorRT å¼•æ“
# =============================================================================
cmd_convert_trt() {
    local model=""
    local output=""
    local env_name="${CY_LLM_CONDA_ENV:-${EW_CONDA_ENV:-$DEFAULT_ENV_NAME}}"

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --model) model="$2"; shift 2 ;;
            --output) output="$2"; shift 2 ;;
            --env) env_name="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    if [[ -z "$model" || -z "$output" ]]; then
        error "ç”¨æ³•: ./cy convert-trt --model <model> --output <output>"
        error "ç¤ºä¾‹: ./cy convert-trt --model Qwen/Qwen2.5-7B-Instruct --output /models/qwen2.5-7b-trt"
        exit 1
    fi

    step "ğŸ”§ è½¬æ¢æ¨¡å‹ä¸º TensorRT-LLM å¼•æ“"

    conda run -n "$env_name" python "$SCRIPT_DIR/scripts/convert_trt.py" \
        --model "$model" \
        --output "$output" \
        "$@"
}

# =============================================================================
# å‘½ä»¤: help - å¸®åŠ©ä¿¡æ¯
# =============================================================================
cmd_help() {
    cat << 'EOF'
CY-LLM Engine - ç»Ÿä¸€éƒ¨ç½²å‘½ä»¤è¡Œå·¥å…·

ç”¨æ³•: ./cy-llm <command> [options]

å‘½ä»¤:
  setup       åˆå§‹åŒ–ç¯å¢ƒ (Conda + ä¾èµ– + Gateway æ„å»º)
  start       å¯åŠ¨å®Œæ•´æœåŠ¡ (Gateway + Worker)
  worker      ä»…å¯åŠ¨ Worker
  stop        åœæ­¢æ‰€æœ‰æœåŠ¡
  status      æŸ¥çœ‹æœåŠ¡çŠ¶æ€
  doctor      ç¯å¢ƒè¯Šæ–­ (æ£€æŸ¥ä¾èµ–ã€GPUã€é…ç½®)
  convert-trt è½¬æ¢æ¨¡å‹ä¸º TensorRT-LLM å¼•æ“
  docker      Docker Compose éƒ¨ç½²
  test        è¿è¡Œæµ‹è¯•
  models      æ¨¡å‹ç®¡ç†
  help        æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    diagnose    ç¯å¢ƒä¸æ¨¡å‹è¯Šæ–­

å¸¸ç”¨é€‰é¡¹:
<<<<<<< HEAD
    --env NAME        Conda ç¯å¢ƒåç§° (é»˜è®¤: cy_llm_worker)
=======
  --env NAME        Conda ç¯å¢ƒåç§° (é»˜è®¤: ew-ai-worker)
>>>>>>> AI-backend
  --engine TYPE     æ¨ç†å¼•æ“ç±»å‹:
                      cuda-vllm      - NVIDIA GPU + vLLM (æ¨è)
                      cuda-trt       - NVIDIA GPU + TensorRT-LLM
                      nvidia         - NVIDIA GPU + Transformers + bitsandbytes
                      ascend-vllm    - åä¸º NPU + vLLM-Ascend
                      ascend-mindie  - åä¸º NPU + MindIE Turbo
  --port PORT       Gateway ç«¯å£ (é»˜è®¤: 8080)
  --model ID        æ¨¡å‹ ID
  --skip-deps       è·³è¿‡ä¾èµ–å®‰è£…
  --skip-gradle     è·³è¿‡ Gateway æ„å»º

ç¤ºä¾‹:
  # åˆå§‹åŒ– vLLM ç¯å¢ƒ
  ./cy-llm setup --engine cuda-vllm

  # åˆå§‹åŒ– TRT ç¯å¢ƒ
  ./cy-llm setup --engine cuda-trt

  # è¯Šæ–­ç¯å¢ƒ
  ./cy-llm doctor

  # è½¬æ¢æ¨¡å‹ä¸º TRT å¼•æ“
  ./cy-llm convert-trt --model Qwen/Qwen2.5-7B-Instruct --output /models/qwen2.5-7b-trt

  # å¯åŠ¨æœåŠ¡
  ./cy-llm start --engine cuda-vllm --model deepseek-v3

  # åå°å¯åŠ¨
  ./cy-llm start -d

  # Docker éƒ¨ç½²
  ./cy-llm docker up --engine cuda-vllm --scale 2

  # æŸ¥çœ‹çŠ¶æ€
  ./cy-llm status

ç¯å¢ƒå˜é‡:
    CY_LLM_ENGINE          é»˜è®¤æ¨ç†å¼•æ“
    CY_LLM_CONDA_ENV       é»˜è®¤ Conda ç¯å¢ƒå
    CY_LLM_PORT            é»˜è®¤ Gateway ç«¯å£
    CY_LLM_WORKER_PORT     é»˜è®¤ Worker ç«¯å£

æ›´å¤šä¿¡æ¯: https://github.com/your-repo/CY-LLM-Engine
EOF
}

# =============================================================================
# ä¸»å…¥å£
# =============================================================================
main() {
    local command="${1:-help}"
    shift || true
    
    case "$command" in
        setup)      cmd_setup "$@" ;;
        start)      cmd_start "$@" ;;
        worker)     cmd_worker "$@" ;;
        docker)     cmd_docker "$@" ;;
        stop)       cmd_stop "$@" ;;
        status)     cmd_status "$@" ;;
        doctor)     cmd_doctor "$@" ;;
        convert-trt) cmd_convert_trt "$@" ;;
        test)       cmd_test "$@" ;;
        models)     cmd_models "$@" ;;
        diagnose)   cmd_diagnose "$@" ;;
        help|--help|-h) cmd_help ;;
        *)
            error "æœªçŸ¥å‘½ä»¤: $command"
            echo "è¿è¡Œ './cy-llm help' æŸ¥çœ‹å¸®åŠ©"
            exit 1
            ;;
    esac
}

main "$@"
