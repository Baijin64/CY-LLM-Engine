# Phase 2 & 3 å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ å¿«é€Ÿä½¿ç”¨

### 1. ä½¿ç”¨ VRAM é¢„ä¼°å™¨ï¼ˆè‡ªåŠ¨ï¼‰

æ¨¡å‹åŠ è½½æ—¶ä¼šè‡ªåŠ¨è¿è¡Œ VRAM é¢„æ£€æŸ¥ï¼š

```python
from CY_LLM_Backend.worker.engines.vllm_cuda_engine import VllmCudaEngine

engine = VllmCudaEngine(
    max_model_len=2048,
    gpu_memory_utilization=0.90,
)

# è‡ªåŠ¨è¿›è¡Œ VRAM é¢„æ£€æŸ¥ï¼Œå¦‚æœä¸å®‰å…¨ä¼šè‡ªåŠ¨é™ä½ gpu_memory_utilization
engine.load_model("Qwen/Qwen2.5-7B-Instruct")
```

**æ—¥å¿—è¾“å‡º**:
```
INFO: VRAM ä¼°ç®—: âœ… æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥åŠ è½½
```

æˆ–ï¼š
```
WARNING: è‡ªåŠ¨è°ƒæ•´ gpu_memory_utilization: 0.90 -> 0.65
```

### 2. æ‰‹åŠ¨ä¼°ç®—æ˜¾å­˜ï¼ˆå¯é€‰ï¼‰

```python
from CY_LLM_Backend.worker.utils.vram_optimizer import estimate_vram_requirements

estimate = estimate_vram_requirements(
    model_name_or_params="Qwen/Qwen2.5-7B-Instruct",
    max_model_len=2048,
    dtype="fp16",
    quantization=None,
    engine_type="vllm"
)

print(f"éœ€è¦æ˜¾å­˜: {estimate.required_gb:.2f}GB")
print(f"å¯ç”¨æ˜¾å­˜: {estimate.available_gb:.2f}GB")
print(f"å»ºè®®: {estimate.recommendation}")
```

### 3. è½¬æ¢æ¨¡å‹ä¸º TRT å¼•æ“

```bash
# æŸ¥çœ‹å¸®åŠ©
./cy convert-trt --help  # or ./cy-llm convert-trt --help

# è½¬æ¢æ¨¡å‹
./cy convert-trt \
  --model Qwen/Qwen2.5-7B-Instruct \
  --output /models/qwen2.5-7b-trt

# ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°
./cy convert-trt \
  --model Qwen/Qwen2.5-7B-Instruct \
  --output /models/qwen2.5-7b-trt \
  --dtype float16 \
  --max-batch-size 64 \
  --max-input-len 4096 \
  --max-output-len 2048
```

### 4. å¯åŠ¨ TRT æœåŠ¡

```bash
# åˆå§‹åŒ–ç¯å¢ƒ
./cy setup --engine cuda-trt

# å¯åŠ¨æœåŠ¡
./cy start --engine cuda-trt --model qwen2.5-7b-trt

# æŸ¥çœ‹çŠ¶æ€
./cy status
```

## ğŸ“Š æ–°å¢ç‰¹æ€§å¯¹ç…§è¡¨

| ç‰¹æ€§ | ä½ç½® | è§¦å‘æ–¹å¼ | ç”¨æˆ·éœ€è¦åšä»€ä¹ˆ |
|------|------|--------|---------------|
| **VRAM é¢„æ£€æŸ¥** | vllm_cuda_engine.py | è‡ªåŠ¨ | æ— ï¼Œè‡ªåŠ¨è¿è¡Œ |
| **VRAM é¢„ä¼°** | vram_optimizer.py | æ‰‹åŠ¨æˆ–è‡ªåŠ¨ | å¯é€‰æ‰‹åŠ¨è°ƒç”¨ |
| **OOM è‡ªåŠ¨é‡è¯•** | server.py | è‡ªåŠ¨ | æ— ï¼Œè‡ªåŠ¨å¤„ç† |
| **TRT çœŸæµå¼** | trt_engine.py | è‡ªåŠ¨ | æ— ï¼Œè‡ªåŠ¨å¤„ç† |
| **TRT è½¬æ¢å·¥å…·** | scripts/convert_trt.py | æ‰‹åŠ¨ | `./cy convert-trt ...` |
| **TRT ä½¿ç”¨æ–‡æ¡£** | docs/TRT_GUIDE.md | å‚è€ƒ | æŸ¥çœ‹æ–‡æ¡£ |

## ğŸ” å…³é”®æ–‡ä»¶ä½ç½®

### ä»£ç æ–‡ä»¶

```
CY_LLM_Backend/worker/
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ vram_optimizer.py          # VRAM é¢„ä¼°å’Œä¼˜åŒ–
â”œâ”€â”€ engines/
â”‚   â”œâ”€â”€ vllm_cuda_engine.py        # vLLM å¼•æ“ï¼ˆå·²é›†æˆ VRAM é¢„æ£€æŸ¥ï¼‰
â”‚   â””â”€â”€ trt_engine.py              # TRT å¼•æ“ï¼ˆå·²æ”¹è¿›æµå¼è¾“å‡ºï¼‰
â””â”€â”€ core/
    â””â”€â”€ server.py                  # æ¨ç†æœåŠ¡å™¨ï¼ˆå·²é›†æˆ OOM é‡è¯•ï¼‰

scripts/
â””â”€â”€ convert_trt.py                 # TRT æ¨¡å‹è½¬æ¢å·¥å…·
```

### æ–‡æ¡£æ–‡ä»¶

```
docs/
â””â”€â”€ TRT_GUIDE.md                   # TensorRT-LLM å®Œæ•´ä½¿ç”¨æŒ‡å—

PHASE2_3_UPGRADE_REPORT.md         # å‡çº§è¯¦ç»†æŠ¥å‘Š
```

### è„šæœ¬

```
cy / cy-llm                         # ä¸»è„šæœ¬
```

## âš¡ å¸¸è§å‘½ä»¤é€ŸæŸ¥

```bash
  # è¯Šæ–­ç¯å¢ƒ
  ./cy doctor

# åˆå§‹åŒ– vLLM ç¯å¢ƒ
./cy setup --engine cuda-vllm

# åˆå§‹åŒ– TRT ç¯å¢ƒ
./cy setup --engine cuda-trt

# è½¬æ¢æ¨¡å‹ä¸º TRT
./cy convert-trt --model <model> --output <dir>

# å¯åŠ¨ vLLM æœåŠ¡
./cy start --engine cuda-vllm --model <model>

# å¯åŠ¨ TRT æœåŠ¡
./cy start --engine cuda-trt --model <model>

# åœæ­¢æœåŠ¡
./cy stop

# æŸ¥çœ‹çŠ¶æ€
./cy status

# å¸®åŠ©ä¿¡æ¯
./cy help
```

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: VRAM é¢„æ£€æŸ¥æŠ¥å‘Š "æ˜¾å­˜ä¸è¶³"

**è§£å†³**:
```python
# æ–¹æ¡ˆ A: è·³è¿‡æ£€æŸ¥
engine.load_model(model_path, skip_vram_check=True)

# æ–¹æ¡ˆ B: æ‰‹åŠ¨è°ƒæ•´é…ç½®
engine.gpu_memory_utilization = 0.65
engine.load_model(model_path)

# æ–¹æ¡ˆ C: ä½¿ç”¨é‡åŒ–
estimate = estimate_vram_requirements(
    "Qwen/Qwen2.5-7B",
    quantization="awq"  # æ”¹ä¸º 4-bit é‡åŒ–
)
```

### é—®é¢˜ 2: æ¨¡å‹åŠ è½½ OOM

**è‡ªåŠ¨å¤„ç†**: ç³»ç»Ÿä¼šè‡ªåŠ¨é‡è¯• 3 æ¬¡ï¼Œå¦‚æœä»å¤±è´¥ï¼ŒæŸ¥çœ‹æ—¥å¿—ï¼š

```bash
tail -f logs/worker.log
```

**æ‰‹åŠ¨è°ƒæ•´**:
```python
# é™ä½æ˜¾å­˜åˆ©ç”¨ç‡
engine.gpu_memory_utilization = 0.50
engine.max_model_len = 2048
engine.load_model(model_path)
```

### é—®é¢˜ 3: TRT è½¬æ¢å¤±è´¥

**æ£€æŸ¥**:
```bash
# 1. éªŒè¯ä¾èµ–å®‰è£…
python -c "import tensorrt_llm; print(tensorrt_llm.__version__)"

# 2. æ£€æŸ¥æ¨¡å‹æ ¼å¼
ls -la /path/to/model/

# 3. æŸ¥çœ‹è¯¦ç»†é”™è¯¯
python scripts/convert_trt.py --model ... --output ... --verbose
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

```bash
# å®æ—¶æ˜¾å­˜ä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/worker.log

# ç»Ÿè®¡æ¨ç†å»¶è¿Ÿ
# é€šè¿‡ API å“åº”å¤´ X-Response-Time æŸ¥çœ‹
curl -i http://localhost:8080/api/v1/health
```

## ğŸ’¡ æœ€ä½³å®è·µ

1. **æ€»æ˜¯è¿è¡Œ doctor å‘½ä»¤**
   ```bash
  ./cy doctor
   ```

2. **è½¬æ¢ TRT åæµ‹è¯•**
   ```bash
   # å°æ‰¹é‡æµ‹è¯•
   curl -X POST http://localhost:8080/api/v1/inference \
     -d '{"modelId":"...","prompt":"test"}'
   ```

3. **ä¸ºå¸¸ç”¨æ¨¡å‹é¢„ç¼–è¯‘ TRT**
   ```bash
   # æå‰è½¬æ¢ï¼Œé¿å…é¦–æ¬¡å¯åŠ¨æ…¢
  ./cy convert-trt --model Qwen/Qwen2.5-7B --output /models/qwen2.5-7b-trt
   ```

4. **å®šæœŸç›‘æ§æ˜¾å­˜**
   ```bash
   # é•¿æœŸè¿è¡Œæ—¶ç›‘æ§
   watch -n 5 'nvidia-smi | grep python'
   ```

## ğŸ“š æ›´å¤šä¿¡æ¯

- è¯¦ç»†å‡çº§æŠ¥å‘Š: `PHASE2_3_UPGRADE_REPORT.md`
- TRT å®Œæ•´æŒ‡å—: `docs/TRT_GUIDE.md`
- æºä»£ç : `CY_LLM_Backend/worker/utils/vram_optimizer.py`
- è½¬æ¢å·¥å…·: `scripts/convert_trt.py`

---

**ç‰ˆæœ¬**: v3.5.0  
**å®Œæˆæ—¶é—´**: 2025-12-03  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
