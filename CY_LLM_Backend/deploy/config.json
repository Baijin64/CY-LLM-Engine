{
  "models": {
    "deepseek-v3": {
      "engine": "cuda-vllm",
      "model_path": "deepseek-ai/DeepSeek-V3",
      "max_model_len": 8192,
      "gpu_memory_utilization": 0.9,
      "tensor_parallel_size": 1,
      "enable_prefix_caching": true,
      "kv_cache_dtype": "auto",
      "enable_prompt_cache": true,
      "prompt_cache_ttl": 3600
    },
    "deepseek-v3-async": {
      "engine": "cuda-vllm-async",
      "model_path": "deepseek-ai/DeepSeek-V3",
      "max_model_len": 8192,
      "gpu_memory_utilization": 0.9,
      "tensor_parallel_size": 1,
      "enable_prefix_caching": true,
      "kv_cache_dtype": "auto",
      "enable_prompt_cache": true,
      "prompt_cache_ttl": 7200
    },
    "qwen2.5-72b": {
      "engine": "cuda-vllm",
      "model_path": "Qwen/Qwen2.5-72B-Instruct",
      "max_model_len": 32768,
      "tensor_parallel_size": 4,
      "enable_prefix_caching": true
    },
    "llama-3.1-8b": {
      "engine": "cuda-vllm",
      "model_path": "meta-llama/Llama-3.1-8B-Instruct",
      "max_model_len": 8192,
      "enable_prefix_caching": true,
      "enable_prompt_cache": true,
      "prompt_cache_ttl": 1800
    },
    "qwen2.5-7b-trt": {
      "engine": "cuda-trt",
      "model_path": "/models/qwen2.5-7b-trt",
      "max_batch_size": 64,
      "max_input_len": 4096,
      "max_output_len": 2048
    },
    "qwen2.5-72b-ascend": {
      "engine": "ascend-vllm",
      "model_path": "Qwen/Qwen2.5-72B-Instruct",
      "max_model_len": 32768,
      "tensor_parallel_size": 8
    },
    "deepseek-v3-mindie": {
      "engine": "ascend-mindie",
      "model_path": "/models/deepseek-v3-mindie",
      "max_batch_size": 32
    }
  },
  "backends": {
    "nvidia": {
      "type": "grpc",
      "endpoint": "worker-nvidia:50051",
      "retry_policy": {
        "max_attempts": 3,
        "backoff_ms": 100
      }
    },
    "ascend": {
      "type": "grpc",
      "endpoint": "worker-ascend:50051",
      "retry_policy": {
        "max_attempts": 5,
        "backoff_ms": 200
      }
    }
  },
  "defaults": {
    "temperature": 0.7,
    "max_tokens": 2048,
    "top_p": 0.9
  }
}
