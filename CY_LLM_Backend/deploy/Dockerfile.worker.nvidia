# Dockerfile.worker.nvidia
# [部署] NVIDIA GPU Worker 镜像
# 用法: docker build -f deploy/Dockerfile.worker.nvidia -t cy-llm-worker-nvidia .

FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

LABEL maintainer="CY-LLM Team"
LABEL description="CY-LLM Worker - PyTorch + NVIDIA CUDA Inference Engine"

# 设置环境变量
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PIP_NO_CACHE_DIR=1
ENV PIP_DISABLE_PIP_VERSION_CHECK=1

# 安装 Python 3.10 和系统依赖
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3.10-venv \
    python3-pip \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.10 /usr/bin/python3 \
    && ln -sf /usr/bin/python3 /usr/bin/python

# 创建非 root 用户
RUN groupadd -g 1000 ewai && useradd -u 1000 -g ewai -m -s /bin/bash ewai

WORKDIR /app

# 复制依赖文件并安装（利用缓存层）
COPY worker/requirements.txt ./requirements.txt
RUN pip3 install --no-cache-dir -r requirements.txt

# 复制 Proto 生成代码和源码
COPY worker/ ./worker/
COPY proto/ ./proto/

# 生成 gRPC 代码（如果尚未生成）
RUN python3 -m grpc_tools.protoc \
    -I./proto \
    --python_out=./worker/proto_gen \
    --pyi_out=./worker/proto_gen \
    --grpc_python_out=./worker/proto_gen \
    ./proto/ai_service.proto \
    && sed -i 's/^import ai_service_pb2/from . import ai_service_pb2/' ./worker/proto_gen/ai_service_pb2_grpc.py \
    || echo "Proto files already generated"

# 创建模型和检查点目录
RUN mkdir -p /models /checkpoints /etc/worker \
    && chown -R ewai:ewai /app /models /checkpoints /etc/worker

USER ewai

# 健康检查（通过 gRPC 健康检查端点）
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD python3 -c "import grpc; from worker.proto_gen import AiInferenceStub, WorkerHealthRequest; \
        ch = grpc.insecure_channel('localhost:50051'); \
        stub = AiInferenceStub(ch); \
        resp = stub.Health(WorkerHealthRequest()); \
        exit(0 if resp.healthy else 1)" || exit 1

# 暴露 gRPC 端口
EXPOSE 50051

# 环境变量（可在运行时覆盖）
ENV CY_LLM_BACKEND=nvidia
ENV CY_LLM_MODEL_REGISTRY=/etc/worker/models.json
ENV CY_LLM_INTERNAL_TOKEN=""

# 启动命令
CMD ["python3", "-m", "worker.main", "--serve", "--port", "50051"]