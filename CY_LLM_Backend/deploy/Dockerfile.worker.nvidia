# Dockerfile.worker.nvidia
# [部署] NVIDIA GPU Worker 镜像
# 用法: docker build -f deploy/Dockerfile.worker.nvidia -t cy-llm-worker-nvidia .

FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

LABEL maintainer="CY-LLM Team"
LABEL description="CY-LLM Worker - PyTorch + NVIDIA CUDA Inference Engine"

# 设置环境变量
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PIP_NO_CACHE_DIR=1
ENV PIP_DISABLE_PIP_VERSION_CHECK=1

# 安装 Python 3.10 和系统依赖
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3.10-venv \
    python3-pip \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.10 /usr/bin/python3 \
    && ln -sf /usr/bin/python3 /usr/bin/python

# 创建非 root 用户
RUN groupadd -g 1000 ewai && useradd -u 1000 -g ewai -m -s /bin/bash ewai

WORKDIR /app

# 复制依赖文件并安装（利用缓存层）
COPY worker/requirements.txt ./requirements.txt

# 安装 PyTorch CUDA (cu121) 版本，避免误装 CPU-only wheel
# 说明：torch 的 CUDA wheels 来自 download.pytorch.org，不能依赖 PyPI 默认解析。
RUN python3 -m pip install --no-cache-dir --upgrade pip \
    && pip3 install --no-cache-dir --index-url https://download.pytorch.org/whl/cu121 torch==2.5.1 \
    && grep -vE '^(torch|torchvision|torchaudio)\b' requirements.txt > requirements.no-torch.txt \
    && pip3 install --no-cache-dir -r requirements.no-torch.txt

# 复制 Proto 生成代码和源码
COPY worker/ ./worker/
COPY proto/ ./proto/

# 生成 gRPC 代码（如果尚未生成）
RUN python3 -m grpc_tools.protoc \
    -I./proto \
    --python_out=./worker/proto_gen \
    --pyi_out=./worker/proto_gen \
    --grpc_python_out=./worker/proto_gen \
    ./proto/ai_service.proto \
    && sed -i 's/^import ai_service_pb2/from . import ai_service_pb2/' ./worker/proto_gen/ai_service_pb2_grpc.py \
    || echo "Proto files already generated"

# 创建模型和检查点目录
RUN mkdir -p /models /checkpoints /etc/worker \
    && chown -R ewai:ewai /app /models /checkpoints /etc/worker

USER ewai

# 健康检查（通过 gRPC 健康检查端点）
# start-period: 10 分钟，支持大模型加载
HEALTHCHECK --interval=30s --timeout=30s --start-period=600s --retries=5 \
    CMD python3 -c "import grpc; from worker.proto_gen import AiInferenceStub, WorkerHealthRequest; \
        ch = grpc.insecure_channel('localhost:50051'); \
        stub = AiInferenceStub(ch); \
        resp = stub.Health(WorkerHealthRequest()); \
        exit(0 if resp.healthy else 1)" || exit 1

# 暴露 gRPC 端口
EXPOSE 50051

# 环境变量（可在运行时覆盖）
ENV CY_LLM_BACKEND=nvidia
ENV CY_LLM_MODEL_REGISTRY=/etc/worker/models.json
ENV CY_LLM_INTERNAL_TOKEN=""

# 安全默认值：生产环境强制内部 token；若仅本地开发需要放行，请显式设置 CY_LLM_ALLOW_INSECURE_INTERNAL_TOKEN=true
ENV CY_LLM_ENV=production
ENV CY_LLM_ALLOW_INSECURE_INTERNAL_TOKEN=false

# 启动命令
CMD ["python3", "-m", "worker.main", "--serve", "--port", "50051"]