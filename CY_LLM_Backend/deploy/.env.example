# =============================================================================
# CY-LLM Engine - 环境变量配置
# 复制此文件为 .env 并根据需要修改
# 用法: cp .env.example .env && vim .env
# =============================================================================

# ========== 引擎选择 ==========
# 可选值: cuda-vllm, cuda-trt, ascend-vllm, ascend-mindie
CY_LLM_ENGINE=cuda-vllm
## (旧变量 EW_* 已移除；请使用 CY_LLM_* 前缀)

# ========== 服务端口 ==========
CY_LLM_PORT=8080
CY_LLM_WORKER_PORT=50051

# ========== Python 环境 ==========
CY_LLM_CONDA_ENV=ew_ai_worker
CY_LLM_PYTHON_VERSION=3.10

# ========== 模型配置 ==========
# 默认模型 ID
CY_LLM_MODEL=deepseek-v3
# 模型缓存目录
CY_LLM_MODEL_CACHE=~/.cache/cy-llm/models
# 检查点目录
CY_LLM_CHECKPOINT_DIR=~/.cache/cy-llm/checkpoints

# ========== vLLM 配置 (cuda-vllm / ascend-vllm) ==========
# 张量并行度 (多卡推理)
VLLM_TP=1
# GPU 显存使用率 (0.0-1.0)
VLLM_GPU_MEM=0.9

# ========== 安全配置 ==========
CY_LLM_SECURITY_ENABLED=true
# API 密钥 (客户端使用)
# 生成命令: openssl rand -hex 32
CY_LLM_API_KEY=your-api-key-change-me
# 内部通信令牌 (Gateway <-> Worker)
CY_LLM_INTERNAL_TOKEN=your-internal-token-change-me

# ========== Docker 配置 ==========
CY_LLM_WORKER_HOST=worker-nvidia
ASCEND_VISIBLE_DEVICES=0

# ========== 资源限制 ==========
JAVA_OPTS=-XX:+UseContainerSupport -XX:MaxRAMPercentage=75.0 -XX:+UseG1GC
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4

# ========== HuggingFace (可选) ==========
HF_HOME=/models/huggingface
# HF_TOKEN=your-huggingface-token

