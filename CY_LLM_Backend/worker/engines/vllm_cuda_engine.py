"""
vllm_cuda_engine.py
[æ¨ç†å¼•æ“] NVIDIA CUDA + vLLM å®ç°
è¯´æ˜ï¼šåŸºäº vLLM çš„é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œæ”¯æŒ PagedAttentionã€Continuous Batching ç­‰ç‰¹æ€§ã€‚

ç‰¹ç‚¹ï¼š
  - PagedAttentionï¼šæ˜¾å­˜åˆ©ç”¨ç‡é«˜ï¼Œæ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡
  - Continuous Batchingï¼šåŠ¨æ€æ‰¹å¤„ç†ï¼Œé«˜å¹¶å‘åå
  - å¤š LoRA çƒ­åˆ‡æ¢ï¼šæ”¯æŒåŒä¸€åŸºåº§æ¨¡å‹æŒ‚è½½å¤šä¸ªé€‚é…å™¨
  - OpenAI å…¼å®¹ï¼šå¯ç›´æ¥ä½¿ç”¨ vLLM çš„ API Server

ä¾èµ–ï¼š
  pip install vllm
"""

from __future__ import annotations

import gc
import logging
from typing import Any, Dict, Generator, List, Optional

from .abstract_engine import BaseEngine

LOGGER = logging.getLogger("cy_llm.worker.engines.vllm_cuda")

# å»¶è¿Ÿå¯¼å…¥ vLLMï¼Œä»…åœ¨å®é™…ä½¿ç”¨æ—¶åŠ è½½
_vllm_imported = False
_LLM = None
_SamplingParams = None


def _ensure_vllm_imported():
    """ç¡®ä¿ vLLM å·²å¯¼å…¥"""
    global _vllm_imported, _LLM, _SamplingParams
    if not _vllm_imported:
        try:
            from vllm import LLM, SamplingParams
            _LLM = LLM
            _SamplingParams = SamplingParams
            _vllm_imported = True
            LOGGER.info("vLLM æ¨¡å—åŠ è½½æˆåŠŸ")
        except ImportError as e:
            raise ImportError(
                "vLLM æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install vllm\n"
                f"åŸå§‹é”™è¯¯: {e}"
            ) from e


class VllmCudaEngine(BaseEngine):
    """
    åŸºäº vLLM çš„ NVIDIA CUDA æ¨ç†å¼•æ“ã€‚
    
    ç‰¹ç‚¹ï¼š
      - é«˜æ€§èƒ½ï¼šPagedAttention + Continuous Batching
      - å¤š LoRAï¼šæ”¯æŒåŠ¨æ€åˆ‡æ¢å¤šä¸ª LoRA é€‚é…å™¨
      - é‡åŒ–æ”¯æŒï¼šAWQ, GPTQ, FP8 ç­‰
      
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        >>> engine = VllmCudaEngine()
        >>> engine.load_model("deepseek-ai/deepseek-llm-7b-chat")
        >>> for token in engine.infer("ä½ å¥½"):
        ...     print(token, end="", flush=True)
    """

    def __init__(
        self,
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.75,  # ä¿®æ”¹: 0.90 -> 0.75
        max_model_len: Optional[int] = None,
        quantization: Optional[str] = None,
        enable_lora: bool = True,
        max_loras: int = 4,
        enable_prefix_caching: bool = False,
        kv_cache_dtype: Optional[str] = None,
        allow_auto_tuning: bool = True,
        **kwargs
    ) -> None:
        """
        åˆå§‹åŒ– vLLM CUDA å¼•æ“ã€‚

        Args:
            tensor_parallel_size: å¼ é‡å¹¶è¡Œæ•°ï¼ˆå¤šå¡æ—¶ä½¿ç”¨ï¼‰
            gpu_memory_utilization: GPU æ˜¾å­˜ä½¿ç”¨ç‡ä¸Šé™ï¼ˆé»˜è®¤ 0.75ï¼Œæ¨è 0.70-0.85ï¼‰
            max_model_len: æœ€å¤§åºåˆ—é•¿åº¦ï¼ŒNone åˆ™è‡ªåŠ¨æ£€æµ‹
            quantization: é‡åŒ–æ–¹æ³•ï¼Œå¦‚ "awq", "gptq", "fp8"ï¼ˆvLLM ä¸æ”¯æŒ bitsandbytesï¼‰
            enable_lora: æ˜¯å¦å¯ç”¨ LoRA æ”¯æŒ
            max_loras: æœ€å¤§åŒæ—¶åŠ è½½çš„ LoRA æ•°é‡
            enable_prefix_caching: å¯ç”¨å‰ç¼€ç¼“å­˜ï¼ˆAutomatic Prefix Cachingï¼‰
            kv_cache_dtype: KV Cache æ•°æ®ç±»å‹ï¼Œ"auto" æˆ– "fp8"
            allow_auto_tuning: æ˜¯å¦å…è®¸è‡ªåŠ¨è°ƒæ•´å‚æ•°ä»¥é¿å… OOMï¼ˆé»˜è®¤ Trueï¼‰
        """
        _ensure_vllm_imported()

        self.allow_auto_tuning = allow_auto_tuning

        # éªŒè¯ gpu_memory_utilization å®‰å…¨æ€§
        if gpu_memory_utilization > 0.90:
            if allow_auto_tuning:
                LOGGER.warning(
                    "gpu_memory_utilization=%.2f è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´ OOMã€‚è‡ªåŠ¨è°ƒæ•´ä¸º 0.85",
                    gpu_memory_utilization
                )
                gpu_memory_utilization = 0.85
            else:
                LOGGER.warning(
                    "gpu_memory_utilization=%.2f è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´ OOMï¼ˆallow_auto_tuning=Falseï¼Œä¿æŒåŸå€¼ï¼‰",
                    gpu_memory_utilization
                )

        self.tensor_parallel_size = tensor_parallel_size
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.quantization = quantization
        self.enable_lora = enable_lora
        self.max_loras = max_loras
        self.enable_prefix_caching = enable_prefix_caching
        self.kv_cache_dtype = kv_cache_dtype
        self.extra_kwargs = kwargs

        self._llm: Optional[Any] = None  # vLLM LLM å®ä¾‹
        self._model_path: Optional[str] = None
        self._loaded_loras: Dict[str, str] = {}  # lora_name -> lora_path

        LOGGER.info(
            "VllmCudaEngine åˆå§‹åŒ–: tp=%d, mem=%.2f, quant=%s, lora=%s, prefix_cache=%s, kv_dtype=%s",
            tensor_parallel_size, gpu_memory_utilization, quantization, enable_lora,
            enable_prefix_caching, kv_cache_dtype
        )

    def load_model(
        self,
        model_path: str,
        adapter_path: Optional[str] = None,
        skip_vram_check: bool = False,
        **kwargs
    ) -> None:
        """
        åŠ è½½æ¨¡å‹åˆ° GPUã€‚
        
        Args:
            model_path: HuggingFace æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
            adapter_path: LoRA é€‚é…å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            skip_vram_check: è·³è¿‡ VRAM é¢„æ£€æŸ¥ï¼ˆé»˜è®¤ Falseï¼‰
            **kwargs: é¢å¤–å‚æ•°ä¼ é€’ç»™ vLLM
                - max_model_len: è¦†ç›–å®ä¾‹é…ç½®çš„æœ€å¤§åºåˆ—é•¿åº¦
                - tensor_parallel_size: è¦†ç›–å®ä¾‹é…ç½®çš„å¼ é‡å¹¶è¡Œæ•°
                - enable_prefix_caching: è¦†ç›–å®ä¾‹é…ç½®çš„å‰ç¼€ç¼“å­˜
                - kv_cache_dtype: è¦†ç›–å®ä¾‹é…ç½®çš„ KV Cache æ•°æ®ç±»å‹
                - gpu_memory_utilization: è¦†ç›–å®ä¾‹é…ç½®çš„æ˜¾å­˜åˆ©ç”¨ç‡
        """
        LOGGER.info("æ­£åœ¨åŠ è½½æ¨¡å‹: %s", model_path)
        
        # === VRAM é¢„æ£€æŸ¥ ===
        if not skip_vram_check:
            try:
                from worker.utils.vram_optimizer import (
                    estimate_vram_requirements,
                    optimize_vram_config,
                    format_vram_report,
                )

                estimate = estimate_vram_requirements(
                    model_name_or_params=model_path,
                    max_model_len=self.max_model_len or 2048,
                    dtype="fp16",
                    quantization=self.quantization,
                    engine_type="vllm",
                    tensor_parallel_size=self.tensor_parallel_size,
                )

                # æ˜¾ç¤ºè¯¦ç»†çš„ VRAM ä¼°ç®—æŠ¥å‘Š
                LOGGER.info("\n%s", format_vram_report(estimate, verbose=True))

                if not estimate.is_safe:
                    if not self.allow_auto_tuning:
                        LOGGER.warning(
                            "âš ï¸  VRAM ä¸è¶³ï¼Œä½† allow_auto_tuning=Falseï¼Œä¿æŒåŸå§‹é…ç½®åŠ è½½"
                        )
                        # ä»æ˜¾ç¤ºå»ºè®®ï¼Œä½†ä¸è‡ªåŠ¨è°ƒæ•´
                        if estimate.suggestions:
                            LOGGER.warning("ğŸ’¡ å»ºè®®:")
                            for suggestion in estimate.suggestions:
                                LOGGER.warning("   - %s", suggestion)
                    else:
                        # å°è¯•ä¼˜åŒ–é…ç½®
                        current_config = {
                            "gpu_memory_utilization": self.gpu_memory_utilization,
                            "max_model_len": self.max_model_len or 2048,
                        }
                        optimized = optimize_vram_config(estimate, current_config)

                        # åº”ç”¨ä¼˜åŒ–åçš„é…ç½®
                        if "gpu_memory_utilization" in optimized:
                            old_util = self.gpu_memory_utilization
                            self.gpu_memory_utilization = optimized["gpu_memory_utilization"]
                            LOGGER.warning(
                                "âš™ï¸  è‡ªåŠ¨è°ƒæ•´ gpu_memory_utilization: %.2f -> %.2f",
                                old_util, self.gpu_memory_utilization
                            )

                        if "max_model_len" in optimized and self.max_model_len != optimized["max_model_len"]:
                            old_len = self.max_model_len or 2048
                            self.max_model_len = optimized["max_model_len"]
                            LOGGER.warning(
                                "âš™ï¸  è‡ªåŠ¨è°ƒæ•´ max_model_len: %d -> %d",
                                old_len, self.max_model_len
                            )

                        # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
                        if estimate.suggestions:
                            LOGGER.warning("ğŸ’¡ å…¶ä»–å»ºè®®:")
                            for suggestion in estimate.suggestions:
                                LOGGER.warning("   - %s", suggestion)

            except ImportError:
                LOGGER.warning("vram_optimizer æœªæ‰¾åˆ°ï¼Œè·³è¿‡ VRAM æ£€æŸ¥")
        
        # ä» kwargs æå–å¯è¦†ç›–çš„é…ç½®
        max_model_len = kwargs.pop("max_model_len", None) or self.max_model_len
        tensor_parallel_size = kwargs.pop("tensor_parallel_size", None) or self.tensor_parallel_size
        enable_prefix_caching = kwargs.pop("enable_prefix_caching", None)
        if enable_prefix_caching is None:
            enable_prefix_caching = self.enable_prefix_caching
        kv_cache_dtype = kwargs.pop("kv_cache_dtype", None) or self.kv_cache_dtype
        gpu_memory_utilization = kwargs.pop("gpu_memory_utilization", None) or self.gpu_memory_utilization
        
        # åˆå¹¶é…ç½®
        llm_kwargs = {
            "model": model_path,
            "tensor_parallel_size": tensor_parallel_size,
            "gpu_memory_utilization": gpu_memory_utilization,
            "trust_remote_code": True,
            "dtype": "auto",
        }
        
        if max_model_len:
            llm_kwargs["max_model_len"] = max_model_len

        if self.enable_lora:
            llm_kwargs["enable_lora"] = True
            llm_kwargs["max_loras"] = self.max_loras
            llm_kwargs["max_lora_rank"] = kwargs.get("max_lora_rank", 64)
        
        # KV Cache ä¼˜åŒ–é…ç½®
        if enable_prefix_caching:
            llm_kwargs["enable_prefix_caching"] = True
            LOGGER.info("å¯ç”¨ Automatic Prefix Caching (APC)")
            
        if kv_cache_dtype:
            llm_kwargs["kv_cache_dtype"] = kv_cache_dtype
            LOGGER.info("KV Cache æ•°æ®ç±»å‹: %s", kv_cache_dtype)
            
        # é‡åŒ–é…ç½®ï¼ˆvLLM æ”¯æŒ AWQ/GPTQ/FP8ï¼‰
        if self.quantization:
            valid_vllm_quant = ["awq", "gptq", "fp8", "fp8_e5m2"]
            if self.quantization.lower() in valid_vllm_quant:
                llm_kwargs["quantization"] = self.quantization
                LOGGER.info("ä½¿ç”¨é‡åŒ–æ–¹æ³•: %s", self.quantization)
            else:
                raise ValueError(
                    f"vLLM ä¸æ”¯æŒé‡åŒ–æ–¹æ³• '{self.quantization}'ã€‚"
                    f"æ”¯æŒçš„æ–¹æ³•: {', '.join(valid_vllm_quant)}ã€‚"
                    f"å¦‚éœ€ä½¿ç”¨ bitsandbytesï¼Œè¯·åˆ‡æ¢åˆ° nvidia å¼•æ“ã€‚"
                )

        # åˆå¹¶é¢å¤–å‚æ•°
        llm_kwargs.update(self.extra_kwargs)
        llm_kwargs.update(kwargs)

        # === OOM è‡ªåŠ¨é‡è¯•æœºåˆ¶ ===
        # ç”Ÿæˆæ¸è¿›å¼é™çº§é…ç½®
        base_config = {
            "gpu_memory_utilization": gpu_memory_utilization,
            "max_model_len": max_model_len,
        }

        try:
            from worker.utils.vram_optimizer import progressive_retry_configs
            retry_configs = progressive_retry_configs(base_config)
        except ImportError:
            # å¦‚æœ vram_optimizer ä¸å¯ç”¨ï¼Œåªä½¿ç”¨åŸå§‹é…ç½®
            retry_configs = [base_config]

        last_error = None
        for attempt, config in enumerate(retry_configs, start=1):
            try:
                # åº”ç”¨å½“å‰é…ç½®åˆ° llm_kwargs
                current_kwargs = llm_kwargs.copy()
                current_kwargs["gpu_memory_utilization"] = config.get(
                    "gpu_memory_utilization", gpu_memory_utilization
                )
                if "max_model_len" in config and config["max_model_len"]:
                    current_kwargs["max_model_len"] = config["max_model_len"]

                if attempt > 1:
                    LOGGER.warning(
                        "ğŸ”„ OOM é‡è¯• [%d/%d]: gpu_mem_util=%.2f, max_model_len=%s",
                        attempt,
                        len(retry_configs),
                        current_kwargs["gpu_memory_utilization"],
                        current_kwargs.get("max_model_len", "auto"),
                    )

                # åŠ è½½è¿›åº¦åé¦ˆ
                LOGGER.info("â³ å¼€å§‹åŠ è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
                LOGGER.info("ğŸ“¥ æ­¥éª¤ 1/3: åˆå§‹åŒ– vLLM å¼•æ“é…ç½®")

                import time
                start_time = time.time()

                # å°è¯•åŠ è½½æ¨¡å‹
                LOGGER.info("ğŸ“¦ æ­¥éª¤ 2/3: åŠ è½½æ¨¡å‹æƒé‡åˆ° GPU æ˜¾å­˜")
                self._llm = _LLM(**current_kwargs)

                LOGGER.info("ğŸ”§ æ­¥éª¤ 3/3: åˆå§‹åŒ– KV Cache å’Œæ¨ç†å¼•æ“")
                elapsed = time.time() - start_time
                LOGGER.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
                self._model_path = model_path

                if attempt > 1:
                    LOGGER.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆç¬¬ %d æ¬¡å°è¯•ï¼‰: %s", attempt, model_path)
                else:
                    LOGGER.info("æ¨¡å‹åŠ è½½æˆåŠŸ: %s", model_path)

                # å¦‚æœæä¾›äº† LoRA é€‚é…å™¨ï¼ŒåŠ è½½å®ƒ
                if adapter_path:
                    self.load_lora(adapter_path, "default")

                # æˆåŠŸåŠ è½½ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                break

            except Exception as e:
                last_error = e
                error_msg = str(e).lower()

                # æ£€æŸ¥æ˜¯å¦æ˜¯ OOM é”™è¯¯
                is_oom = (
                    "out of memory" in error_msg
                    or "cuda error" in error_msg
                    or "cuda out of memory" in error_msg
                    or "allocate" in error_msg
                )

                if is_oom and attempt < len(retry_configs):
                    LOGGER.warning("âš ï¸  æ˜¾å­˜ä¸è¶³ (OOM)ï¼Œå‡†å¤‡ä½¿ç”¨æ›´ä¿å®ˆçš„é…ç½®é‡è¯•...")
                    # æ¸…ç†æ˜¾å­˜
                    try:
                        import torch
                        torch.cuda.empty_cache()
                        gc.collect()
                    except Exception:
                        pass
                    # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
                    continue
                else:
                    # é OOM é”™è¯¯ï¼Œæˆ–å·²ç”¨å°½æ‰€æœ‰é‡è¯•é…ç½®
                    if attempt == len(retry_configs):
                        LOGGER.error(
                            "âŒ æ‰€æœ‰ %d ä¸ªé…ç½®å‡å¤±è´¥ï¼Œæ— æ³•åŠ è½½æ¨¡å‹: %s",
                            len(retry_configs),
                            last_error,
                        )
                    else:
                        LOGGER.error("æ¨¡å‹åŠ è½½å¤±è´¥: %s", e)
                    raise

    def load_lora(self, adapter_path: str, lora_name: str = "default") -> None:
        """
        åŠ è½½ LoRA é€‚é…å™¨ã€‚
        
        Args:
            adapter_path: LoRA é€‚é…å™¨è·¯å¾„
            lora_name: é€‚é…å™¨åç§°ï¼ˆç”¨äºåç»­åˆ‡æ¢ï¼‰
        """
        if not self.enable_lora:
            raise RuntimeError("LoRA æœªå¯ç”¨ï¼Œè¯·åœ¨åˆå§‹åŒ–æ—¶è®¾ç½® enable_lora=True")
            
        if self._llm is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ load_model() åŠ è½½åŸºåº§æ¨¡å‹")
            
        LOGGER.info("åŠ è½½ LoRA é€‚é…å™¨: %s as '%s'", adapter_path, lora_name)
        # vLLM çš„ LoRA é€šè¿‡è¯·æ±‚æ—¶æŒ‡å®šï¼Œè¿™é‡Œåªè®°å½•è·¯å¾„
        self._loaded_loras[lora_name] = adapter_path

    def infer(
        self,
        prompt: str,
        lora_name: Optional[str] = None,
        **kwargs
    ) -> Generator[str, None, None]:
        """
        æµå¼æ¨ç†ã€‚
        
        Args:
            prompt: è¾“å…¥æç¤º
            lora_name: ä½¿ç”¨çš„ LoRA é€‚é…å™¨åç§°
            **kwargs: ç”Ÿæˆå‚æ•°ï¼ˆtemperature, top_p, max_tokens ç­‰ï¼‰
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ token
        """
        if self._llm is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
            
        # æ„å»ºé‡‡æ ·å‚æ•°
        sampling_params = _SamplingParams(
            temperature=kwargs.get("temperature", 0.7),
            top_p=kwargs.get("top_p", 0.9),
            max_tokens=kwargs.get("max_new_tokens", kwargs.get("max_tokens", 512)),
            repetition_penalty=kwargs.get("repetition_penalty", 1.1),
            stop=kwargs.get("stop", None),
        )
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_kwargs = {"sampling_params": sampling_params}
        
        # å¦‚æœæŒ‡å®šäº† LoRA
        if lora_name and lora_name in self._loaded_loras:
            from vllm.lora.request import LoRARequest
            lora_path = self._loaded_loras[lora_name]
            # vLLM ä½¿ç”¨ LoRARequest æŒ‡å®šé€‚é…å™¨
            request_kwargs["lora_request"] = LoRARequest(
                lora_name=lora_name,
                lora_int_id=hash(lora_name) % (2**31),
                lora_local_path=lora_path,
            )
        
        # vLLM çš„ generate æ˜¯åŒæ­¥çš„ï¼Œä½†è¿”å›å®Œæ•´è¾“å‡º
        # ä¸ºäº†å…¼å®¹æµå¼æ¥å£ï¼Œæˆ‘ä»¬é€å­—ç¬¦ yield
        # æ³¨æ„ï¼švLLM æœ¬èº«æ”¯æŒçœŸæ­£çš„æµå¼ï¼Œä½†éœ€è¦ä½¿ç”¨ AsyncLLMEngine
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåç»­å¯å‡çº§ä¸º AsyncLLMEngine
        outputs = self._llm.generate([prompt], **request_kwargs)
        
        if outputs and len(outputs) > 0:
            generated_text = outputs[0].outputs[0].text
            # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ˆå®é™…ç”Ÿäº§ä¸­åº”ä½¿ç”¨ AsyncLLMEngineï¼‰
            for char in generated_text:
                yield char

    def infer_batch(
        self,
        prompts: List[str],
        **kwargs
    ) -> List[str]:
        """
        æ‰¹é‡æ¨ç†ï¼ˆvLLM çš„å¼ºé¡¹ï¼‰ã€‚
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            **kwargs: ç”Ÿæˆå‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        if self._llm is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
            
        sampling_params = _SamplingParams(
            temperature=kwargs.get("temperature", 0.7),
            top_p=kwargs.get("top_p", 0.9),
            max_tokens=kwargs.get("max_new_tokens", kwargs.get("max_tokens", 512)),
            repetition_penalty=kwargs.get("repetition_penalty", 1.1),
        )
        
        outputs = self._llm.generate(prompts, sampling_params)
        return [output.outputs[0].text for output in outputs]

    def unload_model(self) -> None:
        """å¸è½½æ¨¡å‹å¹¶é‡Šæ”¾æ˜¾å­˜ã€‚"""
        if self._llm is not None:
            LOGGER.info("æ­£åœ¨å¸è½½æ¨¡å‹...")
            del self._llm
            self._llm = None
            self._model_path = None
            self._loaded_loras.clear()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            try:
                import torch
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            except Exception:
                pass
                
            LOGGER.info("æ¨¡å‹å·²å¸è½½")

    def get_memory_usage(self) -> Dict[str, float]:
        """è¿”å›å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µã€‚"""
        try:
            import torch
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / (1024 ** 3)
                reserved = torch.cuda.memory_reserved() / (1024 ** 3)
                total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
                return {
                    "allocated_gb": round(allocated, 2),
                    "reserved_gb": round(reserved, 2),
                    "total_gb": round(total, 2),
                    "utilization": round(allocated / total, 4) if total > 0 else 0,
                }
        except Exception as e:
            LOGGER.warning("è·å–æ˜¾å­˜ä¿¡æ¯å¤±è´¥: %s", e)
            
        return {"allocated_gb": 0, "reserved_gb": 0, "total_gb": 0, "utilization": 0}

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰åŠ è½½çš„æ¨¡å‹ä¿¡æ¯ã€‚"""
        return {
            "model_path": self._model_path,
            "engine": "vllm-cuda",
            "tensor_parallel_size": self.tensor_parallel_size,
            "quantization": self.quantization,
            "loaded_loras": list(self._loaded_loras.keys()),
            "is_loaded": self._llm is not None,
        }
