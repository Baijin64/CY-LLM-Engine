"""
CY-LLM Engine Dependency Management System

æä¾›ç¡¬ä»¶æ£€æµ‹ã€ä¾èµ–è§£æå’Œå®‰è£…æ¨èåŠŸèƒ½ã€‚
"""

import json
import platform
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum


class HardwareVendor(Enum):
    NVIDIA = "nvidia"
    HUAWEI = "huawei"
    AMD = "amd"
    CPU = "cpu"
    UNKNOWN = "unknown"


@dataclass
class HardwareProfile:
    """ç¡¬ä»¶é…ç½®ä¿¡æ¯"""

    vendor: HardwareVendor
    device_name: str
    compute_capability: Optional[str] = None
    vram_gb: Optional[float] = None
    driver_version: Optional[str] = None
    cuda_version: Optional[str] = None


@dataclass
class DependencyProfile:
    """ä¾èµ–é…ç½®ä¿¡æ¯"""

    profile_id: str
    hardware: List[str]
    engine: str
    engine_version: str
    python: str
    dependencies: Dict[str, List[str]]
    env_vars: Dict[str, str]
    warnings: List[str]


class HardwareDetector:
    """ç¡¬ä»¶æ£€æµ‹å™¨ - è‡ªåŠ¨æ£€æµ‹GPU/NPUç±»å‹"""

    def __init__(self):
        self.registry_path = Path(__file__).parent.parent / "deploy" / "dependency_registry.json"
        self.registry = self._load_registry()

    def _load_registry(self) -> Dict:
        """åŠ è½½ä¾èµ–æ³¨å†Œè¡¨"""
        if not self.registry_path.exists():
            raise FileNotFoundError(f"Registry not found: {self.registry_path}")
        with open(self.registry_path, "r") as f:
            return json.load(f)

    def detect(self) -> HardwareProfile:
        """æ£€æµ‹å½“å‰ç¡¬ä»¶é…ç½®"""
        # å°è¯•æ£€æµ‹NVIDIA GPU
        nvidia = self._detect_nvidia()
        if nvidia:
            return nvidia

        # å°è¯•æ£€æµ‹åä¸ºAscend
        ascend = self._detect_ascend()
        if ascend:
            return ascend

        # å›é€€åˆ°CPU
        return HardwareProfile(vendor=HardwareVendor.CPU, device_name="CPU Only", vram_gb=0.0)

    def _detect_nvidia(self) -> Optional[HardwareProfile]:
        """æ£€æµ‹NVIDIA GPU"""
        try:
            result = subprocess.run(
                [
                    "nvidia-smi",
                    "--query-gpu=name,compute_cap,memory.total,driver_version",
                    "--format=csv,noheader",
                ],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode != 0:
                return None

            lines = result.stdout.strip().split("\n")
            if not lines:
                return None

            # è§£æç¬¬ä¸€è¡Œ
            parts = [p.strip() for p in lines[0].split(",")]
            if len(parts) >= 4:
                device_name = parts[0]
                compute_cap = parts[1]
                memory_str = parts[2]
                driver_version = parts[3]

                # è§£ææ˜¾å­˜
                vram_gb = 0.0
                if "MiB" in memory_str:
                    vram_gb = float(memory_str.replace("MiB", "").strip()) / 1024
                elif "GiB" in memory_str:
                    vram_gb = float(memory_str.replace("GiB", "").strip())

                # è·å–CUDAç‰ˆæœ¬
                cuda_version = self._get_cuda_version()

                return HardwareProfile(
                    vendor=HardwareVendor.NVIDIA,
                    device_name=device_name,
                    compute_capability=compute_cap,
                    vram_gb=vram_gb,
                    driver_version=driver_version,
                    cuda_version=cuda_version,
                )
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        return None

    def _detect_ascend(self) -> Optional[HardwareProfile]:
        """æ£€æµ‹åä¸ºAscend NPU"""
        try:
            result = subprocess.run(["npu-smi", "info"], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                return HardwareProfile(
                    vendor=HardwareVendor.HUAWEI, device_name="Ascend NPU", compute_capability=None
                )
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        return None

    def _get_cuda_version(self) -> Optional[str]:
        """è·å–CUDAç‰ˆæœ¬"""
        try:
            result = subprocess.run(
                ["nvcc", "--version"], capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                for line in result.stdout.split("\n"):
                    if "release" in line:
                        parts = line.split()
                        for i, part in enumerate(parts):
                            if part == "release":
                                return parts[i + 1].rstrip(",")
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        return None

    def get_hardware_profile_id(self, hw: HardwareProfile) -> Optional[str]:
        """æ ¹æ®ç¡¬ä»¶é…ç½®è·å–profile ID"""
        hw_profiles = self.registry.get("hardware_profiles", {})

        if hw.vendor == HardwareVendor.NVIDIA and hw.compute_capability:
            cap = float(hw.compute_capability)
            if cap >= 8.9:
                return "nvidia_ada"
            elif cap >= 8.0:
                return "nvidia_ampere"
            elif cap >= 7.5:
                return "nvidia_turing"
        elif hw.vendor == HardwareVendor.HUAWEI:
            return "ascend_910b"
        elif hw.vendor == HardwareVendor.CPU:
            return "cpu_only"

        return None


class DependencyResolver:
    """ä¾èµ–è§£æå™¨ - æ ¹æ®ç¡¬ä»¶+å¼•æ“æ¨èä¾èµ–é…ç½®"""

    def __init__(self):
        self.registry_path = Path(__file__).parent.parent / "deploy" / "dependency_registry.json"
        self.registry = self._load_registry()

    def _load_registry(self) -> Dict:
        """åŠ è½½ä¾èµ–æ³¨å†Œè¡¨"""
        with open(self.registry_path, "r") as f:
            return json.load(f)

    def resolve(
        self, hardware_id: str, engine: str, engine_version: Optional[str] = None
    ) -> Optional[DependencyProfile]:
        """
        è§£æä¾èµ–é…ç½®

        Args:
            hardware_id: ç¡¬ä»¶profile ID (e.g., "nvidia_ampere")
            engine: å¼•æ“åç§° (e.g., "vllm")
            engine_version: å¼•æ“ç‰ˆæœ¬ (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ³¨å†Œè¡¨ä¸­çš„ç‰ˆæœ¬)

        Returns:
            DependencyProfile æˆ– None
        """
        matrix = self.registry.get("compatibility_matrix", [])

        for entry in matrix:
            hw_match = hardware_id in entry.get("hardware", [])
            engine_match = entry.get("engine") == engine

            if hw_match and engine_match:
                if engine_version is None or entry.get("engine_version") == engine_version:
                    return DependencyProfile(
                        profile_id=entry.get("profile_id", ""),
                        hardware=entry.get("hardware", []),
                        engine=entry.get("engine", ""),
                        engine_version=entry.get("engine_version", ""),
                        python=entry.get("python", ">=3.10"),
                        dependencies=entry.get("dependencies", {}),
                        env_vars=entry.get("env_vars", {}),
                        warnings=entry.get("warnings", []),
                    )

        return None

    def list_available_profiles(self, hardware_id: Optional[str] = None) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¾èµ–é…ç½®"""
        matrix = self.registry.get("compatibility_matrix", [])

        if hardware_id:
            return [m for m in matrix if hardware_id in m.get("hardware", [])]
        return matrix

    def generate_requirements(
        self, profile: DependencyProfile, mirror: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆrequirements.txtå†…å®¹

        Args:
            profile: ä¾èµ–é…ç½®
            mirror: é•œåƒæºåç§° (tsinghua/aliyun/douban)

        Returns:
            requirements.txt å†…å®¹
        """
        lines = []

        # æ·»åŠ é•œåƒæº
        if mirror:
            mirrors = self.registry.get("mirrors", {})
            if mirror in mirrors:
                m = mirrors[mirror]
                lines.append(f"--index-url {m['url']}")
                lines.append(f"--trusted-host {m['trusted_host']}")
                lines.append("")

        # æ·»åŠ åŸºç¡€ä¾èµ–
        base_deps = self.registry.get("base_dependencies", [])
        lines.append("# Base Dependencies")
        for dep in base_deps:
            lines.append(dep)
        lines.append("")

        # æ·»åŠ å¼•æ“ç‰¹å®šä¾èµ–
        for category, deps in profile.dependencies.items():
            lines.append(f"# {category.title()} Dependencies")
            for dep in deps:
                lines.append(dep)
            lines.append("")

        return "\n".join(lines)

    def check_python_compatibility(self, profile: DependencyProfile) -> Tuple[bool, str]:
        """æ£€æŸ¥Pythonç‰ˆæœ¬å…¼å®¹æ€§"""
        current = platform.python_version()
        requirement = profile.python

        # ç®€åŒ–çš„ç‰ˆæœ¬æ£€æŸ¥
        if ">=" in requirement and "<" in requirement:
            # æ ¼å¼: >=3.10,<3.13
            min_ver = requirement.split(">=")[1].split(",")[0]
            max_ver = requirement.split("<")[1]

            current_tuple = tuple(map(int, current.split(".")[:2]))
            min_tuple = tuple(map(int, min_ver.split(".")[:2]))
            max_tuple = tuple(map(int, max_ver.split(".")[:2]))

            if current_tuple < min_tuple:
                return False, f"Python {current} ä½äºæœ€ä½è¦æ±‚ {min_ver}"
            if current_tuple >= max_tuple:
                return False, f"Python {current} é«˜äºæœ€é«˜è¦æ±‚ {max_ver}"

            return True, f"Python {current} ç¬¦åˆè¦æ±‚ {requirement}"

        return True, "æ— æ³•è§£æç‰ˆæœ¬è¦æ±‚ï¼Œå‡è®¾å…¼å®¹"


def main():
    """CLIå…¥å£ç‚¹"""
    import argparse

    parser = argparse.ArgumentParser(description="CY-LLM Dependency Manager")
    parser.add_argument("command", choices=["detect", "list", "resolve", "generate"])
    parser.add_argument("--hardware", help="Hardware profile ID")
    parser.add_argument("--engine", help="Engine name")
    parser.add_argument("--mirror", help="Mirror name (tsinghua/aliyun/douban)")
    parser.add_argument("--output", help="Output file path")

    args = parser.parse_args()

    if args.command == "detect":
        print("ğŸ” æ£€æµ‹ç¡¬ä»¶é…ç½®...")
        detector = HardwareDetector()
        hw = detector.detect()
        print(f"  å‚å•†: {hw.vendor.value}")
        print(f"  è®¾å¤‡: {hw.device_name}")
        if hw.compute_capability:
            print(f"  è®¡ç®—èƒ½åŠ›: {hw.compute_capability}")
        if hw.vram_gb:
            print(f"  æ˜¾å­˜: {hw.vram_gb:.1f} GB")
        if hw.driver_version:
            print(f"  é©±åŠ¨: {hw.driver_version}")
        if hw.cuda_version:
            print(f"  CUDA: {hw.cuda_version}")

        profile_id = detector.get_hardware_profile_id(hw)
        if profile_id:
            print(f"\nğŸ“‹ ç¡¬ä»¶Profile ID: {profile_id}")

    elif args.command == "list":
        resolver = DependencyResolver()
        profiles = resolver.list_available_profiles(args.hardware)
        print(f"å¯ç”¨é…ç½® ({len(profiles)} ä¸ª):")
        for p in profiles:
            print(f"  - {p['profile_id']}: {p['engine']} {p['engine_version']}")

    elif args.command == "resolve":
        if not args.hardware or not args.engine:
            print("é”™è¯¯: --hardware å’Œ --engine å‚æ•°å¿…éœ€")
            return

        resolver = DependencyResolver()
        profile = resolver.resolve(args.hardware, args.engine)

        if profile:
            print(f"âœ… æ‰¾åˆ°é…ç½®: {profile.profile_id}")
            print(f"   å¼•æ“: {profile.engine} {profile.engine_version}")
            print(f"   Python: {profile.python}")

            ok, msg = resolver.check_python_compatibility(profile)
            print(f"   Pythonå…¼å®¹æ€§: {'âœ…' if ok else 'âŒ'} {msg}")

            if profile.warnings:
                print("\nâš ï¸  è­¦å‘Š:")
                for w in profile.warnings:
                    print(f"   - {w}")
        else:
            print(f"âŒ æœªæ‰¾åˆ° {args.hardware} + {args.engine} çš„é…ç½®")

    elif args.command == "generate":
        if not args.hardware or not args.engine:
            print("é”™è¯¯: --hardware å’Œ --engine å‚æ•°å¿…éœ€")
            return

        resolver = DependencyResolver()
        profile = resolver.resolve(args.hardware, args.engine)

        if profile:
            content = resolver.generate_requirements(profile, args.mirror)
            if args.output:
                with open(args.output, "w") as f:
                    f.write(content)
                print(f"âœ… å·²ç”Ÿæˆ: {args.output}")
            else:
                print(content)
        else:
            print(f"âŒ æœªæ‰¾åˆ° {args.hardware} + {args.engine} çš„é…ç½®")


if __name__ == "__main__":
    main()
