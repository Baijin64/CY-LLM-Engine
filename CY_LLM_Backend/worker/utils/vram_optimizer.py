"""VRAM æ˜¾å­˜ä¼˜åŒ–ä¸é¢„æ£€å·¥å…·ã€‚"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional
import torch
import re

BYTE_PER_GB = 1024 ** 3


@dataclass
class VRAMEstimate:
    """æ˜¾å­˜ä¼°ç®—ç»“æœ"""
    model_weights_gb: float
    kv_cache_gb: float
    activation_gb: float
    overhead_gb: float
    required_gb: float
    available_gb: float
    total_gb: float
    is_safe: bool
    recommendation: str
    suggestions: List[str] = field(default_factory=list)


def extract_param_count(model_name_or_path: str) -> float:
    """ä»æ¨¡å‹åç§°æå–å‚æ•°é‡ï¼ˆå•ä½ï¼š10^9ï¼‰"""
    # åŒ¹é… "7B", "13B", "72B" ç­‰
    match = re.search(r'(\d+\.?\d*)[Bb]', model_name_or_path)
    if match:
        return float(match.group(1))
    # é»˜è®¤å‡è®¾ 7B
    return 7.0


def estimate_model_weights(num_params: float, dtype: str, quantization: Optional[str]) -> float:
    """ä¼°ç®—æ¨¡å‹æƒé‡æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰"""
    dtype_bytes = {
        "fp32": 4, "fp16": 2, "bf16": 2,
        "fp8": 1, "int8": 1, "int4": 0.5
    }

    if quantization in ["awq", "gptq", "bitsandbytes"]:
        bytes_per_param = 0.5  # 4-bit
    elif quantization in ["fp8", "fp8_e5m2"]:
        bytes_per_param = 1
    else:
        bytes_per_param = dtype_bytes.get(dtype, 2)

    return num_params * bytes_per_param


def estimate_kv_cache(
    num_params: float,
    max_model_len: int,
    dtype: str = "fp16",
    tensor_parallel_size: int = 1
) -> float:
    """ä¼°ç®— KV Cache æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰"""
    # ç®€åŒ–å…¬å¼ï¼š2 * num_layers * max_model_len * hidden_size * bytes
    # å‡è®¾ num_layers â‰ˆ sqrt(num_params * 1e9 / 8192)
    # hidden_size â‰ˆ 4096 for 7B, 8192 for 72B
    hidden_size = min(4096 + (num_params - 7) * 100, 8192)
    num_layers = int((num_params * 1e9 / hidden_size / hidden_size) ** 0.5)

    dtype_bytes = {"fp32": 4, "fp16": 2, "fp8": 1}
    bytes_per_elem = dtype_bytes.get(dtype, 2)

    kv_cache_bytes = 2 * num_layers * max_model_len * hidden_size * bytes_per_elem
    return kv_cache_bytes / BYTE_PER_GB / tensor_parallel_size


def estimate_vram_requirements(
    model_name_or_params: str | float,
    max_model_len: int = 2048,
    dtype: str = "fp16",
    quantization: Optional[str] = None,
    engine_type: str = "vllm",
    tensor_parallel_size: int = 1,
) -> VRAMEstimate:
    """ä¼°ç®—æ¨¡å‹åŠ è½½æ‰€éœ€çš„ VRAM"""
    # æå–å‚æ•°é‡
    if isinstance(model_name_or_params, str):
        num_params = extract_param_count(model_name_or_params)
    else:
        num_params = model_name_or_params

    # è®¡ç®—å„éƒ¨åˆ†å ç”¨
    model_weights_gb = estimate_model_weights(num_params, dtype, quantization)
    kv_cache_gb = estimate_kv_cache(num_params, max_model_len, dtype, tensor_parallel_size)
    activation_gb = num_params * 0.15  # ç»éªŒå€¼ï¼š15% çš„æ¨¡å‹å¤§å°

    # æ¡†æ¶å¼€é”€
    overhead_map = {"vllm": 2.0, "trt": 1.5, "nvidia": 1.0}
    overhead_gb = overhead_map.get(engine_type, 1.5)

    # æ€»è®¡ï¼ˆè€ƒè™‘å¼ é‡å¹¶è¡Œï¼‰
    total_per_gpu = (
        model_weights_gb / tensor_parallel_size +
        kv_cache_gb +
        activation_gb +
        overhead_gb
    )

    # è·å–å¯ç”¨/æ€»æ˜¾å­˜
    free_gb, total_gb = get_vram_stats()
    available_gb = free_gb or total_gb

    # åˆ¤æ–­æ˜¯å¦å®‰å…¨ï¼ˆä¿ç•™ 10% ä½™é‡ï¼Œè‡³å°‘ 1GBï¼‰
    safety_budget = max(available_gb - 1.0, available_gb * 0.9)
    is_safe = total_per_gpu <= max(safety_budget, 0.0)

    suggestions: List[str] = []
    if not is_safe:
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥é€šè¿‡å¼ é‡å¹¶è¡Œè§£å†³
        gpu_count = get_gpu_count()
        if tensor_parallel_size == 1 and gpu_count > 1:
            recommended_tp, tp_reason = recommend_tensor_parallel_size(
                model_weights_gb, total_per_gpu, total_gb, gpu_count
            )
            if recommended_tp > 1:
                suggestions.append(
                    f"ğŸ¯ å¯ç”¨ tensor_parallel_size={recommended_tp}ï¼ˆ{tp_reason}ï¼‰"
                )

        # é‡åŒ–æ¨¡å‹æ¨è
        if quantization is None:
            quant_suggestions = suggest_quantized_models(
                model_name_or_params if isinstance(model_name_or_params, str) else "",
                num_params
            )
            if quant_suggestions:
                suggestions.extend(quant_suggestions)
            else:
                suggestions.append("å¯ç”¨ 4-bit é‡åŒ–ï¼ˆAWQ/GPTQ æˆ– bitsandbytesï¼‰")

        # max_model_len å»ºè®®
        if max_model_len > 2048:
            suggestions.append(f"é™ä½ max_model_len è‡³ 2048ï¼ˆå½“å‰ {max_model_len}ï¼‰")

        # é€šç”¨å»ºè®®
        suggestions.append("é™ä½ gpu_memory_utilization æˆ–åœ¨é…ç½®ä¸­è…¾å‡ºæ›´å¤šæ˜¾å­˜")

        recommendation = (
            f"âŒ æ˜¾å­˜ä¸è¶³ (éœ€è¦ {total_per_gpu:.1f}GB, å¯ç”¨ {available_gb:.1f}GB)"
        )
    else:
        recommendation = "âœ… æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥åŠ è½½"

    return VRAMEstimate(
        model_weights_gb=model_weights_gb,
        kv_cache_gb=kv_cache_gb,
        activation_gb=activation_gb,
        overhead_gb=overhead_gb,
        required_gb=total_per_gpu,
        available_gb=available_gb,
        total_gb=total_gb,
        is_safe=is_safe,
        recommendation=recommendation,
        suggestions=suggestions,
    )


def get_vram_stats() -> tuple[float, float]:
    """è¿”å› (free_gb, total_gb)ã€‚"""
    if torch.cuda.is_available():
        free, total = torch.cuda.mem_get_info()
        return free / BYTE_PER_GB, total / BYTE_PER_GB
    return 0.0, 0.0


def get_gpu_count() -> int:
    """è¿”å›å¯ç”¨çš„ GPU æ•°é‡ã€‚"""
    if torch.cuda.is_available():
        return torch.cuda.device_count()
    return 0


def recommend_tensor_parallel_size(
    model_weights_gb: float,
    total_required_gb: float,
    per_gpu_vram_gb: float,
    available_gpus: int
) -> tuple[int, str]:
    """æ¨è tensor_parallel_size

    Args:
        model_weights_gb: æ¨¡å‹æƒé‡å¤§å°ï¼ˆGBï¼‰
        total_required_gb: å•å¡æ€»éœ€æ±‚ï¼ˆGBï¼‰
        per_gpu_vram_gb: å•å¡æ˜¾å­˜å¤§å°ï¼ˆGBï¼‰
        available_gpus: å¯ç”¨ GPU æ•°é‡

    Returns:
        (æ¨èçš„ tp_size, æ¨èç†ç”±)
    """
    if available_gpus <= 1:
        return 1, "åªæœ‰ 1 ä¸ª GPU å¯ç”¨"

    # å¦‚æœå•å¡è¶³å¤Ÿï¼Œä¸éœ€è¦å¼ é‡å¹¶è¡Œ
    if total_required_gb <= per_gpu_vram_gb * 0.85:
        return 1, "å•å¡æ˜¾å­˜å……è¶³"

    # è®¡ç®—éœ€è¦å¤šå°‘ä¸ª GPU æ‰èƒ½å®¹çº³æ¨¡å‹æƒé‡
    # æƒé‡ä¼šè¢«åˆ†ç‰‡ï¼Œå…¶ä»–éƒ¨åˆ†ï¼ˆKV Cacheã€æ¿€æ´»å€¼ï¼‰æ¯å¡éƒ½éœ€è¦
    min_gpus_for_weights = max(1, int(model_weights_gb / (per_gpu_vram_gb * 0.6)) + 1)

    # é™åˆ¶åœ¨å¯ç”¨ GPU æ•°é‡å†…
    recommended_tp = min(min_gpus_for_weights, available_gpus)

    # ä¼˜å…ˆé€‰æ‹© 2 çš„å¹‚æ¬¡
    if recommended_tp > 1:
        power_of_two = 1
        while power_of_two < recommended_tp:
            power_of_two *= 2
        if power_of_two <= available_gpus:
            recommended_tp = power_of_two

    reason = f"æ¨¡å‹æƒé‡ {model_weights_gb:.1f}GB éœ€è¦åˆ†ç‰‡åˆ° {recommended_tp} ä¸ª GPU"
    if recommended_tp < min_gpus_for_weights:
        reason += f"ï¼ˆç†æƒ³éœ€è¦ {min_gpus_for_weights} ä¸ªï¼Œä½†åªæœ‰ {available_gpus} ä¸ªå¯ç”¨ï¼‰"

    return recommended_tp, reason


def suggest_quantized_models(model_path: str, num_params: float) -> List[str]:
    """æ¨èé‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹

    Args:
        model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        num_params: æ¨¡å‹å‚æ•°é‡ï¼ˆå•ä½ï¼š10^9ï¼‰

    Returns:
        é‡åŒ–æ¨¡å‹å»ºè®®åˆ—è¡¨
    """
    suggestions = []

    # å¦‚æœå·²ç»æ˜¯é‡åŒ–æ¨¡å‹ï¼Œä¸å†å»ºè®®
    quantization_suffixes = ["-awq", "-gptq", "-gguf", "-int4", "-int8", "-fp8"]
    model_lower = model_path.lower()
    if any(suffix in model_lower for suffix in quantization_suffixes):
        return suggestions

    # å¤§æ¨¡å‹ï¼ˆ>30Bï¼‰ä¼˜å…ˆæ¨è AWQ/GPTQ
    if num_params >= 30:
        suggestions.append(
            f"ğŸ’¡ è€ƒè™‘ä½¿ç”¨ AWQ é‡åŒ–ç‰ˆæœ¬ï¼ˆèŠ‚çœ 75% æ˜¾å­˜ï¼‰ï¼šåœ¨ HuggingFace æœç´¢ '{model_path}-AWQ'"
        )
        suggestions.append(
            f"ğŸ’¡ æˆ–ä½¿ç”¨ GPTQ é‡åŒ–ç‰ˆæœ¬ï¼šåœ¨ HuggingFace æœç´¢ '{model_path}-GPTQ'"
        )

    # ä¸­å°æ¨¡å‹ï¼ˆ7B-30Bï¼‰å¯ä»¥ä½¿ç”¨ bitsandbytes æˆ– AWQ
    elif num_params >= 7:
        suggestions.append(
            f"ğŸ’¡ å¯å¯ç”¨ 4-bit é‡åŒ–èŠ‚çœæ˜¾å­˜ï¼šè®¾ç½® quantization='bitsandbytes' æˆ–ä½¿ç”¨é¢„é‡åŒ–æ¨¡å‹"
        )

    # æç¤ºå¸¸è§çš„é‡åŒ–æ¨¡å‹å‘½åè§„èŒƒ
    if "/" in model_path:
        org, model_name = model_path.rsplit("/", 1)
        suggestions.append(
            f"æç¤ºï¼šé‡åŒ–æ¨¡å‹é€šå¸¸å‘½åä¸º '{org}/{model_name}-AWQ' æˆ– '{org}/{model_name}-GPTQ'"
        )

    return suggestions


def optimize_vram_config(estimate: VRAMEstimate, current_config: Optional[Dict] = None) -> Dict:
    """æ ¹æ®ä¼°ç®—ç»“æœä¼˜åŒ–é…ç½®

    Args:
        estimate: VRAM ä¼°ç®—ç»“æœ
        current_config: å½“å‰é…ç½®ï¼ˆå¯é€‰ï¼‰

    Returns:
        ä¼˜åŒ–åçš„é…ç½®å­—å…¸
    """
    optimized = current_config.copy() if current_config else {}

    if not estimate.is_safe:
        # é™ä½ gpu_memory_utilization
        ratio = estimate.available_gb / max(estimate.required_gb, 1e-6)
        optimized["gpu_memory_utilization"] = max(0.5, min(ratio * 0.7, 0.85))

        # å¦‚æœä»ç„¶ä¸å¤Ÿï¼Œé™ä½ max_model_len
        if ratio < 0.8:
            current_len = optimized.get("max_model_len", 2048)
            new_len = max(1024, current_len // 2)
            optimized["max_model_len"] = new_len

    return optimized


def progressive_retry_configs(base_config: Dict) -> List[Dict]:
    """ç”Ÿæˆæ¸è¿›å¼é™çº§é…ç½®åˆ—è¡¨ï¼Œç”¨äº OOM é‡è¯•

    Args:
        base_config: åŸºç¡€é…ç½®

    Returns:
        é…ç½®åˆ—è¡¨ï¼ŒæŒ‰ä¿å®ˆç¨‹åº¦æ’åº
    """
    configs = [base_config.copy()]  # é…ç½® 1: ç”¨æˆ·åŸå§‹é…ç½®

    # é…ç½® 2: é™ä½ gpu_memory_utilization
    config2 = base_config.copy()
    current_util = config2.get("gpu_memory_utilization", 0.75)
    config2["gpu_memory_utilization"] = max(0.5, current_util - 0.10)
    configs.append(config2)

    # é…ç½® 3: è¿›ä¸€æ­¥é™ä½ + å‡å°‘ max_model_len
    config3 = base_config.copy()
    config3["gpu_memory_utilization"] = 0.60
    config3["max_model_len"] = min(
        config3.get("max_model_len", 4096), 4096
    )
    configs.append(config3)

    # é…ç½® 4: æœ€ä¿å®ˆé…ç½®
    config4 = base_config.copy()
    config4["gpu_memory_utilization"] = 0.50
    config4["max_model_len"] = 2048
    configs.append(config4)

    return configs


def suggest_kv_cache_strategy(
    kv_cache_gb: float,
    available_vram_gb: float,
    max_model_len: int,
    current_gpu_util: float,
    expected_qps: Optional[int] = None
) -> List[str]:
    """KV Cache é¢„åˆ†é…ç­–ç•¥å»ºè®®

    Args:
        kv_cache_gb: å½“å‰ KV Cache é¢„ä¼°å ç”¨
        available_vram_gb: å¯ç”¨æ˜¾å­˜
        max_model_len: æœ€å¤§åºåˆ—é•¿åº¦
        current_gpu_util: å½“å‰ gpu_memory_utilization è®¾ç½®
        expected_qps: é¢„æœŸ QPSï¼ˆå¯é€‰ï¼‰

    Returns:
        KV Cache ä¼˜åŒ–å»ºè®®åˆ—è¡¨
    """
    suggestions = []
    kv_ratio = kv_cache_gb / max(available_vram_gb, 0.1)

    # åŸºäºå¹¶å‘åœºæ™¯çš„å»ºè®®
    if expected_qps is not None:
        if expected_qps <= 10:
            # ä½å¹¶å‘ï¼šå¯ä»¥é™ä½ gpu_memory_utilization
            if current_gpu_util > 0.70:
                suggestions.append(
                    f"ğŸ¯ ä½å¹¶å‘åœºæ™¯ (QPSâ‰¤10)ï¼šå»ºè®®é™ä½ gpu_memory_utilization è‡³ 0.70 "
                    f"ï¼ˆå½“å‰ {current_gpu_util:.2f}ï¼‰ä»¥èŠ‚çœæ˜¾å­˜"
                )
        elif expected_qps <= 50:
            # ä¸­å¹¶å‘ï¼šæ¨è 0.75
            if current_gpu_util < 0.70 or current_gpu_util > 0.80:
                suggestions.append(
                    f"ğŸ¯ ä¸­å¹¶å‘åœºæ™¯ (QPS 10-50)ï¼šå»ºè®®è®¾ç½® gpu_memory_utilization=0.75 "
                    f"ï¼ˆå½“å‰ {current_gpu_util:.2f}ï¼‰"
                )
        else:
            # é«˜å¹¶å‘ï¼šæ¨è 0.85
            if current_gpu_util < 0.80:
                suggestions.append(
                    f"ğŸ¯ é«˜å¹¶å‘åœºæ™¯ (QPS>50)ï¼šå»ºè®®æå‡ gpu_memory_utilization è‡³ 0.85 "
                    f"ï¼ˆå½“å‰ {current_gpu_util:.2f}ï¼‰ä»¥æ”¯æŒæ›´å¤šå¹¶å‘è¯·æ±‚"
                )

    # åŸºäºåºåˆ—é•¿åº¦çš„å»ºè®®
    if max_model_len <= 2048:
        suggestions.append(
            f"âœ… max_model_len={max_model_len} è¾ƒå°ï¼ŒKV Cache å ç”¨ä½ï¼Œé€‚åˆé«˜å¹¶å‘"
        )
    elif max_model_len <= 8192:
        suggestions.append(
            f"âš¡ max_model_len={max_model_len}ï¼šå¹³è¡¡é…ç½®ï¼Œ"
            f"KV Cache å ç”¨ {kv_cache_gb:.1f}GB ({kv_ratio*100:.0f}% æ˜¾å­˜)"
        )
    else:
        suggestions.append(
            f"âš ï¸  max_model_len={max_model_len} è¾ƒå¤§ï¼Œ"
            f"KV Cache å ç”¨ {kv_cache_gb:.1f}GB ({kv_ratio*100:.0f}% æ˜¾å­˜)ï¼Œ"
            "é«˜å¹¶å‘æ—¶éœ€è¦ç›‘æ§æ˜¾å­˜ä½¿ç”¨ç‡"
        )
        if kv_ratio > 0.5:
            suggestions.append(
                "ğŸ’¡ è€ƒè™‘é™ä½ max_model_len æˆ–å¯ç”¨ Prefix Caching ä»¥ä¼˜åŒ–é•¿åºåˆ—åœºæ™¯"
            )

    # KV Cache dtype ä¼˜åŒ–å»ºè®®
    if kv_cache_gb > 5.0:
        suggestions.append(
            f"ğŸ’¡ KV Cache å ç”¨ {kv_cache_gb:.1f}GB è¾ƒå¤§ï¼Œ"
            "å¯è€ƒè™‘è®¾ç½® kv_cache_dtype='fp8' ä»¥èŠ‚çœ 50% KV Cache æ˜¾å­˜ï¼ˆç•¥å¾®æŸå¤±ç²¾åº¦ï¼‰"
        )

    return suggestions


def suggest_batch_optimization(
    engine_type: str,
    kv_cache_gb: float,
    available_vram_gb: float,
    max_model_len: int
) -> List[str]:
    """ç”Ÿæˆæ‰¹å¤„ç†ä¼˜åŒ–å»ºè®®

    Args:
        engine_type: å¼•æ“ç±»å‹
        kv_cache_gb: KV Cache æ˜¾å­˜å ç”¨
        available_vram_gb: å¯ç”¨æ˜¾å­˜
        max_model_len: æœ€å¤§åºåˆ—é•¿åº¦

    Returns:
        æ‰¹å¤„ç†ä¼˜åŒ–å»ºè®®åˆ—è¡¨
    """
    suggestions = []
    engine_lower = engine_type.lower()

    # vLLM æ‰¹å¤„ç†å»ºè®®
    if "vllm" in engine_lower:
        suggestions.append(
            "âš¡ vLLM Continuous Batching è‡ªåŠ¨å¯ç”¨ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®"
        )

        # å¦‚æœ KV Cache å ç”¨è¾ƒå°ï¼Œå¯ä»¥å¢åŠ å¹¶å‘
        if kv_cache_gb < available_vram_gb * 0.3:
            suggestions.append(
                f"ğŸ’¡ KV Cache ä»…å ç”¨ {kv_cache_gb:.1f}GBï¼Œ"
                f"å¯æ”¯æŒé«˜å¹¶å‘è¯·æ±‚ï¼ˆå»ºè®®é…ç½®ç½‘å…³å±‚çš„å¹¶å‘é™åˆ¶ï¼‰"
            )

        # é•¿åºåˆ—åœºæ™¯çš„å»ºè®®
        if max_model_len > 8192:
            suggestions.append(
                f"âš ï¸  max_model_len={max_model_len} è¾ƒå¤§ï¼Œ"
                "é«˜å¹¶å‘æ—¶ KV Cache å ç”¨ä¼šæ˜¾è‘—å¢åŠ ï¼Œå»ºè®®ç›‘æ§æ˜¾å­˜ä½¿ç”¨ç‡"
            )

    # TensorRT-LLM æ‰¹å¤„ç†å»ºè®®
    elif "trt" in engine_lower or "tensorrt" in engine_lower:
        # ä¼°ç®—å¯æ”¯æŒçš„æ‰¹å¤„ç†å¤§å°
        # KV Cache æ˜¯æŒ‰ max_batch_size * max_model_len é¢„åˆ†é…çš„
        estimated_max_batch = max(1, int(available_vram_gb * 0.6 / max(kv_cache_gb, 0.1)))

        suggestions.append(
            f"ğŸ¯ TensorRT Inflight Batching: å»ºè®®è®¾ç½® max_batch_size={min(estimated_max_batch, 32)}"
        )
        suggestions.append(
            "ğŸ’¡ TRT æ‰¹å¤„ç†éœ€è¦åœ¨æ„å»ºå¼•æ“æ—¶æŒ‡å®š max_batch_sizeï¼Œ"
            "è¿è¡Œæ—¶æ— æ³•åŠ¨æ€è°ƒæ•´"
        )

        if estimated_max_batch > 16:
            suggestions.append(
                f"âœ¨ æ˜¾å­˜å……è¶³ï¼Œå¯æ”¯æŒæœ€å¤š {estimated_max_batch} çš„æ‰¹å¤„ç†ï¼ˆæ¨è 16-32ï¼‰"
            )

    # Nvidia (transformers) æ‰¹å¤„ç†å»ºè®®
    elif "nvidia" in engine_lower or engine_type == "cuda":
        suggestions.append(
            "ğŸ’¡ transformers å¼•æ“æ”¯æŒç®€å•æ‰¹å¤„ç†ï¼Œ"
            "ä½†æ€§èƒ½ä¸å¦‚ vLLM/TRT çš„åŠ¨æ€æ‰¹å¤„ç†"
        )
        suggestions.append(
            "ğŸ¯ é«˜å¹¶å‘åœºæ™¯å»ºè®®åˆ‡æ¢åˆ° vLLM å¼•æ“ä»¥è·å¾—æ›´å¥½çš„ååé‡"
        )

    return suggestions


def format_vram_report(estimate: VRAMEstimate, verbose: bool = True) -> str:
    """æ ¼å¼åŒ– VRAM ä¼°ç®—æŠ¥å‘Š

    Args:
        estimate: VRAM ä¼°ç®—ç»“æœ
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
    """
    lines = []

    if verbose:
        lines.append("æ˜¾å­˜éœ€æ±‚ä¼°ç®—:")
        lines.append(f"  æ¨¡å‹æƒé‡: {estimate.model_weights_gb:.2f} GB")
        lines.append(f"  KV Cache:  {estimate.kv_cache_gb:.2f} GB")
        lines.append(f"  æ¿€æ´»å€¼:    {estimate.activation_gb:.2f} GB")
        lines.append(f"  æ¡†æ¶å¼€é”€:  {estimate.overhead_gb:.2f} GB")
        lines.append(f"  æ€»è®¡:      {estimate.required_gb:.2f} GB")
        lines.append(f"  å¯ç”¨æ˜¾å­˜:  {estimate.available_gb:.2f} GB")
        lines.append("")

    lines.append(estimate.recommendation)

    if estimate.suggestions:
        lines.append("\nå»ºè®®:")
        for suggestion in estimate.suggestions:
            lines.append(f"  - {suggestion}")

    return "\n".join(lines)
