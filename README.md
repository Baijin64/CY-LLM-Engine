# CY-LLM Engine

> ğŸš€ **é«˜æ€§èƒ½** Â· **ä½¿ç”¨ç®€æ´** Â· **é«˜åº¦è‡ªå®šä¹‰** çš„å®Œæ•´ AI æœåŠ¡ç³»ç»Ÿ

ä¸€ä¸ªæ”¯æŒå¤šç§æ¨ç†å¼•æ“ï¼ˆvLLM / TensorRT-LLM / MindIEï¼‰å’Œå¤šç§ç¡¬ä»¶å¹³å°ï¼ˆNVIDIA GPU / åä¸º Ascend NPUï¼‰çš„ç»Ÿä¸€ AI æ¨ç†åç«¯ã€‚

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

> **ğŸ”— å®Œæ•´æ–‡æ¡£åœ¨çº¿é˜…è¯»**: [https://zread.ai/Baijin64/CY-LLM-Engine](https://zread.ai/Baijin64/CY-LLM-Engine)

### æ ¸å¿ƒæ–‡æ¡£

| æ–‡æ¡£ | æè¿° |
|------|------|
| [docs/README.md](./docs/README.md) | é¡¹ç›®è¯¦ç»†ä»‹ç»ä¸å¿«é€Ÿå…¥é—¨ |
| [docs/INSTALL.md](./docs/INSTALL.md) | è¯¦ç»†å®‰è£…ä¸é…ç½®æŒ‡å— |
| [docs/ARCHITECTURE.md](./docs/ARCHITECTURE.md) | æ¶æ„è®¾è®¡ä¸æ•°æ®æµè¯¦è§£ |
| [docs/API.md](./docs/API.md) | REST API ä¸ gRPC æ¥å£æ–‡æ¡£ |
| [docs/CONTRIBUTING.md](./docs/CONTRIBUTING.md) | è´¡çŒ®è€…è§„èŒƒä¸å¼€å‘æŒ‡å— |
| [docs/TESTING.md](./docs/TESTING.md) | æµ‹è¯•æŒ‡å—ä¸ CI é…ç½® |
| [docs/FAQ.md](./docs/FAQ.md) | å¸¸è§é—®é¢˜è§£ç­” |
| [docs/TRT_GUIDE.md](./docs/TRT_GUIDE.md) | TensorRT-LLM ä¸“ç”¨æŒ‡å— |

### å¿«é€ŸæŒ‡å—

| æ–‡ä»¶ | æè¿° |
|------|------|
| [QUICK_START.md](./QUICK_START.md) | å¿«é€Ÿå¼€å§‹æŒ‡ä»¤é€ŸæŸ¥ |

### é¡¹ç›®å†å²

| æ–‡ä»¶ | æè¿° |
|------|------|
| [docs/HISTORY/MIGRATION_SUMMARY.md](./docs/HISTORY/MIGRATION_SUMMARY.md) | EW_AI_Backend â†’ CY_LLM_Backend è¿ç§»æ€»ç»“ |
| [docs/HISTORY/PHASE2_3_UPGRADE_REPORT.md](./docs/HISTORY/PHASE2_3_UPGRADE_REPORT.md) | Phase 2 & 3 ä¼˜åŒ–å‡çº§æŠ¥å‘Š |

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | æè¿° |
|------|------|
| **å¤šå¼•æ“æ”¯æŒ** | vLLM (CUDA/Ascend)ã€TensorRT-LLMã€MindIE |
| **å¤šç¡¬ä»¶å¹³å°** | NVIDIA GPUã€åä¸º Ascend NPU |
| **ç»Ÿä¸€ CLI** | `./cy` / `./cy-llm` ä¸€é”®éƒ¨ç½²å’Œç®¡ç† |
| **åŸºç¡€æ¨ç†** | OpenAI å…¼å®¹éæµå¼è¾“å‡º |
| **è½»é‡åŒ–ç½‘å…³** | Python + FastAPI è½»é‡ç‰ˆ |
| **å¼¹æ€§ä¼¸ç¼©** | è½»é‡ç‰ˆå¯æ‰©å±•å¤š Worker å®ä¾‹ |
| **å®Œæ•´è®­ç»ƒ** | LoRA/PEFT å¾®è°ƒæ”¯æŒ |
| **æ˜¾å­˜ä¼˜åŒ–** | VRAM é¢„ä¼°ä¸ OOM è‡ªåŠ¨é‡è¯• |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ (Community Lite)

### æœ¬åœ°å¯åŠ¨

```bash
# 1. åˆå§‹åŒ–ç¯å¢ƒ
./cy-llm setup --engine cuda-vllm

# 2. å®‰è£… Lite ä¾èµ–
conda run -n ${CY_LLM_CONDA_ENV:-vllm} pip install -r CY_LLM_Backend/gateway_lite/requirements.txt
conda run -n ${CY_LLM_CONDA_ENV:-vllm} pip install -r CY_LLM_Backend/coordinator_lite/requirements.txt

# 3. ä¸€é”®å¯åŠ¨ (Lite Gateway + Lite Coordinator + Worker)
./cy-llm lite --engine cuda-vllm --model qwen2.5-7b

# 4. æµ‹è¯• (OpenAI å…¼å®¹)
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen2.5-7b","messages":[{"role":"user","content":"ä½ å¥½"}]}'
```

> Lite ç‰ˆæœ¬é»˜è®¤ç«¯å£ä¸º 8000ï¼ˆGatewayï¼‰ï¼ŒCoordinator ä¸º 50051ï¼ŒWorker ä¸º 50052ã€‚

### Docker Compose å¯åŠ¨

```bash
# å¯åŠ¨
docker compose -f docker-compose.community.yml up -d

# æŸ¥çœ‹çŠ¶æ€
docker compose -f docker-compose.community.yml ps

# åœæ­¢
docker compose -f docker-compose.community.yml down
```

### ä½¿ç”¨ VRAM é¢„ä¼°å’Œä¼˜åŒ–

```bash
# è¯Šæ–­ç¯å¢ƒä¸æ¨¡å‹æ˜¾å­˜éœ€æ±‚
./cy-llm diagnose qwen2.5-7b

# è½¬æ¢æ¨¡å‹ä¸º TensorRT-LLM å¼•æ“ï¼ˆå¯é€‰ï¼Œæå‡æ€§èƒ½ï¼‰
./cy-llm convert-trt --model Qwen/Qwen2.5-7B-Instruct --output /models/qwen2.5-7b-trt

# ä½¿ç”¨ TRT å¼•æ“å¯åŠ¨
./cy-llm lite --engine cuda-trt --model qwen2.5-7b-trt
```

---

## ğŸ¯ å¼•æ“é€‰æ‹©æŒ‡å—

| å¼•æ“ | ç¡¬ä»¶ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| `cuda-vllm` | NVIDIA GPU | PagedAttention, é«˜åå | é€šç”¨æ¨è |
| `cuda-trt` | NVIDIA GPU | æè‡´æ€§èƒ½, éœ€é¢„ç¼–è¯‘ | å›ºå®šæ¨¡å‹ç”Ÿäº§ |
| `ascend-vllm` | åä¸º NPU | å…¼å®¹ vLLM API | Ascend ç¯å¢ƒ |
| `ascend-mindie` | åä¸º NPU | å®˜æ–¹ä¼˜åŒ– | Ascend é«˜æ€§èƒ½ |

```bash
# ä½¿ç”¨ vLLM (é»˜è®¤)
./cy-llm lite --engine cuda-vllm

# ä½¿ç”¨ TensorRT-LLM
./cy-llm lite --engine cuda-trt

# ä½¿ç”¨åä¸º Ascend vLLM
./cy-llm lite --engine ascend-vllm

# ä½¿ç”¨åä¸º Ascend MindIE
./cy-llm lite --engine ascend-mindie
```

---

## ğŸ“– CLI å‘½ä»¤å‚è€ƒ

```bash
./cy-llm <command> [options]

Commands:
  setup       åˆå§‹åŒ–ç¯å¢ƒ (Conda + ä¾èµ–)
  lite        å¯åŠ¨è½»é‡ç‰ˆæœåŠ¡ (Gateway Lite + Coordinator Lite + Worker)
  worker      ä»…å¯åŠ¨ Worker
  stop        åœæ­¢æ‰€æœ‰æœåŠ¡
  status      æŸ¥çœ‹æœåŠ¡çŠ¶æ€
  docker      Docker Compose éƒ¨ç½² (åç»­è¡¥å…… Lite)
  test        è¿è¡Œæµ‹è¯•
  models      æ¨¡å‹ç®¡ç†
  convert-trt è½¬æ¢æ¨¡å‹ä¸º TensorRT-LLM å¼•æ“
  prepare     æ•°æ®é¢„å¤„ç†
  train       LoRA å¾®è°ƒè®­ç»ƒ
  chat        äº¤äº’å¼ LoRA æ¨ç†
  help        æ˜¾ç¤ºå¸®åŠ©

Options:
  --engine TYPE     æ¨ç†å¼•æ“ (cuda-vllm/cuda-trt/ascend-vllm/ascend-mindie)
  --model ID        æ¨¡å‹ ID
  --port PORT       Lite Gateway ç«¯å£ (é»˜è®¤: 8000)
  -d, --daemon      åå°è¿è¡Œ

Examples:
  ./cy-llm setup --engine cuda-vllm       # åˆå§‹åŒ–
  ./cy-llm lite --engine cuda-vllm --model qwen2.5-7b  # Lite ä¸€é”®å¯åŠ¨
  ./cy-llm status                         # æŸ¥çœ‹çŠ¶æ€
  ./cy-llm convert-trt --model Qwen/Qwen2.5-7B --output /models/trt  # è½¬æ¢ TRT æ¨¡å‹
  ./cy-llm prepare --raw /data/raw --out /data/train.jsonl           # é¢„å¤„ç†æ•°æ®
  ./cy-llm train --dataset /data/train.jsonl --output /models/lora   # å¯åŠ¨è®­ç»ƒ
  ./cy-llm chat --model qwen2.5-7b --lora /models/lora               # åŠ è½½ LoRA å¯¹è¯
```

---

## ğŸ›  æ¨ç†æ¥å£ç¤ºä¾‹ (Lite)

### éæµå¼æ¨ç† (OpenAI å…¼å®¹)

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen2.5-7b","messages":[{"role":"user","content":"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}]}'
```

---

## ğŸ§ª æµ‹è¯•

```bash
 # è¿è¡Œé›†æˆæµ‹è¯• (é»˜è®¤è¿è¡Œæ ¸å¿ƒé›†æˆæµ‹è¯•)
 ./cy-llm test integration
 
+# è¿è¡Œç‰¹å®šé›†æˆæµ‹è¯• (ä¾‹å¦‚: engine, memory, scheduler, stream, telemetry, all)
+./cy-llm test integration all
+
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
./cy-llm test all
```

---

## ğŸ— ç³»ç»Ÿæ¶æ„ (Lite)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Client                                   â”‚
â”‚                  (Browser / API Client)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTP
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Gateway Lite (Python)                            â”‚
â”‚               FastAPI + gRPC Client                              â”‚
â”‚  ç«¯å£: 8000                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ gRPC (:50051)
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Coordinator Lite (Python)                          â”‚
â”‚                 gRPC Proxy + ç®€åŒ–è°ƒåº¦                            â”‚
â”‚  ç«¯å£: 50051                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ gRPC (:50052)
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Worker (Python)    â”‚     â”‚      Worker (Python)    â”‚
â”‚      NVIDIA GPU         â”‚     â”‚      Ascend NPU         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ InferenceEngine   â”‚  â”‚     â”‚  â”‚ InferenceEngine   â”‚  â”‚
â”‚  â”‚  â””â”€ vLLM/TensorRT â”‚  â”‚     â”‚  â”‚  â””â”€ MindIE/vLLM   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ TrainingEngine    â”‚  â”‚     â”‚  â”‚ TrainingEngine    â”‚  â”‚
â”‚  â”‚  â””â”€ LoRA/PEFT     â”‚  â”‚     â”‚  â”‚  â””â”€ LoRA/PEFT     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   (å¯é€‰) Redis    â”‚
                    â”‚   (åç»­æ‰©å±•)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

è¯¦ç»†æ¶æ„è®¾è®¡è¯·å‚è€ƒ [docs/ARCHITECTURE.md](./docs/ARCHITECTURE.md)ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
CY-LLM-Engine/
â”œâ”€â”€ cy                           # ä¸» CLI å·¥å…· (Shell è„šæœ¬)
â”œâ”€â”€ cy-llm                       # CLI åˆ«å
â”œâ”€â”€ CY_LLM_Backend/
|   â”œâ”€â”€ gateway_lite/           # Python Gateway Lite (FastAPI)
|   â”œâ”€â”€ coordinator_lite/       # Python Coordinator Lite (gRPC Proxy)
â”‚   â”œâ”€â”€ worker/                 # Python Worker æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ main.py             # å…¥å£ç‚¹
â”‚   â”‚   â”œâ”€â”€ core/               # æ ¸å¿ƒç»„ä»¶ (server, scheduler, memory, telemetry)
â”‚   â”‚   â”œâ”€â”€ engines/            # æ¨ç†å¼•æ“ (vLLM, TRT, MindIE, Ascend)
â”‚   â”‚   â”œâ”€â”€ training/           # è®­ç»ƒå¼•æ“ (LoRA/PEFT)
â”‚   â”‚   â”œâ”€â”€ config/             # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ cache/              # ç¼“å­˜æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ utils/              # å·¥å…·å‡½æ•°
â”‚   â”‚   â””â”€â”€ tests/              # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ deploy/                 # éƒ¨ç½²é…ç½® (docker-compose, config.json)
â”‚   â””â”€â”€ proto/                  # gRPC åè®®å®šä¹‰
â”œâ”€â”€ CY_LLM_Training/            # è®­ç»ƒç›¸å…³ä»£ç 
â”œâ”€â”€ scripts/                    # è¾…åŠ©è„šæœ¬
â”‚   â”œâ”€â”€ convert_trt.py          # TRT æ¨¡å‹è½¬æ¢å·¥å…·
â”‚   â””â”€â”€ diagnose_env.py         # ç¯å¢ƒè¯Šæ–­
â”œâ”€â”€ docs/                       # é¡¹ç›®æ–‡æ¡£
â”‚   â”œâ”€â”€ HISTORY/                # é¡¹ç›®å†å²æ–‡æ¡£
â”‚   â”œâ”€â”€ README.md               # é¡¹ç›®è¯¦ç»†ä»‹ç»
â”‚   â”œâ”€â”€ INSTALL.md              # å®‰è£…æŒ‡å—
â”‚   â”œâ”€â”€ ARCHITECTURE.md         # æ¶æ„è®¾è®¡
â”‚   â”œâ”€â”€ API.md                  # API æ–‡æ¡£
â”‚   â”œâ”€â”€ CONTRIBUTING.md         # è´¡çŒ®è§„èŒƒ
â”‚   â”œâ”€â”€ TESTING.md              # æµ‹è¯•æŒ‡å—
â”‚   â”œâ”€â”€ FAQ.md                  # å¸¸è§é—®é¢˜
â”‚   â””â”€â”€ TRT_GUIDE.md            # TRT ä½¿ç”¨æŒ‡å—
â”œâ”€â”€ QUICK_START.md              # å¿«é€Ÿå¼€å§‹æŒ‡å—
â”œâ”€â”€ requirements*.txt           # Python ä¾èµ–
â””â”€â”€ LICENSE
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### å…³é”®ç¯å¢ƒå˜é‡

#### Gateway Lite
- `COORDINATOR_GRPC_ADDR`ï¼šCoordinator gRPC åœ°å€ï¼ˆé»˜è®¤ `127.0.0.1:50051`ï¼‰
- `GATEWAY_API_TOKEN`ï¼šå¯é€‰çš„é™æ€é‰´æƒ Token
- `GATEWAY_REQUEST_TIMEOUT`ï¼šè¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼Œé»˜è®¤ 60ï¼‰

#### Coordinator Lite
- `COORDINATOR_GRPC_BIND`ï¼šCoordinator ç›‘å¬åœ°å€ï¼ˆé»˜è®¤ `0.0.0.0:50051`ï¼‰
- `WORKER_GRPC_ADDRS`ï¼šWorker åˆ—è¡¨ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰
- `COORDINATOR_CONFIG`ï¼šå¯é€‰é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONï¼‰

ç¤ºä¾‹é…ç½®ï¼š
```json
{
  "workers": ["worker-1:50052", "worker-2:50052"]
}
```

#### Worker
- `CY_LLM_ENGINE`ï¼šå¼•æ“ç±»å‹ï¼ˆå¦‚ `cuda-vllm`ï¼‰
- `CY_LLM_DEFAULT_MODEL`ï¼šé»˜è®¤æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
- `CY_LLM_DEFAULT_ADAPTER`ï¼šLoRA é€‚é…å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
- `CY_LLM_MODEL_REGISTRY`ï¼šæ¨¡å‹æ³¨å†Œè¡¨ JSONï¼ˆå¯é€‰ï¼Œå­—ç¬¦ä¸²ï¼‰
- `CY_LLM_MODEL_REGISTRY_PATH`ï¼šæ¨¡å‹æ³¨å†Œè¡¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
- `CY_LLM_HEALTH_PORT`ï¼šå¥åº·æ£€æŸ¥ç«¯å£ï¼ˆé»˜è®¤ `9090`ï¼‰
- `VLLM_GPU_MEMORY_UTILIZATION`ï¼šGPU æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆé»˜è®¤ `0.8`ï¼‰

### æ¨¡å‹é…ç½®æ–‡ä»¶

æ¨¡å‹é…ç½®æ–‡ä»¶ä½äº `CY_LLM_Backend/deploy/models.json`ï¼š

```json
{
  "qwen2.5-7b": {
    "model_path": "Qwen/Qwen2.5-7B-Instruct",
    "engine": "cuda-vllm",
    "gpu_memory_utilization": 0.85,
    "max_model_len": 4096,
    "quantization": null
  }
}
```

---

## âš™ï¸ ç¯å¢ƒè¦æ±‚

| ç»„ä»¶ | æœ€ä½ç‰ˆæœ¬ | æ¨èç‰ˆæœ¬ |
|------|----------|----------|
| Python | 3.10+ | 3.11 |
| CUDA | 12.0 | 12.4 |
| CANN | 8.0+ | 8.0.RC1 |
| Redis | å¯é€‰ | å¯é€‰ |
| Docker | 24.0 | 25.0 |

---

## ğŸ· ç‰ˆæœ¬å·è§„èŒƒ

æœ¬é¡¹ç›®é‡‡ç”¨ **å››æ®µå¼ç‰ˆæœ¬å·**: `[major.minor.patch.build-SUFFIX]`

| æ®µ | å«ä¹‰ | è¯´æ˜ |
|----|------|------|
| major | é‡å¤§ä¸å…¼å®¹å˜åŒ– | ç ´åæ€§é‡æ„ |
| minor | å‘åå…¼å®¹æ–°åŠŸèƒ½ | feature |
| patch | Bug ä¿®å¤ä¸ä¼˜åŒ– | bugfix |
| build | æ„å»º/æµ‹è¯•æ¬¡æ•° | é€’å¢ |
| SUFFIX | ç¨³å®šæ€§åç¼€ | PreAlpha/Alpha/Beta/RC/Release |

æäº¤æ¶ˆæ¯æ ¼å¼ï¼š
```
[2.1.1.2-Alpha] feat(worker): add async inference support

Add async inference support for vLLM engine.

ã€ä¸­æ–‡ã€‘workerï¼šæ·»åŠ å¼‚æ­¥æ¨ç†æ”¯æŒ
```

è¯¦ç»†è§„èŒƒè¯·å‚è€ƒ [docs/CONTRIBUTING.md](./docs/CONTRIBUTING.md)ã€‚

---

## ğŸ“ ç‰ˆæœ¬å†å²

- **[1.5.2.0]** - ç®€åŒ–éƒ¨ç½²æµç¨‹ï¼Œç»Ÿä¸€ CLI å·¥å…·ï¼Œæ”¯æŒå››ç§æ¨ç†å¼•æ“
- **[1.5.1.3-alpha]** - C++ å…¥å£ç‚¹ï¼Œå››å¼•æ“æ¶æ„å®ç°
- **[1.5.0.0-alpha]** - VRAM ä¼˜åŒ–ç³»ç»Ÿã€TRT çœŸæµå¼è¾“å‡º
- **[1.0.0-alpha]** - åˆå§‹ç‰ˆæœ¬ï¼ŒGateway + Worker åŸºç¡€æ¶æ„

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue æˆ– Pull Requestï¼

è¯¦ç»†è´¡çŒ®æŒ‡å—è¯·å‚è€ƒ [docs/CONTRIBUTING.md](./docs/CONTRIBUTING.md)ã€‚

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦è§ [LICENSE](./LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ”— ç›¸å…³é“¾æ¥

- **åœ¨çº¿æ–‡æ¡£**: [https://zread.ai/Baijin64/CY-LLM-Engine](https://zread.ai/Baijin64/CY-LLM-Engine)
- **é¡¹ç›®ä»“åº“**: [https://github.com/Baijin64/CY-LLM-Engine](https://github.com/Baijin64/CY-LLM-Engine)
- **é—®é¢˜åé¦ˆ**: [GitHub Issues](https://github.com/Baijin64/CY-LLM-Engine/issues)
