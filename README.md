# Element Warfare AI

> ğŸš€ **é«˜æ€§èƒ½** Â· **ä½¿ç”¨ç®€æ´** Â· **é«˜åº¦è‡ªå®šä¹‰** çš„å®Œæ•´ AI æœåŠ¡ç³»ç»Ÿ

ä¸€ä¸ªæ”¯æŒå¤šç§æ¨ç†å¼•æ“ï¼ˆvLLM / TensorRT-LLM / MindIEï¼‰ã€å¤šç§ç¡¬ä»¶å¹³å°ï¼ˆNVIDIA GPU / åä¸º Ascend NPUï¼‰çš„ç»Ÿä¸€ AI æ¨ç†åç«¯ã€‚

## âœ¨ ç‰¹æ€§

- **å››ç§æ¨ç†å¼•æ“**: `cuda-vllm` / `cuda-trt` / `ascend-vllm` / `ascend-mindie`
- **ä¸€é”®éƒ¨ç½²**: ç»Ÿä¸€çš„ `./ew` å‘½ä»¤è¡Œå·¥å…·
- **æµå¼æ¨ç†**: SSE å®æ—¶æµå¼è¿”å›
- **ä¼ä¸šçº§ç½‘å…³**: Kotlin + Spring WebFlux å“åº”å¼æ¶æ„
- **å¼¹æ€§ä¼¸ç¼©**: æ”¯æŒå¤š Worker å®ä¾‹
- **åŒå¹³å°æ”¯æŒ**: NVIDIA CUDA ä¸ åä¸º Ascend NPU

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 30 ç§’å¯åŠ¨

```bash
# 1. åˆå§‹åŒ–ç¯å¢ƒ
./ew setup --engine cuda-vllm

# 2. å¯åŠ¨æœåŠ¡
./ew start --model deepseek-v3
```

æœåŠ¡å°†åœ¨ `http://localhost:8080` å¯åŠ¨ã€‚

### æµ‹è¯•æ¨ç†

```bash
curl -X POST http://localhost:8080/api/v1/inference/stream \
  -H "Content-Type: application/json" \
  -d '{"modelId": "deepseek-v3", "prompt": "ä½ å¥½"}' \
  --no-buffer
```

## ğŸ“¦ å®‰è£…

### ç¯å¢ƒè¦æ±‚

| ç»„ä»¶ | æœ€ä½ç‰ˆæœ¬ |
|------|----------|
| Python | 3.10+ |
| Java | 21+ |
| CUDA | 12.0+ (NVIDIA) |
| CANN | 8.0+ (Ascend) |

### æ–¹å¼ä¸€ï¼šæœ¬åœ°éƒ¨ç½² (æ¨èå¼€å‘)

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-repo/EW_AI_Deployment.git
cd EW_AI_Deployment

# åˆå§‹åŒ–ç¯å¢ƒ
./ew setup

# å¯åŠ¨æœåŠ¡
./ew start
```

### æ–¹å¼äºŒï¼šDocker éƒ¨ç½² (æ¨èç”Ÿäº§)

```bash
cd EW_AI_Backend/deploy

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
vim .env  # ç¼–è¾‘é…ç½®

# å¯åŠ¨æœåŠ¡
./ew docker up
```

## ğŸ¯ å¼•æ“é€‰æ‹©æŒ‡å—

| å¼•æ“ | ç¡¬ä»¶ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| `cuda-vllm` | NVIDIA GPU | PagedAttention, é«˜åå | é€šç”¨æ¨è |
| `cuda-trt` | NVIDIA GPU | æè‡´æ€§èƒ½, éœ€é¢„ç¼–è¯‘ | å›ºå®šæ¨¡å‹ç”Ÿäº§ |
| `ascend-vllm` | åä¸º NPU | å…¼å®¹ vLLM API | Ascend ç¯å¢ƒ |
| `ascend-mindie` | åä¸º NPU | å®˜æ–¹ä¼˜åŒ– | Ascend é«˜æ€§èƒ½ |

```bash
# ä½¿ç”¨ vLLM (é»˜è®¤)
./ew start --engine cuda-vllm

# ä½¿ç”¨ TensorRT-LLM
./ew start --engine cuda-trt

# ä½¿ç”¨åä¸º Ascend
./ew start --engine ascend-vllm
```

## ğŸ“– CLI å‘½ä»¤å‚è€ƒ

```bash
./ew <command> [options]

å‘½ä»¤:
  setup       åˆå§‹åŒ–ç¯å¢ƒ (Conda + ä¾èµ– + Gateway)
  start       å¯åŠ¨å®Œæ•´æœåŠ¡ (Gateway + Worker)
  worker      ä»…å¯åŠ¨ Worker
  stop        åœæ­¢æ‰€æœ‰æœåŠ¡
  status      æŸ¥çœ‹æœåŠ¡çŠ¶æ€
  docker      Docker Compose éƒ¨ç½²
  test        è¿è¡Œæµ‹è¯•
  models      æ¨¡å‹ç®¡ç†
  help        æ˜¾ç¤ºå¸®åŠ©

å¸¸ç”¨é€‰é¡¹:
  --engine TYPE     æ¨ç†å¼•æ“ (cuda-vllm/cuda-trt/ascend-vllm/ascend-mindie)
  --model ID        æ¨¡å‹ ID
  --port PORT       Gateway ç«¯å£ (é»˜è®¤: 8080)
  -d, --daemon      åå°è¿è¡Œ

ç¤ºä¾‹:
  ./ew setup --engine cuda-vllm       # åˆå§‹åŒ–
  ./ew start --model qwen2.5-72b      # å¯åŠ¨æŒ‡å®šæ¨¡å‹
  ./ew start -d                       # åå°å¯åŠ¨
  ./ew docker up --scale 2            # Docker åŒ Worker
  ./ew status                         # æŸ¥çœ‹çŠ¶æ€
```

## ï¿½ï¿½ æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP/SSE      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚     Gateway     â”‚
â”‚  (Browser)  â”‚                    â”‚  (Kotlin/Spring)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚ gRPC
                                            â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚     Worker      â”‚
                                   â”‚    (Python)     â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                       â”‚                       â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚ cuda-vllm â”‚          â”‚  cuda-trt   â”‚         â”‚ascend-vllm  â”‚
              â”‚   (vLLM)  â”‚          â”‚(TensorRT)   â”‚         â”‚(vLLM-Ascend)â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ï¿½ï¿½ é¡¹ç›®ç»“æ„

```
EW_AI_Deployment/
â”œâ”€â”€ ew                          # ğŸ”§ ç»Ÿä¸€ CLI å·¥å…·
â”œâ”€â”€ EW_AI_Backend/
â”‚   â”œâ”€â”€ gateway/                # Kotlin Gateway æœåŠ¡
â”‚   â”‚   â””â”€â”€ src/main/kotlin/    # Spring WebFlux + gRPC
â”‚   â”œâ”€â”€ worker/                 # Python Worker æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ engines/            # æ¨ç†å¼•æ“å®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ vllm_cuda_engine.py
â”‚   â”‚   â”‚   â”œâ”€â”€ trt_engine.py
â”‚   â”‚   â”‚   â”œâ”€â”€ vllm_ascend_engine.py
â”‚   â”‚   â”‚   â””â”€â”€ mindie_engine.py
â”‚   â”‚   â””â”€â”€ core/               # æ ¸å¿ƒç»„ä»¶
â”‚   â”œâ”€â”€ deploy/                 # éƒ¨ç½²é…ç½®
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”‚   â”œâ”€â”€ config.json         # æ¨¡å‹é…ç½®
â”‚   â”‚   â””â”€â”€ .env.example        # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”‚   â””â”€â”€ proto/                  # gRPC åè®®å®šä¹‰
â””â”€â”€ EW_AI_Training/             # è®­ç»ƒç›¸å…³ (å¯é€‰)
```

## âš™ï¸ é…ç½®

### ç¯å¢ƒå˜é‡

```bash
# æ ¸å¿ƒé…ç½®
EW_ENGINE=cuda-vllm          # æ¨ç†å¼•æ“
EW_PORT=8080                 # Gateway ç«¯å£
EW_MODEL=deepseek-v3         # é»˜è®¤æ¨¡å‹

# vLLM é…ç½®
VLLM_TP=1                    # å¼ é‡å¹¶è¡Œåº¦
VLLM_GPU_MEM=0.9             # GPU æ˜¾å­˜ä½¿ç”¨ç‡
```

å®Œæ•´é…ç½®å‚è§ `EW_AI_Backend/deploy/.env.example`ã€‚

### æ¨¡å‹é…ç½®

ç¼–è¾‘ `EW_AI_Backend/deploy/config.json`:

```json
{
  "models": {
    "my-model": {
      "engine": "cuda-vllm",
      "model_path": "organization/model-name",
      "max_model_len": 8192,
      "tensor_parallel_size": 1
    }
  }
}
```

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œé›†æˆæµ‹è¯•
./ew test integration

# è¿è¡Œå•å…ƒæµ‹è¯•
./ew test unit

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
./ew test all
```

## ğŸ“š æ–‡æ¡£ä¸è®¾è®¡

æœ¬ä»“åº“åŒ…å«ä»¥ä¸‹å…³é”®æ–‡æ¡£ï¼š
- `EW_AI_Backend/ARCHITECTURE.md` - æ¶æ„è¯´æ˜ï¼ˆGateway / Coordinator / Workerï¼‰
- `EW_AI_Backend/DEPLOY.md` - éƒ¨ç½²ä¸ Docker Compose è¯´æ˜
- `TESTING.md` - æµ‹è¯•è¯´æ˜ï¼ˆæœ¬åœ°ä¸ CIï¼‰
- `CONTRIBUTING.md` - æäº¤ä¸ç‰ˆæœ¬ç®¡ç†è§„åˆ™ï¼ˆå››æ®µå¼ç‰ˆæœ¬å· + åç¼€ï¼‰

å»ºè®®åœ¨æäº¤å‰è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥ç¡®ä¿æœ¬åœ°ç¯å¢ƒä¸€è‡´ï¼š

```bash
# Worker å•å…ƒæµ‹è¯•
cd EW_AI_Backend/worker
pytest tests/ -q

# Gateway å•å…ƒæµ‹è¯•ï¼ˆGradleï¼‰
cd ../gateway
./gradlew test

# Coordinator å•å…ƒæµ‹è¯•ï¼ˆGradleï¼‰
cd ../coordinator
./gradlew test
```


## ğŸ· ç‰ˆæœ¬å·è§„èŒƒä¸æäº¤æ ¼å¼

æœ¬é¡¹ç›®é‡‡ç”¨ **å››æ®µå¼ç‰ˆæœ¬å·** å¹¶é…åˆåç¼€æ¥æŒ‡ç¤ºç¨³å®šæ€§ï¼Œä¾‹å­ï¼š `[x.y.z.n-Alpha]`ã€‚

- ç¬¬ä¸€æ®µï¼ˆxï¼‰ï¼šé‡å¤§ã€**ä¸å…¼å®¹**å˜åŒ–ï¼ˆç ´åæ€§é‡æ„ï¼‰
- ç¬¬äºŒæ®µï¼ˆyï¼‰ï¼šå‘åå…¼å®¹çš„æ–°åŠŸèƒ½ï¼ˆfeatureï¼‰
- ç¬¬ä¸‰æ®µï¼ˆzï¼‰ï¼šBug ä¿®å¤ä¸ä¼˜åŒ–
- ç¬¬å››æ®µï¼ˆnï¼‰ï¼šæ„å»º/æµ‹è¯•æ¬¡æ•°ï¼ˆé€’å¢ï¼‰

åç¼€è¯´æ˜ï¼š
- `PreAlpha`ï¼šåŠŸèƒ½ä¸å®Œæ•´ã€ä»å¤„äºè®¾è®¡æ—©æœŸ
- `Alpha`ï¼šå¤§éƒ¨åˆ†åŠŸèƒ½å¯ç”¨ï¼Œå¼€å§‹ç¬¬ä¸€æ¬¡æµ‹è¯•
- `Beta`ï¼šåŠŸèƒ½å®ç°å®Œæ•´ï¼Œå±•å¼€æ›´å¹¿æ³›æµ‹è¯•
- `RC` / `Release`ï¼šå¯ç”¨äºç”Ÿäº§æˆ–å€™é€‰å‘å¸ƒ

æäº¤æ¶ˆæ¯ä¸ç‰ˆæœ¬å·æ ¼å¼ï¼ˆç¤ºä¾‹ï¼‰ï¼š

`[2.1.1.2-Alpha] refactor(worker): API, async, telemetry, security and performance improvements`

è¯·åœ¨æäº¤ä¸­åŒ…å«è‹±æ–‡ä¸ä¸­æ–‡è¯´æ˜ï¼ˆè‹±æ–‡åœ¨å‰ï¼Œç©ºä¸€è¡Œï¼Œéšåä¸­æ–‡ï¼‰ï¼Œå¹¶å°†ç‰ˆæœ¬å·ç”¨æ–¹æ‹¬å·å®Œæ•´åŒ…å›´åœ¨ä¸€è¡Œå¼€å¤´ï¼Œæ ‡é¢˜ä¸ç‰ˆæœ¬å·åŒä¸€è¡Œï¼ˆç‰ˆæœ¬å·åœ¨å‰ï¼‰ã€‚

æ›´å¤šè´¡çŒ®è§„èŒƒè§ `CONTRIBUTING.md`ã€‚

## ğŸ“ ç‰ˆæœ¬å†å²

- **[1.5.2.0]** - ç®€åŒ–éƒ¨ç½²æµç¨‹ï¼Œç»Ÿä¸€ CLI å·¥å…·ï¼Œæ”¯æŒå››ç§æ¨ç†å¼•æ“
- **[1.5.1.3-alpha]** - C++ å…¥å£ç‚¹ï¼Œå››å¼•æ“æ¶æ„å®ç°
- **[1.0.0-alpha]** - åˆå§‹ç‰ˆæœ¬ï¼ŒGateway + Worker åŸºç¡€æ¶æ„

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue æˆ– Pull Requestã€‚
