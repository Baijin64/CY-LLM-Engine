#!/usr/bin/env bash
#curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d "{\"model\": \"default\", \"messages\": [{\"role\": \"user\", \"content\": \"hello\"}], \"temperature\": 0.7}"
#export GATEWAY_SECURITY_API_KEY="111"
#export HF_ENDPOINT="https://hf-mirror.com"
# =============================================================================
# CY-LLM Engine - ç»Ÿä¸€éƒ¨ç½²å‘½ä»¤è¡Œå·¥å…·
# ç”¨æ³•: ./cy [command] [options] (é¦–é€‰: ./cy æˆ– ./cy-llm)
# =============================================================================
set -e

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# é¡¹ç›®è·¯å¾„
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
BACKEND_DIR="$SCRIPT_DIR/CY_LLM_Backend"
GATEWAY_DIR="$BACKEND_DIR/gateway"
WORKER_DIR="$BACKEND_DIR/worker"
DEPLOY_DIR="$BACKEND_DIR/deploy"
TRAINING_DIR="$SCRIPT_DIR/CY_LLM_Training"

# é»˜è®¤é…ç½®
DEFAULT_ENV_NAME="${CY_LLM_CONDA_ENV:-vllm}"
DEFAULT_PYTHON_VERSION="${CY_LLM_PYTHON_VERSION:-3.10}"
DEFAULT_PORT="${CY_LLM_PORT:-8080}"
DEFAULT_WORKER_PORT="${CY_LLM_WORKER_PORT:-50051}"
DEFAULT_LITE_PORT="${CY_LLM_LITE_PORT:-8000}"
DEFAULT_LITE_COORDINATOR_PORT="${CY_LLM_LITE_COORDINATOR_PORT:-50051}"
DEFAULT_LITE_WORKER_PORT="${CY_LLM_LITE_WORKER_PORT:-50052}"
DEFAULT_ENGINE="${CY_LLM_ENGINE:-cuda-vllm}"
DEFAULT_MODEL="${CY_LLM_MODEL:-facebook/opt-2.7b}"
DEFAULT_CHARACTER="${CY_LLM_CHARACTER:-èŠ™å®å¨œ}"
DEFAULT_BATCH_SIZE="${CY_LLM_BATCH_SIZE:-2}"
DEFAULT_GRAD_ACCUM="${CY_LLM_GRAD_ACCUM:-8}"
DEFAULT_EPOCHS="${CY_LLM_EPOCHS:-3}"

PIP_MIRROR="https://pypi.tuna.tsinghua.edu.cn/simple"
PIP_ARGS="--index-url $PIP_MIRROR --trusted-host pypi.tuna.tsinghua.edu.cn"

# å¼•æ“åˆ°ä¾èµ–æ–‡ä»¶çš„æ˜ å°„
declare -A ENGINE_REQUIREMENTS=(
    ["cuda-vllm"]="requirements-vllm.txt"
    ["cuda-vllm-async"]="requirements-vllm.txt"
    ["cuda-trt"]="requirements-trt.txt"
    ["nvidia"]="requirements-nvidia.txt"
    ["cuda"]="requirements-nvidia.txt"
    ["ascend-vllm"]="requirements_ascend.txt"
    ["ascend-mindie"]="requirements_ascend.txt"
)

# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

info()    { echo -e "${BLUE}[INFO]${NC} $*"; }
success() { echo -e "${GREEN}[OK]${NC} $*"; }
warn()    { echo -e "${YELLOW}[WARN]${NC} $*"; }
error()   { echo -e "${RED}[ERROR]${NC} $*" >&2; }
step()    { echo -e "\n${CYAN}â”â”â” $* â”â”â”${NC}"; }

check_command() {
    command -v "$1" &>/dev/null
}

get_conda() {
    if check_command conda; then
        echo "conda"
    elif check_command mamba; then
        echo "mamba"
    elif [[ -f "$HOME/miniforge3/bin/conda" ]]; then
        echo "$HOME/miniforge3/bin/conda"
    elif [[ -f "$HOME/miniconda3/bin/conda" ]]; then
        echo "$HOME/miniconda3/bin/conda"
    else
        error "Conda/Mamba æœªæ‰¾åˆ°ï¼Œè¯·å…ˆå®‰è£… Miniforge/Miniconda"
        exit 1
    fi
}

get_python_path() {
    local env_name="${1:-$DEFAULT_ENV_NAME}"
    local conda_cmd=$(get_conda)
    $conda_cmd run -n "$env_name" which python 2>/dev/null
}

# =============================================================================
# å‘½ä»¤: setup - åˆå§‹åŒ–ç¯å¢ƒ
# =============================================================================
cmd_setup() {
    local env_name="$DEFAULT_ENV_NAME"
    local python_version="$DEFAULT_PYTHON_VERSION"
    local engine="$DEFAULT_ENGINE"
    local skip_gradle=false
    local skip_deps=false

    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)      env_name="$2"; shift 2 ;;
            --python)   python_version="$2"; shift 2 ;;
            --engine)   engine="$2"; shift 2 ;;
            --skip-gradle) skip_gradle=true; shift ;;
            --skip-deps)   skip_deps=true; shift ;;
            *) shift ;;
        esac
    done

    # éªŒè¯å¼•æ“ç±»å‹
    if [[ ! -v ENGINE_REQUIREMENTS["$engine"] ]]; then
        error "ä¸æ”¯æŒçš„å¼•æ“ç±»å‹: $engine"
        echo "æ”¯æŒçš„å¼•æ“: ${!ENGINE_REQUIREMENTS[@]}"
        exit 1
    fi

    local req_file="${ENGINE_REQUIREMENTS[$engine]}"

    step "ğŸš€ CY-LLM Engine ç¯å¢ƒåˆå§‹åŒ–"
    info "å¼•æ“: $engine"
    info "ç¯å¢ƒåç§°: $env_name"
    info "ä¾èµ–æ–‡ä»¶: $req_file"
    local conda_cmd=$(get_conda)
    info "ä½¿ç”¨åŒ…ç®¡ç†å™¨: $conda_cmd"

    # 1. åˆ›å»º/æ£€æŸ¥ Conda ç¯å¢ƒ
    step "1/3 Python ç¯å¢ƒ"
    if $conda_cmd env list | grep -q "^$env_name "; then
        success "ç¯å¢ƒ '$env_name' å·²å­˜åœ¨"
    else
        info "åˆ›å»ºç¯å¢ƒ '$env_name' (Python $python_version)..."
        $conda_cmd create -n "$env_name" python="$python_version" -y
        success "ç¯å¢ƒåˆ›å»ºå®Œæˆ"
    fi

    # 2. å®‰è£… Python ä¾èµ–
    if [[ "$skip_deps" != true ]]; then
        step "2/3 å®‰è£…ä¾èµ–: $req_file"
        if [[ -f "$SCRIPT_DIR/$req_file" ]]; then
            info "å®‰è£… Python ä¾èµ–..."

            # æŒ‰è¡Œå®‰è£… requirementsï¼Œä»¥ä¾¿è¾“å‡ºæ¯ä¸ªåŒ…çš„å®‰è£…çŠ¶æ€ã€‚
            # ç©ºè¡Œã€æ³¨é‡Šè¡Œå’Œä»¥ - å¼€å¤´çš„ pip é€‰é¡¹å°†è¢«è·³è¿‡ï¼ˆä¿ç•™ç»™æ•´ä½“å®‰è£…å›é€€ï¼‰ã€‚
            install_failed=false
            while IFS= read -r req_line || [[ -n "$req_line" ]]; do
                # å»æ‰é¦–å°¾ç©ºç™½
                line_trimmed=$(echo "$req_line" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if [[ -z "$line_trimmed" || "$line_trimmed" =~ ^# ]]; then
                    continue
                fi
                # å¦‚æœæ˜¯ pip é€‰é¡¹ï¼ˆä¾‹å¦‚ --extra-index-url æˆ– -fï¼‰ï¼Œè·³è¿‡é€è¡Œå®‰è£…
                if [[ "$line_trimmed" =~ ^- ]]; then
                    continue
                fi

                # æ˜¾ç¤ºå®‰è£…å¼€å§‹ä¿¡æ¯ï¼ˆå°½é‡æå–åŒ…å==ç‰ˆæœ¬æˆ–æ˜¾ç¤ºæ•´è¡Œï¼‰
                echo -e "${YELLOW}æ­£åœ¨å®‰è£… ${line_trimmed}${NC}"

                if ! $conda_cmd run -n "$env_name" pip install --no-cache-dir \
                    $PIP_ARGS \
                    "$line_trimmed" -q; then
                    warn "å®‰è£… ${line_trimmed} å¤±è´¥ï¼Œå›é€€ä½¿ç”¨æ•´ä½“ requirements å®‰è£…ä»¥è·å¾—å®Œæ•´é”™è¯¯ä¿¡æ¯"
                    install_failed=true
                    break
                fi

                echo -e "${GREEN}å®‰è£…å®Œæˆ: ${line_trimmed}${NC}"
            done < "$SCRIPT_DIR/$req_file"

            if $install_failed; then
                # å›é€€åˆ°ä¸€æ¬¡æ€§å®‰è£…ä»¥æ˜¾ç¤º pip çš„å®Œæ•´é”™è¯¯è¾“å‡º
                $conda_cmd run -n "$env_name" pip install \
                $PIP_ARGS \
                -r "$SCRIPT_DIR/$req_file"
            fi

            success "ä¾èµ–å®‰è£…å®Œæˆ"
        else
            warn "ä¾èµ–æ–‡ä»¶ä¸å­˜åœ¨: $req_file"
        fi

        # ä¸º TRT æ·»åŠ ç‰¹æ®Šæç¤º
        if [[ "$engine" == "cuda-trt" ]]; then
            echo ""
            echo "${YELLOW}âš ï¸  TensorRT-LLM éœ€è¦æ‰‹åŠ¨å®‰è£…${NC}"
            echo "  æ–¹æ³• 1: pip install tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com"
            echo "  æ–¹æ³• 2: ä½¿ç”¨ NGC å®¹å™¨ nvcr.io/nvidia/tensorrt-llm:latest"
            echo "  æ–‡æ¡£: https://github.com/NVIDIA/TensorRT-LLM"
            echo ""
        fi
    else
        info "è·³è¿‡ä¾èµ–å®‰è£…"
    fi
    success "ä¾èµ–å®‰è£…å®Œæˆ"
    
    # 3. æ„å»º Gateway (å¯é€‰)
    if [[ "$skip_gradle" == false ]]; then
        step "2/3 Gateway æ„å»º"
        if check_command java; then
            info "æ„å»º Gateway JAR..."
            (cd "$GATEWAY_DIR" && ./gradlew build -x test --quiet)
            success "Gateway æ„å»ºå®Œæˆ"
        else
            warn "Java æœªå®‰è£…ï¼Œè·³è¿‡ Gateway æ„å»º"
        fi
    else
        info "è·³è¿‡ Gateway æ„å»º"
    fi
    
    # 4. å®Œæˆ
    step "3/3 éªŒè¯"
    local python_path=$(get_python_path "$env_name")
    success "Python: $python_path"
    
    echo ""
    success "âœ¨ ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼"
    echo ""
    echo -e "ä¸‹ä¸€æ­¥: ${CYAN}./cy-llm start${NC} å¯åŠ¨æœåŠ¡"
    echo -e "        ${CYAN}./cy-llm worker${NC} ä»…å¯åŠ¨ Worker"
}

# =============================================================================
# å‘½ä»¤: start - å¯åŠ¨å®Œæ•´æœåŠ¡ (Gateway + Worker)
# =============================================================================
cmd_start() {
    local env_name="$DEFAULT_ENV_NAME"
    local port="$DEFAULT_PORT"
    local worker_port="$DEFAULT_WORKER_PORT"
    local engine="$DEFAULT_ENGINE"
    local daemon=false
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)      env_name="$2"; shift 2 ;;
            --port)     port="$2"; shift 2 ;;
            --worker-port) worker_port="$2"; shift 2 ;;
            --engine)   engine="$2"; shift 2 ;;
            -d|--daemon) daemon=true; shift ;;
            *) shift ;;
        esac
    done

    step "ğŸŒŸ å¯åŠ¨ CY-LLM Engine æœåŠ¡"
    
    local python_path=$(get_python_path "$env_name")
    if [[ -z "$python_path" ]]; then
        error "ç¯å¢ƒ '$env_name' æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ: ./cy setup"
        exit 1
    fi
    
    local jar_path="$GATEWAY_DIR/build/libs/gateway-0.0.1-SNAPSHOT.jar"
    if [[ ! -f "$jar_path" ]]; then
        error "Gateway JAR æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ: ./cy setup"
        exit 1
    fi
    
    info "Engine: $engine"
    info "Gateway Port: $port"
    info "Worker Port: $worker_port"
    info "Python: $python_path"

    # ç»Ÿä¸€ç¯å¢ƒå˜é‡ï¼ˆä»…ä½¿ç”¨ CY_LLM_*ï¼‰
    export CY_LLM_ENGINE="$engine"

    # Ensure VLLM GPU memory env vars are set for local starts (default to 0.8)
    export VLLM_GPU_MEM=${VLLM_GPU_MEM:-0.8}
    export VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-$VLLM_GPU_MEM}

    # CY_LLM model registry path
    export CY_LLM_MODEL_REGISTRY_PATH="${CY_LLM_MODEL_REGISTRY_PATH:-\"$SCRIPT_DIR/CY_LLM_Backend/deploy/models.json\"}"
    
    if [[ "$daemon" == true ]]; then
        info "åå°æ¨¡å¼å¯åŠ¨..."
        java -jar "$jar_path" \
            --server.port="$port" \
            --self-deploy.enabled=true \
            --self-deploy.python-executable="$python_path" \
            --self-deploy.worker-dir="$WORKER_DIR" \
            --self-deploy.worker-port="$worker_port" \
            > /tmp/cy-llm-gateway.log 2>&1 &
        echo $! > /tmp/cy-llm-gateway.pid
        success "Gateway PID: $(cat /tmp/cy-llm-gateway.pid)"
        success "æ—¥å¿—: /tmp/cy-llm-gateway.log"
    else
        java -jar "$jar_path" \
            --server.port="$port" \
            --self-deploy.enabled=true \
            --self-deploy.python-executable="$python_path" \
            --self-deploy.worker-dir="$WORKER_DIR" \
            --self-deploy.worker-port="$worker_port"
    fi
}

# =============================================================================
# å‘½ä»¤: worker - ä»…å¯åŠ¨ Worker
# =============================================================================
cmd_worker() {
    local env_name="$DEFAULT_ENV_NAME"
    local port="$DEFAULT_WORKER_PORT"
    local engine="$DEFAULT_ENGINE"
    local model=""
    local serve=true
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)      env_name="$2"; shift 2 ;;
            --port)     port="$2"; shift 2 ;;
            --engine)   engine="$2"; shift 2 ;;
            --model)    model="$2"; shift 2 ;;
            --no-serve) serve=false; shift ;;
            *) shift ;;
        esac
    done

    step "ğŸ”§ å¯åŠ¨ Worker"
    
    local conda_cmd=$(get_conda)
    
    export CY_LLM_ENGINE="$engine"
    [[ -n "$model" ]] && export CY_LLM_MODEL="$model"
    
    info "Engine: $engine"
    info "Port: $port"
    
    local serve_flag=""
    [[ "$serve" == true ]] && serve_flag="--serve"
    
    $conda_cmd run -n "$env_name" python "$WORKER_DIR/main.py" \
        --port "$port" \
        ${model:+--model "$model"} \
        $serve_flag
}

# =============================================================================
# å‘½ä»¤: lite - å¯åŠ¨è½»é‡ç‰ˆæœåŠ¡ (Gateway Lite + Coordinator Lite + Worker)
# =============================================================================
cmd_lite() {
    local env_name="$DEFAULT_ENV_NAME"
    local port="$DEFAULT_LITE_PORT"
    local coordinator_port="$DEFAULT_LITE_COORDINATOR_PORT"
    local worker_port="$DEFAULT_LITE_WORKER_PORT"
    local engine="$DEFAULT_ENGINE"
    local model="${CY_LLM_MODEL:-}" # ä»ç¯å¢ƒå˜é‡è¯»å–é»˜è®¤å€¼
    local adapter=""
    local daemon=false

    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)      env_name="$2"; shift 2 ;;
            --port)     port="$2"; shift 2 ;;
            --coordinator-port) coordinator_port="$2"; shift 2 ;;
            --worker-port) worker_port="$2"; shift 2 ;;
            --engine)   engine="$2"; shift 2 ;;
            --model)    model="$2"; shift 2 ;;
            --adapter|--checkpoint) adapter="$2"; shift 2 ;;
            -d|--daemon) daemon=true; shift ;;
            *) shift ;;
        esac
    done

    # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„æ¨¡å‹ (å…¥å‚ > ç¯å¢ƒå˜é‡ > é»˜è®¤æ¨¡å‹)
    local target_model="${model:-$DEFAULT_MODEL}"

    step "âš¡ å¯åŠ¨è½»é‡ç‰ˆæœåŠ¡ (Gateway Lite + Coordinator Lite + Worker)"

    local conda_cmd=$(get_conda)
    local python_path=$(get_python_path "$env_name")
    if [[ -z "$python_path" ]]; then
        error "ç¯å¢ƒ '$env_name' æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ: ./cy-llm setup"
        exit 1
    fi

    info "Engine: $engine"
    info "Gateway Lite Port: $port"
    info "Coordinator Port: $coordinator_port"
    info "Worker Port: $worker_port"
    info "Python: $python_path"

    # æ£€æŸ¥å¹¶å°è¯•å®‰è£… Lite ä¾èµ–
    if ! $conda_cmd run -n "$env_name" python -c "import fastapi, uvicorn" &>/dev/null; then
        warn "æ£€æµ‹åˆ°ç¼ºå°‘ Lite ä¾èµ–ï¼Œæ­£åœ¨å°è¯•å®‰è£…..."
        $conda_cmd run -n "$env_name" pip install -r "$BACKEND_DIR/gateway_lite/requirements.txt" -r "$BACKEND_DIR/coordinator_lite/requirements.txt"
    fi

    export CY_LLM_ENGINE="$engine"
    export CY_LLM_DEFAULT_MODEL="$target_model"
    
    # è®¾ç½®æ¨¡å‹æ³¨å†Œè¡¨è·¯å¾„ï¼Œç¡®ä¿ Worker èƒ½è¯»åˆ°æ¨¡å‹é…ç½®
    export CY_LLM_MODEL_REGISTRY_PATH="${CY_LLM_MODEL_REGISTRY_PATH:-$BACKEND_DIR/deploy/models.json}"
    
    if [[ -n "$adapter" ]]; then
        if [[ ! -d "$adapter" && -d "$TRAINING_DIR/checkpoints/$adapter" ]]; then
            adapter="$TRAINING_DIR/checkpoints/$adapter"
        fi
        
        # å¦‚æœæ˜¯ä¸€ä¸ªç›®å½•ï¼Œå°è¯•æŸ¥æ‰¾æœ€æ–°çš„ checkpoint-xxx å­ç›®å½•
        if [[ -d "$adapter" ]]; then
            local latest=$(ls -d "$adapter"/checkpoint-* 2>/dev/null | sort -V | tail -n 1)
            if [[ -n "$latest" ]]; then
                adapter="$latest"
                info "è‡ªåŠ¨å®šä½æœ€æ–°è®­ç»ƒèŠ‚ç‚¹: $adapter"
            else
                info "ä½¿ç”¨ Checkpoint ç›®å½•: $adapter"
            fi
        fi
        export CY_LLM_DEFAULT_ADAPTER="$adapter"
    fi
    
    export PYTHONPATH="$BACKEND_DIR"

    local coordinator_log="/tmp/cy-llm-lite-coordinator.log"
    local gateway_log="/tmp/cy-llm-lite-gateway.log"
    local worker_log="/tmp/cy-llm-lite-worker.log"

    local coord_env=(
        "PYTHONPATH=$BACKEND_DIR"
        "COORDINATOR_GRPC_BIND=0.0.0.0:${coordinator_port}"
        "WORKER_GRPC_ADDR=127.0.0.1:${worker_port}"
    )

    local gw_env=(
        "PYTHONPATH=$BACKEND_DIR"
        "COORDINATOR_GRPC_ADDR=127.0.0.1:${coordinator_port}"
        "GATEWAY_REQUEST_TIMEOUT=300"
    )

    # å‡†å¤‡ Worker å¯åŠ¨å‚æ•°
    local worker_args=("--port" "$worker_port" "--serve" "--model" "$target_model")

    if [[ "$daemon" == true ]]; then
        info "åå°æ¨¡å¼å¯åŠ¨..."
        (env "${coord_env[@]}" "$python_path" -u -m coordinator_lite.app.server > "$coordinator_log" 2>&1 & echo $! > /tmp/cy-llm-lite-coordinator.pid)
        (env "${gw_env[@]}" "$python_path" -m uvicorn gateway_lite.app.main:app --host 0.0.0.0 --port "$port" > "$gateway_log" 2>&1 & echo $! > /tmp/cy-llm-lite-gateway.pid)
        ("$python_path" -u -m worker.main "${worker_args[@]}" > "$worker_log" 2>&1 & echo $! > /tmp/cy-llm-lite-worker.pid)

        success "Coordinator PID: $(cat /tmp/cy-llm-lite-coordinator.pid)"
        success "Gateway PID: $(cat /tmp/cy-llm-lite-gateway.pid)"
        success "Worker PID: $(cat /tmp/cy-llm-lite-worker.pid)"
        return
    fi

    info "å‰å°æ¨¡å¼å¯åŠ¨ (Ctrl+C é€€å‡º)"
    info "ç›‘æ§æ—¥å¿—å¯é€šè¿‡ tail -f $gateway_log æŸ¥çœ‹"
    
    # æ˜¾å¼å¯åŠ¨èƒŒæ™¯è¿›ç¨‹å¹¶æ•è·é”™è¯¯
    info "æ­£åœ¨å¯åŠ¨ç»„ä»¶..."
    export CY_LLM_HEALTH_PORT=$((worker_port + 1000)) # é¿å… 9090 å†²çª

    env "${coord_env[@]}" "$python_path" -u -m coordinator_lite.app.server > "$coordinator_log" 2>&1 &
    local coord_pid=$!
    
    "$python_path" -u -m worker.main "${worker_args[@]}" > "$worker_log" 2>&1 &
    local worker_pid=$!

    trap 'kill $coord_pid $worker_pid 2>/dev/null || true; exit' INT TERM EXIT

    # ç­‰å¾… Worker å¯åŠ¨å¹¶è¾“å‡ºå°±ç»ªçŠ¶æ€
    info "æ­£åœ¨ç­‰å¾…æœ¬åœ°æ¨ç†å¼•æ“å‡†å¤‡å°±ç»ª..."
    local retry=0
    local max_retries=600  # å¢åŠ åˆ° 10 åˆ†é’Ÿï¼Œç»™æ¨¡å‹ä¸‹è½½ç•™å‡ºæ—¶é—´
    local ready=false
    while [[ $retry -lt $max_retries ]]; do
        if grep -q "Worker gRPC æœåŠ¡å·²å¯åŠ¨" "$worker_log" 2>/dev/null; then
            ready=true
            break
        fi

        # æ ¸å¿ƒæ”¹è¿›ï¼šå®æ—¶æ£€æµ‹æ˜¾å­˜ä¸è¶³å…³é”®å­—å¹¶ä¸­æ­¢
        if grep -qiE "æ˜¾å­˜ä¸è¶³|OutOfMemory|OOM|Insufficient VRAM" "$worker_log" 2>/dev/null; then
            echo ""
            error "æ£€æµ‹åˆ°æ˜¾å­˜ä¸è¶³ (VRAM Insufficient)ï¼"
            warn "æœ€è¿‘çš„é”™è¯¯è¯¦ç»†ä¿¡æ¯:"
            grep -iE "æ˜¾å­˜ä¸è¶³|OutOfMemory|OOM|Insufficient VRAM" "$worker_log" | tail -n 5 | sed 's/^/  /'
            info "æ­£åœ¨æ¸…ç†æ®‹ç•™è¿›ç¨‹..."
            cmd_stop
            exit 1
        fi
        
        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
        if ! kill -0 $worker_pid 2>/dev/null; then
            echo ""
            error "Worker è¿›ç¨‹å·²æ„å¤–é€€å‡ºï¼"
            warn "æœ€è¿‘çš„é”™è¯¯æ—¥å¿— ($worker_log):"
            tail -n 20 "$worker_log" | sed 's/^/  /'
            info "æ­£åœ¨æ¸…ç†æ®‹ç•™è¿›ç¨‹..."
            cmd_stop
            exit 1
        fi
        if ! kill -0 $coord_pid 2>/dev/null; then
            echo ""
            error "Coordinator è¿›ç¨‹å·²æ„å¤–é€€å‡ºï¼"
            warn "æœ€è¿‘çš„é”™è¯¯æ—¥å¿— ($coordinator_log):"
            tail -n 20 "$coordinator_log" | sed 's/^/  /'
            info "æ­£åœ¨æ¸…ç†æ®‹ç•™è¿›ç¨‹..."
            cmd_stop
            exit 1
        fi

        sleep 1
        retry=$((retry + 1))
        echo -n "."
    done
    echo ""
    
    if [[ "$ready" != true ]]; then
        error "Worker å¯åŠ¨è¶…æ—¶ ($max_retries ç§’)ã€‚"
        warn "è¯·æ£€æŸ¥æ—¥å¿—: $worker_log"
        
        # å°è¯•è¾“å‡ºæœ€åå‡ è¡Œæ—¥å¿—ä»¥ä¾›å‚è€ƒ
        if [[ -s "$worker_log" ]]; then
            warn "Worker æœ€åæ—¥å¿—å†…å®¹:"
            tail -n 15 "$worker_log" | sed 's/^/  /'
        else
            warn "Worker æ—¥å¿—æ–‡ä»¶ä¸ºç©ºï¼Œè¿›ç¨‹å¯èƒ½å°šæœªè¾“å‡ºä»»ä½•å†…å®¹æˆ–ç¯å¢ƒå¯åŠ¨è¿‡æ…¢ã€‚"
        fi

        warn "æç¤º: å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å¯åŠ¨ï¼Œé©±åŠ¨æˆ–æ¨¡å‹ (${model:-$DEFAULT_MODEL}) å¯èƒ½æ­£åœ¨åˆå§‹åŒ–/ä¸‹è½½ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥ç½‘ç»œæˆ–æ˜¾å­˜ã€‚"
        
        # è‡ªåŠ¨æ‰§è¡Œåœæ­¢æ“ä½œ
        info "è¶…æ—¶ï¼Œæ­£åœ¨è‡ªåŠ¨æ¸…ç†æ®‹ç•™è¿›ç¨‹..."
        cmd_stop
        exit 1
    fi
    
    success "ğŸš€ æ‰€æœ‰ç»„ä»¶å·²å°±ç»ªï¼"
    success "  - Gateway: http://0.0.0.0:$port (OpenAI API)"
    success "  - Coordinator: $coordinator_port (gRPC)"
    success "  - Worker: $worker_port (gRPC)"
    echo -e "ä½ å¯ä»¥ç°åœ¨å¼€å§‹å‘é€è¯·æ±‚ã€‚æŒ‰ ${CYAN}Ctrl+C${NC} åœæ­¢æ‰€æœ‰æœåŠ¡ã€‚\n"

    # å‰å°å¯åŠ¨ç½‘å…³ï¼Œæ–¹ä¾¿ç›´æ¥çœ‹æŠ¥é”™
    env "${gw_env[@]}" "$python_path" -m uvicorn gateway_lite.app.main:app --host 0.0.0.0 --port "$port"
}

# =============================================================================
# å‘½ä»¤: docker - Docker Compose éƒ¨ç½²
# =============================================================================
cmd_docker() {
    local action="${1:-up}"
    shift || true
    local engine="$DEFAULT_ENGINE"
    local scale=1
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --engine)   engine="$2"; shift 2 ;;
            --scale)    scale="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    step "ğŸ³ Docker éƒ¨ç½²"
    
    cd "$DEPLOY_DIR"
    
    export CY_LLM_ENGINE="$engine"
        export CY_LLM_ENGINE="$engine"
    
    case "$action" in
        up)
            info "å¯åŠ¨å®¹å™¨ (Engine: $engine, Scale: $scale)..."
            case "$engine" in
                ascend-*) docker compose --profile ascend up -d --scale worker-ascend="$scale" ;;
                *)        docker compose up -d --scale worker-nvidia="$scale" ;;
            esac
            success "å®¹å™¨å¯åŠ¨å®Œæˆ"
            docker compose ps
            ;;
        down)
            info "åœæ­¢å®¹å™¨..."
            docker compose --profile ascend down
            success "å®¹å™¨å·²åœæ­¢"
            ;;
        logs)
            docker compose logs -f
            ;;
        ps)
            docker compose ps
            ;;
        build)
            info "æ„å»ºé•œåƒ..."
            docker compose build
            success "é•œåƒæ„å»ºå®Œæˆ"
            ;;
        *)
            error "æœªçŸ¥æ“ä½œ: $action"
            echo "å¯ç”¨æ“ä½œ: up, down, logs, ps, build"
            exit 1
            ;;
    esac
}

# =============================================================================
# å‘½ä»¤: stop - åœæ­¢æœåŠ¡
# =============================================================================
cmd_stop() {
    step "ğŸ›‘ åœæ­¢æœåŠ¡"
    
    if [[ -f /tmp/cy-llm-gateway.pid ]]; then
        local pid=$(cat /tmp/cy-llm-gateway.pid)
        if kill -0 "$pid" 2>/dev/null; then
            kill "$pid"
            rm /tmp/cy-llm-gateway.pid
            success "Gateway (PID $pid) å·²åœæ­¢"
        fi
    fi
    
    # æŸ¥æ‰¾å¹¶åœæ­¢æ‰€æœ‰ç›¸å…³è¿›ç¨‹
    pkill -f "gateway-.*-SNAPSHOT.jar" 2>/dev/null && info "Legacy Gateway è¿›ç¨‹å·²åœæ­¢" || true
    pkill -f "worker/main.py" 2>/dev/null && info "Worker è¿›ç¨‹å·²åœæ­¢" || true
    pkill -f "coordinator_lite.app.server" 2>/dev/null && info "Coordinator Lite è¿›ç¨‹å·²åœæ­¢" || true
    pkill -f "gateway_lite.app.main" 2>/dev/null && info "Gateway Lite è¿›ç¨‹å·²åœæ­¢" || true
    pkill -f "uvicorn.*gateway_lite" 2>/dev/null && info "Uvicorn Gateway è¿›ç¨‹å·²åœæ­¢" || true

    # æ¸…ç† PID æ–‡ä»¶
    rm -f /tmp/cy-llm-lite-*.pid
    
    success "æœåŠ¡å·²åœæ­¢"
}

# =============================================================================
# å‘½ä»¤: status - æŸ¥çœ‹çŠ¶æ€
# =============================================================================
cmd_status() {
    step "ğŸ“Š æœåŠ¡çŠ¶æ€"
    
    echo -e "\n${CYAN}è¿›ç¨‹çŠ¶æ€:${NC}"
    if pgrep -f "gateway-.*-SNAPSHOT.jar" >/dev/null; then
        success "Gateway: è¿è¡Œä¸­"
    else
        warn "Gateway: æœªè¿è¡Œ"
    fi
    
    if pgrep -f "worker/main.py" >/dev/null; then
        success "Worker: è¿è¡Œä¸­"
    else
        warn "Worker: æœªè¿è¡Œ"
    fi
    
    echo -e "\n${CYAN}ç«¯å£ç›‘å¬:${NC}"
    ss -tlnp 2>/dev/null | grep -E ":8080|:50051" || echo "  æ— ç›¸å…³ç«¯å£ç›‘å¬"
    
    echo -e "\n${CYAN}ç¯å¢ƒå˜é‡:${NC}"
    echo "  CY_LLM_ENGINE=${CY_LLM_ENGINE:-æœªè®¾ç½®}"
    echo "  CY_LLM_CONDA_ENV=${CY_LLM_CONDA_ENV:-$DEFAULT_ENV_NAME}"
}

cmd_diagnose() {
    local model_name="${CY_LLM_MODEL:-deepseek-v3}"
    local verbose=false

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --model)
                model_name="$2"
                shift 2
                ;;
            --verbose|-v)
                verbose=true
                shift
                ;;
            *)
                model_name="$1"
                shift
                ;;
        esac
    done

    step "ğŸ©º CY-LLM ç¯å¢ƒè¯Šæ–­: $model_name"

    export CY_LLM_MODEL="$model_name"
    export CY_LLM_MODEL_REGISTRY_PATH="$BACKEND_DIR/deploy/config.json"
    export CY_LLM_VERBOSE="$verbose"

    PYTHONPATH="$BACKEND_DIR" CY_LLM_MODEL="$model_name" CY_LLM_VERBOSE="$verbose" \
        python - <<'PY'
import os
from worker.config.config_loader import load_worker_config
from worker.utils.diagnostic import (
    check_vram_for_model,
    format_vram_report,
    gather_gpu_summary,
    check_dependencies,
    format_dependency_summary,
    check_version_compatibility,
    format_compatibility_warnings,
)
from worker.utils.vram_optimizer import suggest_batch_optimization, suggest_kv_cache_strategy, get_vram_stats

verbose = os.environ.get("CY_LLM_VERBOSE", "false").lower() == "true"

config = load_worker_config(registry_path=os.environ.get("CY_LLM_MODEL_REGISTRY_PATH"))
model_id = os.environ["CY_LLM_MODEL"]
spec = config.model_registry.get(model_id)
if spec is None:
    raise SystemExit(f"æ¨¡å‹ {model_id} æœªåœ¨æ³¨å†Œè¡¨ä¸­å®šä¹‰")

gpu = gather_gpu_summary()
print("CY-LLM ç¯å¢ƒè¯Šæ–­æŠ¥å‘Š")
print("=====================")
print("ç¡¬ä»¶ä¿¡æ¯:")
if gpu.get("available"):
    print(f"  GPU: {gpu.get('name')} ({gpu.get('total')} æ€», {gpu.get('free')} å¯ç”¨)")
    print(f"  CUDA: {gpu.get('cuda')}")
else:
    print("  GPU: æœªæ£€æµ‹åˆ°å¯ç”¨ CUDA è®¾å¤‡")

# å¯¹æ¨¡å‹è·¯å¾„è¿›è¡Œè„±æ•å¤„ç†ï¼ˆé verbose æ¨¡å¼ï¼‰
def sanitize_path(path: str) -> str:
    """éšè—å®Œæ•´è·¯å¾„ï¼Œä»…æ˜¾ç¤ºæ¨¡å‹åç§°"""
    if not verbose and path:
        parts = path.split("/")
        if len(parts) > 2:
            return f".../{parts[-2]}/{parts[-1]}" if len(parts[-1]) else f".../{parts[-2]}"
        return parts[-1] if parts else path
    return path

# æ˜¾ç¤ºæ¨¡å‹è·¯å¾„ï¼ˆè„±æ•ï¼‰
model_path_display = sanitize_path(spec.model_path)
if not verbose:
    print("  (ä½¿ç”¨ --verbose æŸ¥çœ‹å®Œæ•´è·¯å¾„)")

report = check_vram_for_model(model_id, spec)
print("")
print(f"æ¨¡å‹æ£€æµ‹: {model_id}")
print(format_vram_report(report))

print("\nä¾èµ–æ£€æŸ¥:")
statuses = check_dependencies()
for line in format_dependency_summary(statuses):
    print(line)

print("\nç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥:")
compat_warnings = check_version_compatibility()
for line in format_compatibility_warnings(compat_warnings):
    print(line)

# KV Cache ç­–ç•¥å»ºè®®
print("\nKV Cache é¢„åˆ†é…ç­–ç•¥:")
free_gb, total_gb = get_vram_stats()
engine_value = getattr(spec.engine, "value", spec.engine)
engine_type = (engine_value or "cuda-vllm").lower()
kv_suggestions = suggest_kv_cache_strategy(
    kv_cache_gb=report.kv_cache_gb,
    available_vram_gb=free_gb,
    max_model_len=spec.max_model_len or 8192,
    current_gpu_util=spec.gpu_memory_utilization or 0.75,
    expected_qps=None,  # å¯ä»¥ä»ç¯å¢ƒå˜é‡è¯»å–
    engine_type=engine_type
)
if kv_suggestions:
    for suggestion in kv_suggestions:
        print(f"  {suggestion}")
else:
    print("  ï¼ˆå½“å‰é…ç½®åˆç†ï¼‰")

# æ‰¹å¤„ç†ä¼˜åŒ–å»ºè®®
print("\næ‰¹å¤„ç†ä¼˜åŒ–å»ºè®®:")
batch_suggestions = suggest_batch_optimization(
    engine_type=engine_type,
    kv_cache_gb=report.kv_cache_gb,
    available_vram_gb=free_gb,
    max_model_len=spec.max_model_len or 8192
)
if batch_suggestions:
    for suggestion in batch_suggestions:
        print(f"  {suggestion}")
else:
    print("  ï¼ˆå½“å‰å¼•æ“æ— ç‰¹å®šæ‰¹å¤„ç†ä¼˜åŒ–å»ºè®®ï¼‰")
PY
}

# =============================================================================
# å‘½ä»¤: test - è¿è¡Œæµ‹è¯•
# =============================================================================
cmd_test() {
    local env_name="$DEFAULT_ENV_NAME"
    local type="${1:-all}"
    shift || true
    local sub_test="$1"
    
    step "ğŸ§ª è¿è¡Œæµ‹è¯•"
    
    local conda_cmd=$(get_conda)
    
    case "$type" in
        integration|int)
            if [[ -n "$sub_test" ]]; then
                info "è¿è¡ŒæŒ‡å®šé›†æˆæµ‹è¯•: $sub_test..."
                $conda_cmd run -n "$env_name" python -u "$BACKEND_DIR/tests/test_integration.py" --test "$sub_test"
            else
                info "è¿è¡Œé›†æˆæµ‹è¯• (é»˜è®¤: integration)..."
                # å¦‚æœæ²¡æœ‰å­æµ‹è¯•åï¼Œé»˜è®¤è¿è¡Œåä¸º integration çš„æ ¸å¿ƒæµ‹è¯•ï¼Œä»¥é¿å…äº¤äº’å¤±è´¥
                $conda_cmd run -n "$env_name" python -u "$BACKEND_DIR/tests/test_integration.py" --test integration
            fi
            ;;
        unit)
            info "è¿è¡Œå•å…ƒæµ‹è¯•..."
            # æ£€æŸ¥å¹¶å®‰è£…æµ‹è¯•ä¾èµ–
            local missing_deps=()
            if ! $conda_cmd run -n "$env_name" python -c "import pytest" 2>/dev/null; then
                missing_deps+=("pytest")
            fi
            if ! $conda_cmd run -n "$env_name" python -c "import pytest_asyncio" 2>/dev/null; then
                missing_deps+=("pytest-asyncio")
            fi
            
            if [ ${#missing_deps[@]} -gt 0 ]; then
                warn "ç¼ºå°‘æµ‹è¯•ä¾èµ–: ${missing_deps[*]}ï¼Œæ­£åœ¨å®‰è£…..."
                $conda_cmd run -n "$env_name" pip install "${missing_deps[@]}" -q
            fi
            
            # è¿è¡Œ worker å•å…ƒæµ‹è¯•
            if [[ -d "$WORKER_DIR/tests" ]]; then
                $conda_cmd run -n "$env_name" python -m pytest "$WORKER_DIR/tests" -v --tb=short
            else
                warn "æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: $WORKER_DIR/tests"
                exit 1
            fi
            ;;
        all)
            cmd_test integration
            cmd_test unit
            ;;
        *)
            error "æœªçŸ¥æµ‹è¯•ç±»å‹: $type"
            echo "å¯ç”¨ç±»å‹: integration, unit, all"
            exit 1
            ;;
    esac
}

# =============================================================================
# å‘½ä»¤: models - æ¨¡å‹ç®¡ç†
# =============================================================================
cmd_models() {
    local env_name="$DEFAULT_ENV_NAME"
    local action="${1:-list}"

    local conda_cmd=$(get_conda)

    case "$action" in
        list|ls)
            step "ğŸ“¦ å¯ç”¨æ¨¡å‹"
            $conda_cmd run -n "$env_name" python "$WORKER_DIR/main.py" --list-models
            ;;
        *)
            error "æœªçŸ¥æ“ä½œ: $action"
            echo "å¯ç”¨æ“ä½œ: list"
            exit 1
            ;;
    esac
}

# =============================================================================
# å‘½ä»¤: prepare - æ•°æ®é¢„å¤„ç†
# =============================================================================
cmd_prepare() {
    local env_name="$DEFAULT_ENV_NAME"
    local raw_dir=""
    local output_file=""
    local character="$DEFAULT_CHARACTER"
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)      env_name="$2"; shift 2 ;;
            --raw)      raw_dir="$2"; shift 2 ;;
            --out)      output_file="$2"; shift 2 ;;
            --char)     character="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    if [[ -z "$raw_dir" || -z "$output_file" ]]; then
        error "ç”¨æ³•: ./cy-llm prepare --raw <raw_dir> --out <output_file> [--char <character>]"
        exit 1
    fi

    step "æ•°æ®é¢„å¤„ç†: $character"
    
    local conda_cmd=$(get_conda)
    $conda_cmd run -n "$env_name" python "$TRAINING_DIR/src/dataset_converter.py" \
        --raw_dir "$raw_dir" \
        --output_file "$output_file" \
        --character "$character"
        
    success "é¢„å¤„ç†å®Œæˆ: $output_file"
}

# =============================================================================
# å‘½ä»¤: train - LoRA å¾®è°ƒè®­ç»ƒ
# =============================================================================
cmd_train() {
    local env_name="$DEFAULT_ENV_NAME"
    local model="$DEFAULT_MODEL"
    local dataset=""
    local settings=""
    local output=""
    local batch="$DEFAULT_BATCH_SIZE"
    local grad_accum="$DEFAULT_GRAD_ACCUM"
    local epochs="$DEFAULT_EPOCHS"
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)          env_name="$2"; shift 2 ;;
            --model)        model="$2"; shift 2 ;;
            --dataset)      dataset="$2"; shift 2 ;;
            --settings)     settings="$2"; shift 2 ;;
            --output)       output="$2"; shift 2 ;;
            --batch)        batch="$2"; shift 2 ;;
            --grad-accum)   grad_accum="$2"; shift 2 ;;
            --epochs)       epochs="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    if [[ -z "$dataset" || -z "$output" ]]; then
        error "ç”¨æ³•: ./cy-llm train --dataset <path> --output <dir> [--model <id>] [--settings <path>] ..."
        exit 1
    fi

    step "å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ"
    info "åŸºåº§æ¨¡å‹: $model"
    info "è®­ç»ƒæ•°æ®: $dataset"
    [[ -n "$settings" ]] && info "è®¾å®šæ•°æ®: $settings"
    info "è¾“å‡ºç›®å½•: $output"

    local conda_cmd=$(get_conda)
    local -a cmd_args=(
        "$TRAINING_DIR/src/train_lora.py"
        --model_name "$model"
        --dataset_path "$dataset"
        --output_dir "$output"
        --batch_size "$batch"
        --grad_accum "$grad_accum"
        --epochs "$epochs"
    )
    
    [[ -n "$settings" ]] && cmd_args+=(--settings_path "$settings")

    $conda_cmd run -n "$env_name" python "${cmd_args[@]}"
    
    success "è®­ç»ƒå®Œæˆï¼Œæƒé‡ä¿å­˜è‡³: $output"
}

# =============================================================================
# å‘½ä»¤: chat - äº¤äº’å¼ LoRA æ¨ç†
# =============================================================================
cmd_chat() {
    local env_name="$DEFAULT_ENV_NAME"
    local model="$DEFAULT_MODEL"
    local lora=""
    local character="$DEFAULT_CHARACTER"
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --env)      env_name="$2"; shift 2 ;;
            --model)    model="$2"; shift 2 ;;
            --lora)     lora="$2"; shift 2 ;;
            --char)     character="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    if [[ -z "$model" || -z "$lora" ]]; then
        error "ç”¨æ³•: ./cy-llm chat --model <base_model> --lora <lora_path> [--char <character>]"
        exit 1
    fi

    step "è¿›å…¥äº¤äº’å¼å¯¹è¯ ($character)"
    
    local conda_cmd=$(get_conda)
    $conda_cmd run -n "$env_name" python "$TRAINING_DIR/src/inference.py" \
        --base_model "$model" \
        --lora_path "$lora" \
        --character "$character"
}

# =============================================================================
# å‘½ä»¤: config - é…ç½®ç®¡ç†
# =============================================================================
cmd_config() {
    local action="${1:-validate}"
    local config_file="${2:-$BACKEND_DIR/deploy/config.json}"

    case "$action" in
        validate)
            step "ğŸ” éªŒè¯é…ç½®æ–‡ä»¶: $config_file"

            if [[ ! -f "$config_file" ]]; then
                error "é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $config_file"
                exit 1
            fi

            PYTHONPATH="$BACKEND_DIR" python - "$config_file" <<'PY'
import json
import sys
from pathlib import Path

config_path = Path(sys.argv[1])

# æ£€æŸ¥ JSON è¯­æ³•
try:
    with open(config_path) as f:
        config = json.load(f)
    print("âœ… JSON è¯­æ³•æ­£ç¡®")
except json.JSONDecodeError as e:
    print(f"âŒ JSON è¯­æ³•é”™è¯¯: {e}")
    sys.exit(1)

# æ£€æŸ¥å¿…å¡«å­—æ®µ
if "models" not in config:
    print("âŒ ç¼ºå°‘ 'models' å­—æ®µ")
    sys.exit(1)

print(f"âœ… æ‰¾åˆ° {len(config['models'])} ä¸ªæ¨¡å‹é…ç½®")

# éªŒè¯æ¯ä¸ªæ¨¡å‹é…ç½®
errors = []
warnings = []
valid_engines = ["cuda-vllm", "cuda-vllm-async", "cuda-trt", "nvidia", "ascend-vllm", "ascend-mindie", "ascend"]
valid_quantization = ["awq", "gptq", "bitsandbytes", "fp8", "fp8_e5m2", None]

for model_name, model_config in config["models"].items():
    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    if "engine" not in model_config:
        errors.append(f"æ¨¡å‹ '{model_name}': ç¼ºå°‘ 'engine' å­—æ®µ")
        continue
    if "model_path" not in model_config:
        errors.append(f"æ¨¡å‹ '{model_name}': ç¼ºå°‘ 'model_path' å­—æ®µ")
        continue

    # éªŒè¯å¼•æ“ç±»å‹
    engine = model_config["engine"]
    if engine not in valid_engines:
        warnings.append(f"æ¨¡å‹ '{model_name}': æœªçŸ¥å¼•æ“ç±»å‹ '{engine}'ï¼ˆæ”¯æŒ: {', '.join(valid_engines)}ï¼‰")

    # éªŒè¯é‡åŒ–æ–¹æ³•
    quantization = model_config.get("quantization")
    if quantization and quantization not in valid_quantization:
        warnings.append(f"æ¨¡å‹ '{model_name}': æœªçŸ¥é‡åŒ–æ–¹æ³• '{quantization}'ï¼ˆæ”¯æŒ: awq, gptq, bitsandbytes, fp8ï¼‰")

    # vLLM + bitsandbytes å†²çªæ£€æŸ¥
    if "vllm" in engine and quantization == "bitsandbytes":
        errors.append(f"æ¨¡å‹ '{model_name}': vLLM å¼•æ“ä¸æ”¯æŒ bitsandbytes é‡åŒ–ï¼Œè¯·ä½¿ç”¨ AWQ/GPTQ æˆ–åˆ‡æ¢åˆ° nvidia å¼•æ“")

    # æ£€æŸ¥ gpu_memory_utilization
    gpu_util = model_config.get("gpu_memory_utilization")
    if gpu_util and gpu_util > 0.90:
        warnings.append(f"æ¨¡å‹ '{model_name}': gpu_memory_utilization={gpu_util:.2f} è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´ OOMï¼ˆæ¨è 0.70-0.85ï¼‰")

    print(f"  âœ“ {model_name}: {engine} ({model_config['model_path']})")

# è¾“å‡ºç»“æœ
print()
if errors:
    print("âŒ å‘ç°é…ç½®é”™è¯¯:")
    for err in errors:
        print(f"  â€¢ {err}")
    sys.exit(1)

if warnings:
    print("âš ï¸  é…ç½®è­¦å‘Š:")
    for warn in warnings:
        print(f"  â€¢ {warn}")

if not errors and not warnings:
    print("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œæ— é—®é¢˜")
else:
    print("\nğŸ’¡ å»ºè®®ä¿®å¤è­¦å‘Šä»¥è·å¾—æœ€ä½³æ€§èƒ½")
PY
            ;;
        *)
            error "æœªçŸ¥æ“ä½œ: $action"
            echo "å¯ç”¨æ“ä½œ: validate"
            exit 1
            ;;
    esac
}

# =============================================================================
# å‘½ä»¤: doctor - ç¯å¢ƒè¯Šæ–­
# =============================================================================
cmd_doctor() {
    local env_name="${CY_LLM_CONDA_ENV:-$DEFAULT_ENV_NAME}"

    step "ğŸ” ç¯å¢ƒè¯Šæ–­"

    # æ£€æŸ¥ Conda ç¯å¢ƒ
    echo -e "\n${CYAN}Conda ç¯å¢ƒ:${NC}"
    local conda_cmd=$(get_conda)
    if $conda_cmd env list | grep -q "^$env_name "; then
        echo "  âœ… ç¯å¢ƒå­˜åœ¨: $env_name"
    else
        echo "  âŒ ç¯å¢ƒä¸å­˜åœ¨: $env_name"
        echo "  è¿è¡Œ: ./cy-llm setup --engine <engine_type>"
        return 1
    fi

    # æ£€æŸ¥å…³é”®ä¾èµ–
    echo -e "\n${CYAN}ä¾èµ–æ£€æŸ¥:${NC}"
    $conda_cmd run -n "$env_name" python - <<'PY'
import sys
def check_import(module, name=None):
    try:
        mod = __import__(module)
        version = getattr(mod, "__version__", "æœªçŸ¥")
        print(f"  âœ… {name or module}: {version}")
        return True
    except ImportError:
        print(f"  âŒ {name or module}: æœªå®‰è£…")
        return False

check_import("torch", "PyTorch")
check_import("transformers")
check_import("vllm", "vLLM")
try:
    __import__("tensorrt_llm")
    print("  âœ… TensorRT-LLM: å·²å®‰è£…")
except ImportError:
    print("  âŒ TensorRT-LLM: æœªå®‰è£…")
PY

    # æ£€æŸ¥ GPU
    echo -e "\n${CYAN}GPU çŠ¶æ€:${NC}"
    if command -v nvidia-smi &> /dev/null; then
        nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader 2>/dev/null | \
            awk -F', ' '{printf "  GPU: %s (%s æ€», %s å¯ç”¨)\n", $1, $2, $3}' || \
            echo "  âš ï¸  æ— æ³•è·å– GPU ä¿¡æ¯"
    else
        echo "  âŒ nvidia-smi ä¸å¯ç”¨"
    fi

    echo ""
    success "è¯Šæ–­å®Œæˆ"
}

# =============================================================================
# å‘½ä»¤: convert-trt - æ¨¡å‹è½¬æ¢ä¸º TensorRT å¼•æ“
# =============================================================================
cmd_convert_trt() {
    local model=""
    local output=""
    local env_name="${CY_LLM_CONDA_ENV:-$DEFAULT_ENV_NAME}"

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --model) model="$2"; shift 2 ;;
            --output) output="$2"; shift 2 ;;
            --env) env_name="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    if [[ -z "$model" || -z "$output" ]]; then
        error "ç”¨æ³•: ./cy convert-trt --model <model> --output <output>"
        error "ç¤ºä¾‹: ./cy convert-trt --model Qwen/Qwen2.5-7B-Instruct --output /models/qwen2.5-7b-trt"
        exit 1
    fi

    step "ğŸ”§ è½¬æ¢æ¨¡å‹ä¸º TensorRT-LLM å¼•æ“"

    conda run -n "$env_name" python "$SCRIPT_DIR/scripts/convert_trt.py" \
        --model "$model" \
        --output "$output" \
        "$@"
}

# =============================================================================
# å‘½ä»¤: help - å¸®åŠ©ä¿¡æ¯
# =============================================================================
cmd_help() {
    cat << 'EOF'
CY-LLM Engine - ç»Ÿä¸€éƒ¨ç½²å‘½ä»¤è¡Œå·¥å…·

ç”¨æ³•: ./cy-llm <command> [options]

å‘½ä»¤:
  setup       åˆå§‹åŒ–ç¯å¢ƒ (Conda + ä¾èµ– + Gateway æ„å»º)
  start       å¯åŠ¨å®Œæ•´æœåŠ¡ (Gateway + Worker)
    lite        å¯åŠ¨è½»é‡ç‰ˆæœåŠ¡ (Gateway Lite + Coordinator Lite + Worker)
  worker      ä»…å¯åŠ¨ Worker
  stop        åœæ­¢æ‰€æœ‰æœåŠ¡
  status      æŸ¥çœ‹æœåŠ¡çŠ¶æ€
  diagnose    ç¯å¢ƒä¸æ¨¡å‹è¯Šæ–­ (VRAMã€ä¾èµ–ã€å…¼å®¹æ€§)
  doctor      ç¯å¢ƒè¯Šæ–­ (ç®€åŒ–ç‰ˆ)
  config      é…ç½®æ–‡ä»¶ç®¡ç† (validate éªŒè¯é…ç½®)
  convert-trt è½¬æ¢æ¨¡å‹ä¸º TensorRT-LLM å¼•æ“
  docker      Docker Compose éƒ¨ç½²
  test        è¿è¡Œæµ‹è¯•
  models      æ¨¡å‹ç®¡ç†
  prepare     æ•°æ®é¢„å¤„ç† (dataset_converter)
  train       LoRA å¾®è°ƒè®­ç»ƒ (train_lora)
  chat        äº¤äº’å¼ LoRA æ¨ç†æµ‹è¯• (inference)
  help        æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

å¸¸ç”¨é€‰é¡¹:
    --env NAME        Conda ç¯å¢ƒåç§° (é»˜è®¤: cy_llm_worker)
  --engine TYPE     æ¨ç†å¼•æ“ç±»å‹:
                      cuda-vllm      - NVIDIA GPU + vLLM (æ¨è)
                      cuda-trt       - NVIDIA GPU + TensorRT-LLM
                      nvidia         - NVIDIA GPU + Transformers + bitsandbytes
                      ascend-vllm    - åä¸º NPU + vLLM-Ascend
                      ascend-mindie  - åä¸º NPU + MindIE Turbo
  --port PORT       Gateway ç«¯å£ (é»˜è®¤: 8080)
  --skip-deps       è·³è¿‡ä¾èµ–å®‰è£…
  --skip-gradle     è·³è¿‡ Gateway æ„å»º

ç¤ºä¾‹:
  # åˆå§‹åŒ– vLLM ç¯å¢ƒ
  ./cy-llm setup --engine cuda-vllm

  # åˆå§‹åŒ– TRT ç¯å¢ƒ
  ./cy-llm setup --engine cuda-trt

  # è¯Šæ–­ç¯å¢ƒä¸æ¨¡å‹
  ./cy-llm diagnose deepseek-v3

  # éªŒè¯é…ç½®æ–‡ä»¶
  ./cy-llm config validate

  # è½¬æ¢æ¨¡å‹ä¸º TRT å¼•æ“
  ./cy-llm convert-trt --model Qwen/Qwen2.5-7B-Instruct --output /models/qwen2.5-7b-trt

  # å¯åŠ¨æœåŠ¡
  ./cy-llm start --engine cuda-vllm

    # å¯åŠ¨è½»é‡ç‰ˆæœåŠ¡ (å¼€æºç‰ˆ Lite)
    ./cy-llm lite --engine cuda-vllm --model deepseek-v3

  # åå°å¯åŠ¨
  ./cy-llm start -d

  # Docker éƒ¨ç½²
  ./cy-llm docker up --engine cuda-vllm --scale 2

  # æŸ¥çœ‹çŠ¶æ€
  ./cy-llm status

  # æ•°æ®é¢„å¤„ç†
  ./cy-llm prepare --raw ./raw_data --out ./data/train.jsonl --char èŠ™å®å¨œ

  # å¯åŠ¨ LoRA è®­ç»ƒ
  ./cy-llm train --dataset ./data/train.jsonl --output ./checkpoints/lora_v1 --model facebook/opt-2.7b

  # äº¤äº’å¼å¯¹è¯æµ‹è¯•
  ./cy-llm chat --model facebook/opt-2.7b --lora ./checkpoints/lora_v1

ç¯å¢ƒå˜é‡:
    CY_LLM_ENGINE          é»˜è®¤æ¨ç†å¼•æ“
    CY_LLM_CONDA_ENV       é»˜è®¤ Conda ç¯å¢ƒå
    CY_LLM_PORT            é»˜è®¤ Gateway ç«¯å£
    CY_LLM_WORKER_PORT     é»˜è®¤ Worker ç«¯å£
    CY_LLM_LITE_PORT        Lite Gateway ç«¯å£
    CY_LLM_LITE_COORDINATOR_PORT  Lite Coordinator ç«¯å£
    CY_LLM_LITE_WORKER_PORT       Lite Worker ç«¯å£

æ›´å¤šä¿¡æ¯: https://github.com/your-repo/CY-LLM-Engine
EOF
}

# =============================================================================
# ä¸»å…¥å£
# =============================================================================
main() {
    local command="${1:-help}"
    shift || true
    
    case "$command" in
        setup)      cmd_setup "$@" ;;
        start)      cmd_start "$@" ;;
        lite)       cmd_lite "$@" ;;
        worker)     cmd_worker "$@" ;;
        docker)     cmd_docker "$@" ;;
        stop)       cmd_stop "$@" ;;
        status)     cmd_status "$@" ;;
        diagnose)   cmd_diagnose "$@" ;;
        doctor)     cmd_doctor "$@" ;;
        config)     cmd_config "$@" ;;
        convert-trt) cmd_convert_trt "$@" ;;
        test)       cmd_test "$@" ;;
        models)     cmd_models "$@" ;;
        prepare)    cmd_prepare "$@" ;;
        train)      cmd_train "$@" ;;
        chat)       cmd_chat "$@" ;;
        help|--help|-h) cmd_help ;;
        *)
            error "æœªçŸ¥å‘½ä»¤: $command"
            echo "è¿è¡Œ './cy-llm help' æŸ¥çœ‹å¸®åŠ©"
            exit 1
            ;;
    esac
}

main "$@"
